commit 1489748879b104a2470340770e838a58c053d9f9
Author: Adrilaynez <adrilaynezortiz@gmail.com>
Date:   Thu Feb 19 20:21:52 2026 +0100

    feat(i18n): translate BigramNarrative and BigramMatrixBuilder components

diff --git a/src/components/lab/BigramMatrixBuilder.tsx b/src/components/lab/BigramMatrixBuilder.tsx
new file mode 100644
index 0000000..de90700
--- /dev/null
+++ b/src/components/lab/BigramMatrixBuilder.tsx
@@ -0,0 +1,306 @@
+"use client";
+
+import { useCallback, useEffect, useMemo, useState } from "react";
+import { useTranslation } from "@/context/i18n";
+
+const ALLOWED_VOCABULARY = "abcdefghijklmnopqrstuvwxyz ";
+const MAX_EDUCATIONAL_VOCAB = 12;
+const ANIMATION_DELAY_MS = 380;
+
+type BuildStep = {
+    from: string;
+    to: string;
+    row: number;
+    col: number;
+    position: number;
+};
+
+function createZeroMatrix(size: number): number[][] {
+    return Array.from({ length: size }, () => Array(size).fill(0));
+}
+
+function normalizeToVocabulary(input: string): string {
+    const allowed = new Set(ALLOWED_VOCABULARY.split(""));
+    return input
+        .toLowerCase()
+        .split("")
+        .filter((char) => allowed.has(char))
+        .join("");
+}
+
+function buildEducationalVocabulary(text: string): string[] {
+    const seen = new Set<string>();
+    const chars: string[] = [];
+
+    for (const char of text) {
+        if (!seen.has(char)) {
+            seen.add(char);
+            chars.push(char);
+        }
+        if (chars.length >= MAX_EDUCATIONAL_VOCAB) {
+            break;
+        }
+    }
+
+    return chars.length > 0 ? chars : ["a", "b", " "];
+}
+
+function formatCharLabel(char: string): string {
+    return char === " " ? "ÔÉá" : char;
+}
+
+export function BigramMatrixBuilder() {
+    const { t } = useTranslation();
+    const [inputText, setInputText] = useState("hello world");
+    const [normalizedText, setNormalizedText] = useState("hello world");
+    const [currentStepIndex, setCurrentStepIndex] = useState(-1);
+    const [isPlaying, setIsPlaying] = useState(false);
+    const [vocab, setVocab] = useState<string[]>(() =>
+        buildEducationalVocabulary("hello world")
+    );
+
+    const charToIndex = useMemo(
+        () => new Map(vocab.map((char, index) => [char, index])),
+        [vocab]
+    );
+
+    const steps = useMemo<BuildStep[]>(() => {
+        const entries: BuildStep[] = [];
+        for (let i = 0; i < normalizedText.length - 1; i += 1) {
+            const from = normalizedText[i];
+            const to = normalizedText[i + 1];
+            const row = charToIndex.get(from) ?? -1;
+            const col = charToIndex.get(to) ?? -1;
+
+            if (row >= 0 && col >= 0) {
+                entries.push({ from, to, row, col, position: i });
+            }
+        }
+        return entries;
+    }, [charToIndex, normalizedText]);
+
+    const activeStep =
+        currentStepIndex >= 0 && currentStepIndex < steps.length
+            ? steps[currentStepIndex]
+            : null;
+
+    const matrix = useMemo(() => {
+        const nextMatrix = createZeroMatrix(vocab.length);
+        const lastAppliedStep = Math.min(currentStepIndex, steps.length - 1);
+        for (let i = 0; i <= lastAppliedStep; i += 1) {
+            const step = steps[i];
+            if (step) {
+                nextMatrix[step.row][step.col] += 1;
+            }
+        }
+        return nextMatrix;
+    }, [currentStepIndex, steps, vocab.length]);
+
+    const handleBuild = useCallback(() => {
+        const normalized = normalizeToVocabulary(inputText);
+        const nextVocab = buildEducationalVocabulary(normalized);
+        setNormalizedText(normalized);
+        setVocab(nextVocab);
+        setCurrentStepIndex(-1);
+        setIsPlaying(false);
+    }, [inputText]);
+
+    const applyNextStep = useCallback(() => {
+        setCurrentStepIndex((prevIndex) => {
+            const nextIndex = prevIndex + 1;
+            if (nextIndex >= steps.length) {
+                setIsPlaying(false);
+                return prevIndex;
+            }
+            return nextIndex;
+        });
+    }, [steps]);
+
+    const handleResetProgress = useCallback(() => {
+        setCurrentStepIndex(-1);
+        setIsPlaying(false);
+    }, []);
+
+    const handleInstantComplete = useCallback(() => {
+        if (steps.length === 0 || currentStepIndex >= steps.length - 1) {
+            return;
+        }
+        setCurrentStepIndex(steps.length - 1);
+        setIsPlaying(false);
+    }, [currentStepIndex, steps]);
+
+    const skippedCharacters = useMemo(() => {
+        const uniqueChars = new Set(normalizedText.split(""));
+        return Math.max(0, uniqueChars.size - vocab.length);
+    }, [normalizedText, vocab.length]);
+
+    useEffect(() => {
+        if (!isPlaying) {
+            return;
+        }
+        if (steps.length === 0 || currentStepIndex >= steps.length - 1) {
+            setIsPlaying(false);
+            return;
+        }
+
+        const timer = window.setTimeout(() => {
+            applyNextStep();
+        }, ANIMATION_DELAY_MS);
+
+        return () => window.clearTimeout(timer);
+    }, [applyNextStep, currentStepIndex, isPlaying, steps.length]);
+
+    return (
+        <div className="rounded-2xl border border-emerald-500/20 bg-emerald-500/[0.03] p-4 md:p-6">
+            <p className="text-center text-[15px] md:text-base text-white/55 leading-relaxed max-w-2xl mx-auto">
+                {t("bigramBuilder.description")}
+            </p>
+
+            <div className="mt-6 max-w-2xl mx-auto">
+                <textarea
+                    value={inputText}
+                    onChange={(event) => setInputText(event.target.value)}
+                    rows={4}
+                    className="w-full rounded-xl border border-white/10 bg-black/20 px-4 py-3 text-sm text-white/85 outline-none focus:border-emerald-500/40 focus:ring-1 focus:ring-emerald-500/30 resize-y"
+                    placeholder={t("bigramBuilder.placeholder")}
+                />
+                <p className="text-center text-xs text-white/35 mt-2">
+                    {t("bigramBuilder.hint")}
+                </p>
+                <div className="mt-4 flex flex-wrap justify-center gap-2">
+                    <button
+                        type="button"
+                        onClick={handleBuild}
+                        className="px-4 py-2 rounded-lg border border-emerald-500/40 bg-emerald-500/10 text-emerald-300 text-xs font-mono uppercase tracking-wider hover:bg-emerald-500/15 transition-colors cursor-pointer"
+                    >
+                        {t("bigramBuilder.buttons.build")}
+                    </button>
+                    <button
+                        type="button"
+                        onClick={applyNextStep}
+                        disabled={steps.length === 0 || currentStepIndex >= steps.length - 1}
+                        className="px-4 py-2 rounded-lg border border-indigo-500/35 bg-indigo-500/10 text-indigo-300 text-xs font-mono uppercase tracking-wider disabled:opacity-40 disabled:cursor-not-allowed hover:bg-indigo-500/15 transition-colors cursor-pointer"
+                    >
+                        {t("bigramBuilder.buttons.next")}
+                    </button>
+                    <button
+                        type="button"
+                        onClick={() => setIsPlaying((prev) => !prev)}
+                        disabled={steps.length === 0 || currentStepIndex >= steps.length - 1}
+                        className="px-4 py-2 rounded-lg border border-amber-500/35 bg-amber-500/10 text-amber-300 text-xs font-mono uppercase tracking-wider disabled:opacity-40 disabled:cursor-not-allowed hover:bg-amber-500/15 transition-colors cursor-pointer"
+                    >
+                        {isPlaying ? t("bigramBuilder.buttons.pause") : t("bigramBuilder.buttons.autoPlay")}
+                    </button>
+                    <button
+                        type="button"
+                        onClick={handleInstantComplete}
+                        disabled={steps.length === 0 || currentStepIndex >= steps.length - 1}
+                        className="px-4 py-2 rounded-lg border border-cyan-500/35 bg-cyan-500/10 text-cyan-300 text-xs font-mono uppercase tracking-wider disabled:opacity-40 disabled:cursor-not-allowed hover:bg-cyan-500/15 transition-colors cursor-pointer"
+                    >
+                        {t("bigramBuilder.buttons.instant")}
+                    </button>
+                    <button
+                        type="button"
+                        onClick={handleResetProgress}
+                        disabled={steps.length === 0}
+                        className="px-4 py-2 rounded-lg border border-white/20 bg-white/[0.04] text-white/60 text-xs font-mono uppercase tracking-wider disabled:opacity-40 disabled:cursor-not-allowed hover:bg-white/[0.08] transition-colors cursor-pointer"
+                    >
+                        {t("bigramBuilder.buttons.reset")}
+                    </button>
+                </div>
+            </div>
+
+            <div className="mt-6 text-center">
+                <p className="text-[11px] font-mono uppercase tracking-widest text-white/35">
+                    {t("bigramBuilder.vocab")} ({vocab.length}/{MAX_EDUCATIONAL_VOCAB}):{" "}
+                    {vocab.map(formatCharLabel).join(" ")}
+                </p>
+                <p className="text-sm text-white/45 mt-2">
+                    {t("bigramBuilder.normalized")}{" "}
+                    <span className="font-mono text-white/70">
+                        {normalizedText.length > 0 ? normalizedText : t("bigramBuilder.empty")}
+                    </span>
+                </p>
+                {skippedCharacters > 0 && (
+                    <p className="text-xs text-amber-300/70 mt-2">
+                        {t("bigramBuilder.skipped").replace("{max}", MAX_EDUCATIONAL_VOCAB.toString()).replace("{count}", skippedCharacters.toString())}
+                    </p>
+                )}
+            </div>
+
+            <div className="mt-4 text-center text-sm text-white/55">
+                {activeStep ? (
+                    <p>
+                        {t("bigramBuilder.step1")} {currentStepIndex + 1} / {steps.length}:{" "}
+                        <span className="font-mono text-emerald-300">
+                            {formatCharLabel(activeStep.from)}
+                        </span>{" "}
+                        ÔåÆ{" "}
+                        <span className="font-mono text-emerald-300">
+                            {formatCharLabel(activeStep.to)}
+                        </span>{" "}
+                        {t("bigramBuilder.step2")}
+                        <span className="font-mono text-indigo-300">
+                            {formatCharLabel(activeStep.from)}
+                        </span>
+                        ,{" "}
+                        <span className="font-mono text-indigo-300">
+                            {formatCharLabel(activeStep.to)}
+                        </span>
+                        {t("bigramBuilder.step3")}
+                    </p>
+                ) : (
+                    <p>
+                        {t("bigramBuilder.pressBuild")}
+                    </p>
+                )}
+            </div>
+
+            <div className="mt-6 overflow-auto rounded-xl border border-white/10 bg-black/20">
+                <table className="w-max min-w-full border-collapse text-[10px] font-mono">
+                    <thead className="sticky top-0 z-10 bg-[#11131a]">
+                        <tr>
+                            <th className="px-2 py-2 text-white/60 border-b border-r border-white/10">
+                                {t("bigramBuilder.table.curnxt")}
+                            </th>
+                            {vocab.map((colChar) => (
+                                <th
+                                    key={`col-${colChar}`}
+                                    className="px-2 py-2 text-white/60 border-b border-r border-white/10"
+                                >
+                                    {formatCharLabel(colChar)}
+                                </th>
+                            ))}
+                        </tr>
+                    </thead>
+                    <tbody>
+                        {matrix.map((row, rowIndex) => (
+                            <tr key={`row-${vocab[rowIndex]}`}>
+                                <th className="px-2 py-1.5 text-white/60 border-r border-b border-white/10 bg-[#11131a] sticky left-0">
+                                    {formatCharLabel(vocab[rowIndex])}
+                                </th>
+                                {row.map((value, colIndex) => {
+                                    const isActiveCell =
+                                        activeStep?.row === rowIndex &&
+                                        activeStep?.col === colIndex;
+                                    return (
+                                        <td
+                                            key={`cell-${rowIndex}-${colIndex}`}
+                                            className={`px-2 py-1.5 text-center border-r border-b border-white/10 transition-colors ${
+                                                isActiveCell
+                                                    ? "bg-emerald-500/30 text-emerald-100 font-bold"
+                                                    : "text-white/70"
+                                            }`}
+                                        >
+                                            {value}
+                                        </td>
+                                    );
+                                })}
+                            </tr>
+                        ))}
+                    </tbody>
+                </table>
+            </div>
+        </div>
+    );
+}
\ No newline at end of file
diff --git a/src/components/lab/BigramNarrative.tsx b/src/components/lab/BigramNarrative.tsx
index ab14f51..35bef27 100644
--- a/src/components/lab/BigramNarrative.tsx
+++ b/src/components/lab/BigramNarrative.tsx
@@ -6,7 +6,9 @@ import { ModeToggle } from "@/components/lab/ModeToggle";
 import { TransitionMatrix } from "@/components/lab/TransitionMatrix";
 import { InferenceConsole } from "@/components/lab/InferenceConsole";
 import { GenerationPlayground } from "@/components/lab/GenerationPlayground";
-import type { TransitionMatrixViz, Prediction } from "@/types/lmLab";
+import { BigramMatrixBuilder } from "@/components/lab/BigramMatrixBuilder";
+import type { TransitionMatrixViz, Prediction, TrainingViz } from "@/types/lmLab";
+import { useTranslation } from "@/context/i18n";
 
 import { BlockMath } from "react-katex";
 import "katex/dist/katex.min.css";
@@ -166,7 +168,7 @@ function FormulaBlock({ formula, caption }: { formula: string; caption: string }
         >
             <div className="flex items-center justify-center mb-10">
                 <div className="inline-block px-8 py-4 rounded-2xl bg-indigo-500/[0.04] border border-indigo-500/[0.15] backdrop-blur-sm shadow-[0_0_40px_-15px_rgba(99,102,241,0.15)]">
-                    <BlockMath math="P(x_{t+1} \mid x_t)" />
+                    <BlockMath math={formula} />
                 </div>
             </div>
 
@@ -205,10 +207,12 @@ function SectionBreak() {
 function FigureWrapper({
     label,
     hint,
+    showWindowDots = true,
     children,
 }: {
     label: string;
     hint?: string;
+    showWindowDots?: boolean;
     children: React.ReactNode;
 }) {
     return (
@@ -221,11 +225,13 @@ function FigureWrapper({
         >
             <div className="rounded-2xl border border-white/[0.08] bg-white/[0.02] backdrop-blur-sm overflow-hidden shadow-[0_0_80px_-20px_rgba(16,185,129,0.06)]">
                 <div className="flex items-center gap-3 px-5 py-3 border-b border-white/[0.06] bg-white/[0.02]">
-                    <div className="flex gap-1.5">
-                        <span className="w-2.5 h-2.5 rounded-full bg-red-500/30" />
-                        <span className="w-2.5 h-2.5 rounded-full bg-yellow-500/30" />
-                        <span className="w-2.5 h-2.5 rounded-full bg-green-500/30" />
-                    </div>
+                    {showWindowDots && (
+                        <div className="flex gap-1.5">
+                            <span className="w-2.5 h-2.5 rounded-full bg-red-500/30" />
+                            <span className="w-2.5 h-2.5 rounded-full bg-yellow-500/30" />
+                            <span className="w-2.5 h-2.5 rounded-full bg-green-500/30" />
+                        </div>
+                    )}
                     <span className="text-[10px] font-mono uppercase tracking-widest text-white/30">
                         {label}
                     </span>
@@ -247,6 +253,7 @@ function FigureWrapper({
 
 interface BigramNarrativeProps {
     matrixData: TransitionMatrixViz | null;
+    trainingData?: TrainingViz | null;
     onCellClick: (row: string, col: string) => void;
 
     onAnalyze: (text: string, topK: number) => void;
@@ -264,6 +271,7 @@ interface BigramNarrativeProps {
 
 export function BigramNarrative({
     matrixData,
+    trainingData,
     onCellClick,
     onAnalyze,
     predictions,
@@ -276,6 +284,8 @@ export function BigramNarrative({
     genLoading,
     genError,
 }: BigramNarrativeProps) {
+    const { t } = useTranslation();
+
     return (
         <article className="max-w-[740px] mx-auto px-6 pt-8 pb-24">
 
@@ -288,19 +298,18 @@ export function BigramNarrative({
                 >
                     <span className="inline-flex items-center gap-2 text-[10px] font-mono font-bold uppercase tracking-[0.25em] text-emerald-400/60 mb-6">
                         <BookOpen className="w-3.5 h-3.5" />
-                        Understanding Language Models
+                        {t("bigramNarrative.hero.eyebrow")}
                     </span>
 
                     <h1 className="text-5xl md:text-7xl font-bold tracking-tight text-white mb-6">
-                        The Bigram{" "}
+                        {t("bigramNarrative.hero.titlePrefix")}{" "}
                         <span className="bg-gradient-to-r from-emerald-400 to-teal-300 bg-clip-text text-transparent">
-                            Model
+                            {t("bigramNarrative.hero.titleSuffix")}
                         </span>
                     </h1>
 
                     <p className="text-lg md:text-xl text-white/35 max-w-xl mx-auto leading-relaxed mb-12">
-                        A first-principles exploration of the simplest statistical
-                        language model ÔÇö and why it still matters.
+                        {t("bigramNarrative.hero.description")}
                     </p>
 
                     <div className="flex justify-center mb-14">
@@ -320,33 +329,27 @@ export function BigramNarrative({
             {/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ 1 ┬À THE PROBLEM ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */}
             <Section>
                 <SectionLabel number="1" label="Foundation" />
-                <Heading>The Problem of Prediction</Heading>
+                <Heading>{t("bigramNarrative.problem.title")}</Heading>
 
                 <Lead>
-                    Language is fundamentally sequential. Every word you read right
-                    now is informed by the words that came before it.
+                    {t("bigramNarrative.problem.lead")}
                 </Lead>
 
                 <P>
-                    This property ÔÇö that each token in a sequence carries <Highlight>expectations
-                    about what follows</Highlight> ÔÇö is what makes language both expressive
-                    and predictable. It&apos;s also what makes it so hard to model computationally.
+                    {t("bigramNarrative.problem.p1")} <Highlight>{t("bigramNarrative.problem.p1Highlight")}</Highlight> {t("bigramNarrative.problem.p1End")}
                 </P>
 
                 <P>
-                    The central challenge of language modeling is deceptively simple to state:
+                    {t("bigramNarrative.problem.p2")}
                 </P>
 
                 <PullQuote>
-                    Given what we have already seen, what should come next?
+                    {t("bigramNarrative.problem.quote")}
                 </PullQuote>
 
                 <P>
-                    This question has driven decades of research in <Highlight color="indigo">computational
-                    linguistics</Highlight>, <Highlight color="indigo">information theory</Highlight>,
-                    and, more recently, <Highlight color="indigo">deep learning</Highlight>. To build
-                    a model that can answer it, we need a way to capture the statistical structure
-                    of language. Let&apos;s start with the simplest possible approach.
+                    {t("bigramNarrative.problem.p3")} <Highlight color="indigo">{t("bigramNarrative.problem.p3H1")}</Highlight>, <Highlight color="indigo">{t("bigramNarrative.problem.p3H2")}</Highlight>,
+                    and, more recently, <Highlight color="indigo">{t("bigramNarrative.problem.p3H3")}</Highlight>{t("bigramNarrative.problem.p3End")}
                 </P>
             </Section>
 
@@ -355,38 +358,28 @@ export function BigramNarrative({
             {/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ 2 ┬À THE SIMPLEST IDEA ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */}
             <Section>
                 <SectionLabel number="2" label="Core Idea" />
-                <Heading>The Simplest Statistical Idea</Heading>
+                <Heading>{t("bigramNarrative.idea.title")}</Heading>
 
                 <Lead>
-                    What if, instead of trying to understand meaning, we simply
-                    observed patterns?
+                    {t("bigramNarrative.idea.lead")}
                 </Lead>
 
                 <P>
-                    Specifically: <Highlight>how often does one character follow
-                    another?</Highlight> This is the core insight behind the Bigram
-                    model. It ignores grammar, semantics, and long-range dependencies
-                    entirely. It asks only one question: given the current token, what
-                    is the probability distribution over the next token?
+                    {t("bigramNarrative.idea.p1")} <Highlight>{t("bigramNarrative.idea.p1Highlight")}</Highlight> {t("bigramNarrative.idea.p1End")}
                 </P>
 
                 <FormulaBlock
                     formula="P( x_{t+1} | x_t )"
-                    caption="The Bigram assumption: the next token depends only on the current one."
+                    caption={t("bigramNarrative.idea.caption")}
                 />
 
                 <P>
-                    We model <InlineCode>P(x&#8346;&#8330;&#8321; | x&#8346;)</InlineCode> ÔÇö
-                    the chance of seeing a particular next token given only the token we just
-                    observed. Nothing more, nothing less. This radical simplification is what
-                    makes the model both tractable and limited.
+                    {t("bigramNarrative.idea.p2")}
                 </P>
 
-                <Callout icon={Lightbulb} accent="emerald" title="Key Insight">
+                <Callout icon={Lightbulb} accent="emerald" title={t("bigramNarrative.idea.insightTitle")}>
                     <p>
-                        The &ldquo;bi&rdquo; in Bigram means <em>two</em>. The model considers
-                        pairs of tokens ÔÇö the current one and the next one. It has zero
-                        memory of anything before the current token.
+                        {t("bigramNarrative.idea.insightText")}
                     </p>
                 </Callout>
             </Section>
@@ -396,46 +389,49 @@ export function BigramNarrative({
             {/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ 3 ┬À TRANSITION TABLE ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */}
             <Section>
                 <SectionLabel number="3" label="Mechanics" />
-                <Heading>Building a Transition Table</Heading>
+                <Heading>{t("bigramNarrative.table.title")}</Heading>
 
                 <Lead>
-                    To learn these probabilities, the model scans through a training
-                    corpus and counts every pair of consecutive tokens.
+                    {t("bigramNarrative.table.lead")}
                 </Lead>
 
                 <P>
-                    For each token <InlineCode>A</InlineCode>, it records how often each
-                    possible token <InlineCode>B</InlineCode> appears immediately after it.
-                    These counts form a <Highlight>matrix</Highlight> ÔÇö a two-dimensional
-                    table where rows represent the current token and columns represent the
-                    next token. Each cell holds the number of times that specific transition
-                    was observed in the training data.
+                    {t("bigramNarrative.table.p1")} <Highlight>{t("bigramNarrative.table.p1Highlight")}</Highlight> {t("bigramNarrative.table.p1End")}
                 </P>
 
                 <P>
-                    The visualization below is a live rendering of this transition matrix.
-                    Brighter cells indicate more frequent pairings ÔÇö patterns the model has
-                    learned from real text.
+                    {t("bigramNarrative.table.p2")}
                 </P>
             </Section>
 
+            <FigureWrapper
+                label={t("bigramNarrative.table.fig1")}
+                hint={t("bigramNarrative.table.fig1Hint")}
+            >
+                <BigramMatrixBuilder />
+            </FigureWrapper>
+
             {/* Figure: Transition Matrix */}
             <FigureWrapper
-                label="Interactive ┬À Transition Matrix"
-                hint="Hover to inspect probabilities. Use the search to highlight a specific character's row and column."
+                label={t("bigramNarrative.table.fig2")}
+                hint={t("bigramNarrative.table.fig2Hint")}
+                showWindowDots={false}
             >
                 <TransitionMatrix
                     data={matrixData}
                     onCellClick={onCellClick}
+                    datasetMeta={{
+                        corpusName: "Paul Graham essays (paulgraham.com)",
+                        rawTextSize: trainingData?.raw_text_size,
+                        trainDataSize: trainingData?.train_data_size,
+                        vocabSize: trainingData?.unique_characters,
+                    }}
                 />
             </FigureWrapper>
 
-            <Callout icon={Zap} accent="indigo" title="Reading the Matrix">
+            <Callout icon={Zap} accent="indigo" title={t("bigramNarrative.table.readingTitle")}>
                 <p>
-                    Each row represents a &ldquo;given&rdquo; character. Each column represents a
-                    &ldquo;next&rdquo; character. The brightness of a cell encodes how likely that
-                    transition is. Notice how some rows are nearly uniform (the model is unsure)
-                    while others have sharp peaks (strong preferences).
+                    {t("bigramNarrative.table.readingText")}
                 </p>
             </Callout>
 
@@ -444,38 +440,29 @@ export function BigramNarrative({
             {/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ 4 ┬À COUNTS ÔåÆ PROBABILITIES ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */}
             <Section>
                 <SectionLabel number="4" label="Normalization" />
-                <Heading>From Counts to Probabilities</Heading>
+                <Heading>{t("bigramNarrative.normalization.title")}</Heading>
 
                 <Lead>
-                    Raw counts alone don&apos;t tell us much. To make predictions,
-                    we need to convert them into probabilities.
+                    {t("bigramNarrative.normalization.lead")}
                 </Lead>
 
                 <P>
-                    We do this by <Highlight>normalizing each row</Highlight> of the count
-                    matrix ÔÇö dividing every count by the total number of transitions from
-                    that row&apos;s token. After normalization, each row sums
-                    to <InlineCode>1.0</InlineCode>, forming a valid probability distribution.
+                    {t("bigramNarrative.normalization.p1")} <Highlight>{t("bigramNarrative.normalization.p1Highlight")}</Highlight> {t("bigramNarrative.normalization.p1End")}
                 </P>
 
                 <P>
-                    The model can now make concrete statements: <em>&ldquo;After the
-                    letter <InlineCode>h</InlineCode>, there is a 32% chance the next
-                    character is <InlineCode>e</InlineCode>, a 15% chance
-                    it&apos;s <InlineCode>a</InlineCode>, and so on.&rdquo;</em>
+                    {t("bigramNarrative.normalization.p2")}
                 </P>
 
                 <P>
-                    Try it yourself below. Type any text to see what the model predicts
-                    will come next ÔÇö based <Highlight color="amber">solely on the very last
-                    character</Highlight> of your input.
+                    {t("bigramNarrative.normalization.p3")} <Highlight color="amber">{t("bigramNarrative.normalization.p3Highlight")}</Highlight> {t("bigramNarrative.normalization.p3End")}
                 </P>
             </Section>
 
             {/* Figure: Inference Console */}
             <FigureWrapper
-                label="Interactive ┬À Next-Token Predictions"
-                hint="The model considers only the last character ÔÇö it has no memory of earlier context."
+                label={t("bigramNarrative.normalization.fig")}
+                hint={t("bigramNarrative.normalization.figHint")}
             >
                 <InferenceConsole
                     onAnalyze={onAnalyze}
@@ -492,41 +479,31 @@ export function BigramNarrative({
             {/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ 5 ┬À GENERATION ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */}
             <Section>
                 <SectionLabel number="5" label="Sampling" />
-                <Heading>Generating New Text</Heading>
+                <Heading>{t("bigramNarrative.generation.title")}</Heading>
 
                 <Lead>
-                    Once we have a probability distribution, we can do something
-                    remarkable: generate entirely new text.
+                    {t("bigramNarrative.generation.lead")}
                 </Lead>
 
                 <P>
-                    The process is called <Highlight>autoregressive sampling</Highlight>.
-                    Start with a seed character, sample the next one from its probability
-                    distribution, then use that new character as the seed for the next step.
-                    Repeat indefinitely.
+                    {t("bigramNarrative.generation.p1")} <Highlight>{t("bigramNarrative.generation.p1Highlight")}</Highlight>{t("bigramNarrative.generation.p1End")}
                 </P>
 
-                <Callout icon={Lightbulb} accent="amber" title="Temperature">
+                <Callout icon={Lightbulb} accent="amber" title={t("bigramNarrative.generation.tempTitle")}>
                     <p>
-                        The <strong className="text-white/70">temperature</strong> parameter controls
-                        how &ldquo;creative&rdquo; the generation is. At <strong className="text-white/70">low
-                        temperatures</strong>, the model almost always picks the most likely next
-                        token. At <strong className="text-white/70">high temperatures</strong>, it
-                        samples more uniformly ÔÇö producing surprising and often nonsensical output.
+                        {t("bigramNarrative.generation.tempText")}
                     </p>
                 </Callout>
 
                 <P>
-                    Generate some text below and observe how a model with <Highlight color="amber">only
-                    one character of memory</Highlight> produces output that is statistically plausible
-                    at the character level, yet meaningless at any higher level.
+                    {t("bigramNarrative.generation.p2")} <Highlight color="amber">{t("bigramNarrative.generation.p2Highlight")}</Highlight> {t("bigramNarrative.generation.p2End")}
                 </P>
             </Section>
 
             {/* Figure: Generation Playground */}
             <FigureWrapper
-                label="Interactive ┬À Text Generation"
-                hint="Try different temperatures. Low = predictable, High = chaotic."
+                label={t("bigramNarrative.generation.fig")}
+                hint={t("bigramNarrative.generation.figHint")}
             >
                 <GenerationPlayground
                     onGenerate={onGenerate}
@@ -541,40 +518,29 @@ export function BigramNarrative({
             {/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ 6 ┬À LIMITATIONS ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */}
             <Section>
                 <SectionLabel number="6" label="Reflection" />
-                <Heading>Power and Limitations</Heading>
+                <Heading>{t("bigramNarrative.limitations.title")}</Heading>
 
                 <Lead>
-                    The Bigram model is powerful precisely because of its simplicity.
+                    {t("bigramNarrative.limitations.lead")}
                 </Lead>
 
                 <P>
-                    It requires very few parameters ÔÇö just a <InlineCode>V &times; V</InlineCode> matrix,
-                    where <InlineCode>V</InlineCode> is the vocabulary size. It trains
-                    instantly. And it provides a clear <Highlight>probabilistic baseline</Highlight> for
-                    language generation that every more sophisticated model must beat.
+                    {t("bigramNarrative.limitations.p1")} <Highlight>{t("bigramNarrative.limitations.p1Highlight")}</Highlight> {t("bigramNarrative.limitations.p1End")}
                 </P>
 
-                <Callout icon={AlertTriangle} accent="rose" title="The Fundamental Limitation">
+                <Callout icon={AlertTriangle} accent="rose" title={t("bigramNarrative.limitations.limitationTitle")}>
                     <p>
-                        The model has <strong className="text-white/70">no memory beyond a single
-                        token</strong>. It cannot learn that &ldquo;th&rdquo; is often followed
-                        by &ldquo;e&rdquo;, because by the time it sees &ldquo;h&rdquo;, it has
-                        already forgotten the &ldquo;t&rdquo;. It captures local co-occurrence but
-                        nothing about words, phrases, or meaning.
+                        {t("bigramNarrative.limitations.limitationText")}
                     </p>
                 </Callout>
 
                 <P>
-                    This limitation is exactly what motivates the progression to more
-                    sophisticated architectures: <Highlight color="indigo">N-grams</Highlight> extend
-                    the context window, <Highlight color="indigo">MLPs</Highlight> learn dense
-                    representations, and <Highlight color="indigo">Transformers</Highlight> attend
-                    to the entire sequence at once.
+                    {t("bigramNarrative.limitations.p2")} <Highlight color="indigo">{t("bigramNarrative.limitations.p2H1")}</Highlight>, <Highlight color="indigo">{t("bigramNarrative.limitations.p2H2")}</Highlight>,
+                    and <Highlight color="indigo">{t("bigramNarrative.limitations.p2H3")}</Highlight> {t("bigramNarrative.limitations.p2End")}
                 </P>
 
                 <PullQuote>
-                    Each model in this lab builds on the same core question:
-                    given context, what comes next?
+                    {t("bigramNarrative.limitations.quote")}
                 </PullQuote>
             </Section>
 
@@ -586,14 +552,13 @@ export function BigramNarrative({
                 className="mt-8 pt-12 border-t border-white/[0.06] text-center"
             >
                 <p className="text-sm text-white/25 italic max-w-md mx-auto leading-relaxed mb-10">
-                    Continue exploring the other models in the lab to see how each
-                    one addresses the limitations of its predecessor.
+                    {t("bigramNarrative.footer.text")}
                 </p>
                 <div className="flex items-center justify-center gap-2 text-[10px] font-mono uppercase tracking-widest text-white/10">
                     <FlaskConical className="h-3 w-3" />
-                    LM-Lab &middot; Educational Mode
+                    {t("bigramNarrative.footer.brand")}
                 </div>
             </motion.footer>
         </article>
     );
-}
+}
\ No newline at end of file
diff --git a/src/i18n/en.ts b/src/i18n/en.ts
index bbdf8fb..15a9f75 100644
--- a/src/i18n/en.ts
+++ b/src/i18n/en.ts
@@ -583,4 +583,121 @@ export const en = {
             },
         },
     },
+    bigramNarrative: {
+        hero: {
+            eyebrow: "Understanding Language Models",
+            titlePrefix: "The Bigram",
+            titleSuffix: "Model",
+            description: "A first-principles exploration of the simplest statistical language model ÔÇö and why it still matters."
+        },
+        problem: {
+            title: "The Problem of Prediction",
+            lead: "Language is fundamentally sequential. Every word you read right now is informed by the words that came before it.",
+            p1: "This property ÔÇö that each token in a sequence carries",
+            p1Highlight: "expectations about what follows",
+            p1End: "ÔÇö is what makes language both expressive and predictable. It's also what makes it so hard to model computationally.",
+            p2: "The central challenge of language modeling is deceptively simple to state:",
+            quote: "Given what we have already seen, what should come next?",
+            p3: "This question has driven decades of research in",
+            p3H1: "computational linguistics",
+            p3H2: "information theory",
+            p3H3: "deep learning",
+            p3End: ". To build a model that can answer it, we need a way to capture the statistical structure of language. Let's start with the simplest possible approach.",
+        },
+        idea: {
+            title: "The Simplest Statistical Idea",
+            lead: "What if, instead of trying to understand meaning, we simply observed patterns?",
+            p1: "Specifically:",
+            p1Highlight: "how often does one character follow another?",
+            p1End: "This is the core insight behind the Bigram model. It ignores grammar, semantics, and long-range dependencies entirely. It asks only one question: given the current token, what is the probability distribution over the next token?",
+            caption: "The Bigram assumption: the next token depends only on the current one.",
+            p2: "We model P(x_{t+1} | x_t) ÔÇö the chance of seeing a particular next token given only the token we just observed. Nothing more, nothing less. This radical simplification is what makes the model both tractable and limited.",
+            insightTitle: "Key Insight",
+            insightText: "The \"bi\" in Bigram means two. The model considers pairs of tokens ÔÇö the current one and the next one. It has zero memory of anything before the current token."
+        },
+        table: {
+            title: "Building a Transition Table",
+            lead: "To learn these probabilities, the model scans through a training corpus and counts every pair of consecutive tokens.",
+            p1: "For each token A, it records how often each possible token B appears immediately after it. These counts form a",
+            p1Highlight: "matrix",
+            p1End: "ÔÇö a two-dimensional table where rows represent the current token and columns represent the next token. Each cell holds the number of times that specific transition was observed in the training data.",
+            p2: "Start with the educational builder below to see each update one step at a time. Then compare it with the full transition table rendered from the model.",
+            fig1: "Interactive ┬À Step-by-Step Matrix Builder",
+            fig1Hint: "Start here: build a small count matrix from your own text and follow each pair update.",
+            fig2: "Interactive ┬À Transition Matrix",
+            fig2Hint: "Hover to inspect probabilities. Use the search to highlight a specific character's row and column.",
+            readingTitle: "Reading the Matrix",
+            readingText: "Each row represents a \"given\" character. Each column represents a \"next\" character. The brightness of a cell encodes how likely that transition is. Notice how some rows are nearly uniform (the model is unsure) while others have sharp peaks (strong preferences)."
+        },
+        normalization: {
+            title: "From Counts to Probabilities",
+            lead: "Raw counts alone don't tell us much. To make predictions, we need to convert them into probabilities.",
+            p1: "We do this by",
+            p1Highlight: "normalizing each row",
+            p1End: "of the count matrix ÔÇö dividing every count by the total number of transitions from that row's token. After normalization, each row sums to 1.0, forming a valid probability distribution.",
+            p2: "The model can now make concrete statements: \"After the letter h, there is a 32% chance the next character is e, a 15% chance it's a, and so on.\"",
+            p3: "Try it yourself below. Type any text to see what the model predicts will come next ÔÇö based",
+            p3Highlight: "solely on the very last character",
+            p3End: "of your input.",
+            fig: "Interactive ┬À Next-Token Predictions",
+            figHint: "The model considers only the last character ÔÇö it has no memory of earlier context."
+        },
+        generation: {
+            title: "Generating New Text",
+            lead: "Once we have a probability distribution, we can do something remarkable: generate entirely new text.",
+            p1: "The process is called",
+            p1Highlight: "autoregressive sampling",
+            p1End: ". Start with a seed character, sample the next one from its probability distribution, then use that new character as the seed for the next step. Repeat indefinitely.",
+            tempTitle: "Temperature",
+            tempText: "The temperature parameter controls how \"creative\" the generation is. At low temperatures, the model almost always picks the most likely next token. At high temperatures, it samples more uniformly ÔÇö producing surprising and often nonsensical output.",
+            p2: "Generate some text below and observe how a model with",
+            p2Highlight: "only one character of memory",
+            p2End: "produces output that is statistically plausible at the character level, yet meaningless at any higher level.",
+            fig: "Interactive ┬À Text Generation",
+            figHint: "Try different temperatures. Low = predictable, High = chaotic."
+        },
+        limitations: {
+            title: "Power and Limitations",
+            lead: "The Bigram model is powerful precisely because of its simplicity.",
+            p1: "It requires very few parameters ÔÇö just a V ├ù V matrix, where V is the vocabulary size. It trains instantly. And it provides a clear",
+            p1Highlight: "probabilistic baseline",
+            p1End: "for language generation that every more sophisticated model must beat.",
+            limitationTitle: "The Fundamental Limitation",
+            limitationText: "The model has no memory beyond a single token. It cannot learn that \"th\" is often followed by \"e\", because by the time it sees \"h\", it has already forgotten the \"t\". It captures local co-occurrence but nothing about words, phrases, or meaning.",
+            p2: "This limitation is exactly what motivates the progression to more sophisticated architectures:",
+            p2H1: "N-grams",
+            p2H2: "MLPs",
+            p2H3: "Transformers",
+            p2End: "attend to the entire sequence at once.",
+            quote: "Each model in this lab builds on the same core question: given context, what comes next?"
+        },
+        footer: {
+            text: "Continue exploring the other models in the lab to see how each one addresses the limitations of its predecessor.",
+            brand: "LM-Lab ┬À Educational Mode"
+        }
+    },
+    bigramBuilder: {
+        description: "We build the bigram matrix by scanning the text character by character. For each pair of consecutive characters (current ÔåÆ next), we increment the cell [current, next]. This table captures how often each character is followed by another.",
+        placeholder: "Type text here...",
+        hint: "Enter some text to see how the bigram matrix is constructed.",
+        buttons: {
+            build: "Build Bigram Matrix",
+            next: "Next Step",
+            autoPlay: "Auto Play",
+            pause: "Pause",
+            instant: "Instant Complete",
+            reset: "Reset Steps"
+        },
+        vocab: "Educational vocabulary",
+        normalized: "Normalized text:",
+        empty: "(empty after filtering)",
+        skipped: "Showing the first {max} unique characters for clarity ({count} unique character(s) omitted).",
+        step1: "Step",
+        step2: "updates cell [",
+        step3: "].",
+        pressBuild: "Press Build Bigram Matrix and start stepping through character pairs.",
+        table: {
+            curnxt: "cur \\ nxt"
+        }
+    }
 };
diff --git a/src/i18n/es.ts b/src/i18n/es.ts
index 3e50742..35739cf 100644
--- a/src/i18n/es.ts
+++ b/src/i18n/es.ts
@@ -585,4 +585,121 @@ export const es: TranslationDictionary = {
             },
         },
     }
+    bigramNarrative: {
+        hero: {
+            eyebrow: "Entendiendo los Modelos de Lenguaje",
+            titlePrefix: "El Modelo",
+            titleSuffix: "Bigrama",
+            description: "Una exploraci├│n desde primeros principios del modelo de lenguaje estad├¡stico m├ís simple ÔÇö y por qu├® todav├¡a importa."
+        },
+        problem: {
+            title: "El Problema de la Predicci├│n",
+            lead: "El lenguaje es fundamentalmente secuencial. Cada palabra que lees ahora mismo est├í influenciada por las palabras que la precedieron.",
+            p1: "Esta propiedad ÔÇö que cada token en una secuencia conlleva",
+            p1Highlight: "expectativas sobre lo que sigue",
+            p1End: "ÔÇö es lo que hace que el lenguaje sea a la vez expresivo y predecible. Tambi├®n es lo que hace que sea tan dif├¡cil de modelar computacionalmente.",
+            p2: "El desaf├¡o central del modelado de lenguaje es enga├▒osamente simple de plantear:",
+            quote: "Dado lo que ya hemos visto, ┬┐qu├® deber├¡a venir a continuaci├│n?",
+            p3: "Esta pregunta ha impulsado d├®cadas de investigaci├│n en",
+            p3H1: "ling├╝├¡stica computacional",
+            p3H2: "teor├¡a de la informaci├│n",
+            p3H3: "aprendizaje profundo",
+            p3End: ". Para construir un modelo que pueda responderla, necesitamos una forma de capturar la estructura estad├¡stica del lenguaje. Empecemos con el enfoque m├ís simple posible.",
+        },
+        idea: {
+            title: "La Idea Estad├¡stica M├ís Simple",
+            lead: "┬┐Qu├® pasar├¡a si, en lugar de intentar entender el significado, simplemente observ├íramos patrones?",
+            p1: "Espec├¡ficamente:",
+            p1Highlight: "┬┐con qu├® frecuencia un car├ícter sigue a otro?",
+            p1End: "Esta es la idea central detr├ís del modelo Bigrama. Ignora por completo la gram├ítica, la sem├íntica y las dependencias a largo plazo. Solo hace una pregunta: dado el token actual, ┬┐cu├íl es la distribuci├│n de probabilidad sobre el siguiente token?",
+            caption: "La suposici├│n del Bigrama: el siguiente token depende ├║nicamente del actual.",
+            p2: "Modelamos P(x_{t+1} | x_t) ÔÇö la probabilidad de ver un pr├│ximo token particular dado solo el token que acabamos de observar. Nada m├ís, nada menos. Esta simplificaci├│n radical es lo que hace al modelo tanto tratable como limitado.",
+            insightTitle: "Idea Clave",
+            insightText: "El prefijo \"bi\" en Bigrama significa dos. El modelo considera pares de tokens ÔÇö el actual y el siguiente. No tiene memoria de nada antes del token actual."
+        },
+        table: {
+            title: "Construyendo una Tabla de Transici├│n",
+            lead: "Para aprender estas probabilidades, el modelo escanea un corpus de entrenamiento y cuenta cada par de tokens consecutivos.",
+            p1: "Por cada token A, registra con qu├® frecuencia cada posible token B aparece inmediatamente despu├®s. Estos recuentos forman una",
+            p1Highlight: "matriz",
+            p1End: "ÔÇö una tabla bidimensional donde las filas representan el token actual y las columnas el siguiente token. Cada celda contiene el n├║mero de veces que se observ├│ esa transici├│n espec├¡fica en los datos de entrenamiento.",
+            p2: "Comienza con el constructor educativo a continuaci├│n para ver cada actualizaci├│n paso a paso. Luego comp├íralo con la tabla de transici├│n completa renderizada desde el modelo.",
+            fig1: "Interactivo ┬À Constructor de Matrices Paso a Paso",
+            fig1Hint: "Empieza aqu├¡: construye una peque├▒a matriz de conteo con tu propio texto y sigue cada actualizaci├│n de pares.",
+            fig2: "Interactivo ┬À Matriz de Transici├│n",
+            fig2Hint: "Pasa el rat├│n para inspeccionar las probabilidades. Usa la b├║squeda para resaltar la fila y columna de un car├ícter espec├¡fico.",
+            readingTitle: "Leyendo la Matriz",
+            readingText: "Cada fila representa un car├ícter \"dado\". Cada columna representa un car├ícter \"siguiente\". El brillo de una celda codifica qu├® tan probable es esa transici├│n. Nota c├│mo algunas filas son casi uniformes (el modelo no est├í seguro) mientras que otras tienen picos pronunciados (preferencias fuertes)."
+        },
+        normalization: {
+            title: "De Conteos a Probabilidades",
+            lead: "Los conteos crudos por s├¡ solos no nos dicen mucho. Para hacer predicciones, necesitamos convertirlos en probabilidades.",
+            p1: "Hacemos esto",
+            p1Highlight: "normalizando cada fila",
+            p1End: "de la matriz de conteos ÔÇö dividiendo cada conteo por el n├║mero total de transiciones desde el token de esa fila. Despu├®s de la normalizaci├│n, cada fila suma 1.0, formando una distribuci├│n de probabilidad v├ílida.",
+            p2: "El modelo ahora puede hacer declaraciones concretas: \"Despu├®s de la letra h, hay un 32% de probabilidad de que el siguiente car├ícter sea e, un 15% de que sea a, y as├¡ sucesivamente.\"",
+            p3: "Pru├®balo t├║ mismo a continuaci├│n. Escribe cualquier texto para ver qu├® predice el modelo que vendr├í despu├®s ÔÇö basado",
+            p3Highlight: "├║nicamente en el ultim├¡simo car├ícter",
+            p3End: "de tu entrada.",
+            fig: "Interactivo ┬À Predicciones del Siguiente Token",
+            figHint: "El modelo considera solo el ├║ltimo car├ícter ÔÇö no tiene memoria del contexto anterior."
+        },
+        generation: {
+            title: "Generando Nuevo Texto",
+            lead: "Una vez que tenemos una distribuci├│n de probabilidad, podemos hacer algo notable: generar texto completamente nuevo.",
+            p1: "El proceso se llama",
+            p1Highlight: "muestreo autorregresivo",
+            p1End: ". Comienza con un car├ícter semilla, extrae el siguiente de su distribuci├│n de probabilidad, y luego usa ese nuevo car├ícter como semilla para el siguiente paso. Repite indefinidamente.",
+            tempTitle: "Temperatura",
+            tempText: "El par├ímetro de temperatura controla cu├ín \"creativa\" es la generaci├│n. A bajas temperaturas, el modelo casi siempre elige el siguiente token m├ís probable. A altas temperaturas, el muestreo es m├ís uniforme ÔÇö produciendo resultados sorprendentes y a menudo sin sentido.",
+            p2: "Genera algo de texto a continuaci├│n y observa c├│mo un modelo con",
+            p2Highlight: "solo un car├ícter de memoria",
+            p2End: "produce una salida que es estad├¡sticamente plausible a nivel de car├ícter, pero sin sentido en niveles superiores.",
+            fig: "Interactivo ┬À Generaci├│n de Texto",
+            figHint: "Prueba diferentes temperaturas. Baja = predecible, Alta = ca├│tica."
+        },
+        limitations: {
+            title: "Poder y Limitaciones",
+            lead: "El modelo Bigrama es poderoso precisamente por su simplicidad.",
+            p1: "Requiere muy pocos par├ímetros ÔÇö solo una matriz V ├ù V, donde V es el tama├▒o del vocabulario. Entrena instant├íneamente. Y proporciona una clara",
+            p1Highlight: "l├¡nea base probabil├¡stica",
+            p1End: "para la generaci├│n de lenguaje que cualquier modelo m├ís sofisticado debe superar.",
+            limitationTitle: "La Limitaci├│n Fundamental",
+            limitationText: "El modelo no tiene memoria m├ís all├í de un solo token. No puede aprender que \"th\" es a menudo seguido por \"e\", porque para cuando ve la \"h\", ya ha olvidado la \"t\". Captura la co-ocurrencia local pero nada sobre palabras, frases o significado.",
+            p2: "Esta limitaci├│n es exactamente lo que motiva la progresi├│n a arquitecturas m├ís sofisticadas:",
+            p2H1: "N-gramas",
+            p2H2: "MLPs",
+            p2H3: "Transformers",
+            p2End: "atienden a toda la secuencia a la vez.",
+            quote: "Cada modelo en este laboratorio se basa en la misma pregunta central: dado un contexto, ┬┐qu├® viene despu├®s?"
+        },
+        footer: {
+            text: "Contin├║a explorando los otros modelos en el laboratorio para ver c├│mo cada uno aborda las limitaciones de su predecesor.",
+            brand: "LM-Lab ┬À Modo Educativo"
+        }
+    },
+    bigramBuilder: {
+        description: "Construimos la matriz bigrama escaneando el texto car├ícter por car├ícter. Por cada par de caracteres consecutivos (actual ÔåÆ siguiente), incrementamos la celda [actual, siguiente]. Esta tabla captura con qu├® frecuencia un car├ícter es seguido por otro.",
+        placeholder: "Escribe texto aqu├¡...",
+        hint: "Introduce alg├║n texto para ver c├│mo se construye la matriz bigrama.",
+        buttons: {
+            build: "Construir Matriz",
+            next: "Siguiente Paso",
+            autoPlay: "Auto-Reproducir",
+            pause: "Pausar",
+            instant: "Completar",
+            reset: "Reiniciar"
+        },
+        vocab: "Vocabulario educativo",
+        normalized: "Texto normalizado:",
+        empty: "(vac├¡o tras filtrar)",
+        skipped: "Mostrando los primeros {max} caracteres ├║nicos por claridad (omitiendo {count} car├ícter(es) ├║nico(s)).",
+        step1: "Paso",
+        step2: "actualiza celda [",
+        step3: "].",
+        pressBuild: "Pulsa Construir Matriz y empieza a iterar sobre los pares de caracteres.",
+        table: {
+            curnxt: "act \\ sig"
+        }
+    }
 };
"use client";

import { LabShell } from "@/components/lab/LabShell";
import { ModelHero } from "@/components/lab/ModelHero";
import dynamic from "next/dynamic";

import { useNgramVisualization } from "@/hooks/useNgramVisualization";
import { useNgramStepwise } from "@/hooks/useNgramStepwise";
import { useNgramGeneration } from "@/hooks/useNgramGeneration";
import { visualizeNgram } from "@/lib/lmLabClient";
import { motion, AnimatePresence } from "framer-motion";
import {
    FlaskConical,
    Database,
    Hash,
    Activity,
    Zap,
    TrendingDown,
    BarChart3,
    Eye,
    Layers,
    Type,
    Sparkles,
    AlertTriangle,
} from "lucide-react";
import { useEffect, useCallback, useRef, useState, useMemo } from "react";
import { useI18n } from "@/i18n/context";
import { useLabMode, LabModeProvider } from "@/context/LabModeContext";

const ContextControl = dynamic(() =>
    import("@/components/lab/ContextControl").then((m) => m.ContextControl)
);
const TransitionMatrix = dynamic(() =>
    import("@/components/lab/TransitionMatrix").then((m) => m.TransitionMatrix)
);
const InferenceConsole = dynamic(() =>
    import("@/components/lab/InferenceConsole").then((m) => m.InferenceConsole)
);
const StepwisePrediction = dynamic(() =>
    import("@/components/lab/StepwisePrediction").then((m) => m.StepwisePrediction)
);
const GenerationPlayground = dynamic(() =>
    import("@/components/lab/GenerationPlayground").then((m) => m.GenerationPlayground)
);
const NgramFiveGramScale = dynamic(() =>
    import("@/components/lab/NgramPedagogyPanels").then((m) => m.NgramFiveGramScale)
);
const NgramNarrative = dynamic(() =>
    import("@/components/lab/NgramNarrative").then((m) => m.NgramNarrative)
);

/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ
   Lab section wrapper
   ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */

function LabSection({
    icon: Icon,
    title,
    description,
    children,
    accent = "cyan",
}: {
    icon: React.ComponentType<{ className?: string }>;
    title: string;
    description: string;
    children: React.ReactNode;
    accent?: "cyan" | "violet" | "amber" | "emerald" | "red";
}) {
    const accentMap = {
        cyan: { icon: "text-cyan-400", bg: "bg-cyan-500/15", border: "border-cyan-500/20", bar: "bg-cyan-400" },
        violet: { icon: "text-violet-400", bg: "bg-violet-500/15", border: "border-violet-500/20", bar: "bg-violet-400" },
        amber: { icon: "text-amber-400", bg: "bg-amber-500/15", border: "border-amber-500/20", bar: "bg-amber-400" },
        emerald: { icon: "text-emerald-400", bg: "bg-emerald-500/15", border: "border-emerald-500/20", bar: "bg-emerald-400" },
        red: { icon: "text-red-400", bg: "bg-red-500/15", border: "border-red-500/20", bar: "bg-red-400" },
    };
    const a = accentMap[accent];

    return (
        <motion.div
            initial={{ opacity: 0, y: 16 }}
            whileInView={{ opacity: 1, y: 0 }}
            viewport={{ once: true, margin: "-60px" }}
            transition={{ duration: 0.5 }}
        >
            <div className={`rounded-2xl border ${a.border} bg-gradient-to-br from-white/[0.02] to-black/20 overflow-hidden`}>
                <div className="flex items-center gap-3 px-5 py-3.5 border-b border-white/[0.06] bg-white/[0.015]">
                    <div className={`p-1.5 rounded-lg ${a.bg}`}>
                        <Icon className={`w-4 h-4 ${a.icon}`} />
                    </div>
                    <div className="flex-1 min-w-0">
                        <h3 className="text-sm font-bold text-white tracking-tight">{title}</h3>
                        <p className="text-[10px] text-white/35 truncate">{description}</p>
                    </div>
                </div>
                <div className="p-5">{children}</div>
            </div>
        </motion.div>
    );
}

/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ
   Sparsity indicator
   ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */

function SparsityIndicator({
    training,
    diagnostics,
}: {
    training: { unique_contexts: number; context_utilization: number; sparsity: number; transition_density: number } | null;
    diagnostics: { estimated_context_space: number; context_size: number } | null;
}) {
    if (!training || !diagnostics) return null;
    const utilPct = (training.context_utilization * 100);
    const sparsityPct = (training.sparsity * 100);

    return (
        <div className="space-y-4">
            <div className="grid grid-cols-2 gap-3">
                <div className="bg-black/30 rounded-xl p-4 border border-white/[0.06]">
                    <p className="text-[10px] font-mono uppercase tracking-widest text-white/30 mb-1">Observed Contexts</p>
                    <p className="text-xl font-bold text-cyan-300 font-mono">{training.unique_contexts.toLocaleString()}</p>
                    <p className="text-[10px] text-white/25 mt-1">of {diagnostics.estimated_context_space.toLocaleString()} possible</p>
                </div>
                <div className="bg-black/30 rounded-xl p-4 border border-white/[0.06]">
                    <p className="text-[10px] font-mono uppercase tracking-widest text-white/30 mb-1">Avg. Transitions / Context</p>
                    <p className="text-xl font-bold text-emerald-300 font-mono">{training.transition_density.toFixed(1)}</p>
                    <p className="text-[10px] text-white/25 mt-1">next-tokens per observed context</p>
                </div>
            </div>

            <div className="space-y-3">
                <div>
                    <div className="flex justify-between text-[10px] mb-1.5">
                        <span className="font-mono uppercase tracking-widest text-white/30">Context utilization</span>
                        <span className="font-mono text-cyan-400">{utilPct.toFixed(2)}%</span>
                    </div>
                    <div className="h-2 rounded-full bg-white/[0.06] overflow-hidden">
                        <motion.div
                            initial={{ width: 0 }}
                            animate={{ width: `${Math.max(1, utilPct)}%` }}
                            transition={{ duration: 0.8 }}
                            className="h-full rounded-full bg-gradient-to-r from-cyan-600/70 to-cyan-400/80"
                        />
                    </div>
                    <p className="text-[10px] text-white/20 mt-1">Fraction of possible contexts seen in training data</p>
                </div>
                <div>
                    <div className="flex justify-between text-[10px] mb-1.5">
                        <span className="font-mono uppercase tracking-widest text-white/30">Table sparsity</span>
                        <span className="font-mono text-red-400">{sparsityPct.toFixed(1)}%</span>
                    </div>
                    <div className="h-2 rounded-full bg-white/[0.06] overflow-hidden">
                        <motion.div
                            initial={{ width: 0 }}
                            animate={{ width: `${sparsityPct}%` }}
                            transition={{ duration: 0.8 }}
                            className="h-full rounded-full bg-gradient-to-r from-red-600/70 to-red-400/80"
                        />
                    </div>
                    <p className="text-[10px] text-white/20 mt-1">Fraction of (context, next-token) pairs never observed</p>
                </div>
            </div>
        </div>
    );
}

/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ
   Training loss chart
   ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */

function LossChart({
    lossHistory,
    perplexity,
    finalLoss,
}: {
    lossHistory: number[];
    perplexity?: number;
    finalLoss?: number;
}) {
    const maxLoss = Math.max(...lossHistory);
    const minLoss = Math.min(...lossHistory);
    const range = maxLoss - minLoss || 1;

    const points = lossHistory.map((v, i) => {
        const x = (i / (lossHistory.length - 1)) * 100;
        const y = 100 - ((v - minLoss) / range) * 90 - 5;
        return `${x},${y}`;
    }).join(" ");

    return (
        <div className="space-y-4">
            <div className="bg-black/30 rounded-xl p-4 border border-white/[0.06]">
                <div className="flex items-center justify-between mb-3">
                    <p className="text-[10px] font-mono uppercase tracking-widest text-white/30">Training loss (NLL)</p>
                    <div className="flex gap-4 text-[10px] font-mono text-white/40">
                        {finalLoss != null && <span>Final: <span className="text-emerald-400">{finalLoss.toFixed(3)}</span></span>}
                        {perplexity != null && <span>PPL: <span className="text-amber-400">{perplexity.toFixed(1)}</span></span>}
                    </div>
                </div>
                <svg viewBox="0 0 100 100" className="w-full h-32" preserveAspectRatio="none">
                    <defs>
                        <linearGradient id="lossGradient" x1="0%" y1="0%" x2="0%" y2="100%">
                            <stop offset="0%" stopColor="rgb(6,182,212)" stopOpacity="0.3" />
                            <stop offset="100%" stopColor="rgb(6,182,212)" stopOpacity="0" />
                        </linearGradient>
                    </defs>
                    <polyline
                        points={points}
                        fill="none"
                        stroke="rgb(6,182,212)"
                        strokeWidth="0.8"
                        strokeLinejoin="round"
                        vectorEffect="non-scaling-stroke"
                    />
                    <polygon
                        points={`0,100 ${points} 100,100`}
                        fill="url(#lossGradient)"
                    />
                </svg>
                <div className="flex justify-between text-[10px] text-white/20 font-mono mt-1">
                    <span>Start</span>
                    <span>Training progress</span>
                    <span>End</span>
                </div>
            </div>

            <div className="grid grid-cols-2 gap-3">
                <div className="bg-black/30 rounded-lg p-3 border border-white/[0.06]">
                    <p className="text-[10px] font-mono uppercase tracking-widest text-white/30 mb-1">Perplexity</p>
                    <p className="text-lg font-bold text-amber-300 font-mono">{perplexity?.toFixed(1) ?? "ÔÇö"}</p>
                    <p className="text-[10px] text-white/20 mt-0.5">Lower = more confident predictions</p>
                </div>
                <div className="bg-black/30 rounded-lg p-3 border border-white/[0.06]">
                    <p className="text-[10px] font-mono uppercase tracking-widest text-white/30 mb-1">Final NLL</p>
                    <p className="text-lg font-bold text-emerald-300 font-mono">{finalLoss?.toFixed(3) ?? "ÔÇö"}</p>
                    <p className="text-[10px] text-white/20 mt-0.5">Negative log-likelihood on train data</p>
                </div>
            </div>
        </div>
    );
}

/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ
   Comparison metrics overlay for multiple N
   ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */

function ComparisonDashboard({
    metrics,
    currentN,
}: {
    metrics: Record<number, { perplexity: number | null; contextUtilization: number | null; contextSpace: number | null }>;
    currentN: number;
}) {
    const ns = [1, 2, 3, 4, 5];
    return (
        <div className="space-y-2">
            {ns.map((n) => {
                const m = metrics[n];
                const isActive = n === currentN;
                return (
                    <div
                        key={n}
                        className={`flex items-center gap-3 rounded-lg border px-4 py-2.5 transition-colors ${isActive
                                ? "border-cyan-500/30 bg-cyan-500/[0.06]"
                                : "border-white/[0.06] bg-white/[0.015]"
                            }`}
                    >
                        <span className={`font-mono text-xs font-bold w-12 ${isActive ? "text-cyan-300" : "text-white/40"}`}>
                            N={n}
                        </span>
                        <div className="flex-1 grid grid-cols-3 gap-2 text-[10px] font-mono">
                            <div>
                                <span className="text-white/25">PPL </span>
                                <span className={isActive ? "text-amber-300" : "text-white/50"}>
                                    {m?.perplexity != null ? m.perplexity.toFixed(1) : "ÔÇö"}
                                </span>
                            </div>
                            <div>
                                <span className="text-white/25">Util </span>
                                <span className={isActive ? "text-emerald-300" : "text-white/50"}>
                                    {m?.contextUtilization != null ? `${(m.contextUtilization * 100).toFixed(1)}%` : "ÔÇö"}
                                </span>
                            </div>
                            <div>
                                <span className="text-white/25">Space </span>
                                <span className={isActive ? "text-purple-300" : "text-white/50"}>
                                    {m?.contextSpace != null ? m.contextSpace.toLocaleString() : "ÔÇö"}
                                </span>
                            </div>
                        </div>
                    </div>
                );
            })}
        </div>
    );
}

/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ
   Page component
   ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */

function NgramPageContent() {
    const { t } = useI18n();
    const { mode } = useLabMode();
    const isEdu = mode === "educational";
    const viz = useNgramVisualization();
    const stepwise = useNgramStepwise(viz.contextSize);
    const generation = useNgramGeneration(viz.contextSize);
    const [comparisonMetrics, setComparisonMetrics] = useState<Record<number, {
        perplexity: number | null;
        contextUtilization: number | null;
        contextSpace: number | null;
    }>>({});

    const lastTextRef = useRef<string>("hello");

    useEffect(() => {
        if (!viz.data && !viz.loading && !viz.error) {
            viz.analyze("hello", 10);
        }
    }, []); // eslint-disable-line react-hooks/exhaustive-deps

    useEffect(() => {
        if (lastTextRef.current && viz.contextSize < 5) {
            viz.analyze(lastTextRef.current, 10);
        }
    }, [viz.contextSize]); // eslint-disable-line react-hooks/exhaustive-deps

    useEffect(() => {
        let active = true;
        const timer = setTimeout(async () => {
            const entries = await Promise.all(
                Array.from({ length: 5 }, async (_, index) => {
                    const n = index + 1;
                    try {
                        const res = await visualizeNgram("hello", n, 5);
                        return [n, {
                            perplexity: res.visualization.training?.perplexity ?? res.visualization.diagnostics?.perplexity ?? null,
                            contextUtilization: res.visualization.training?.context_utilization ?? res.visualization.diagnostics?.context_utilization ?? null,
                            contextSpace: res.visualization.training?.context_space_size ?? res.visualization.diagnostics?.estimated_context_space ?? null,
                        }] as const;
                    } catch {
                        return [n, { perplexity: null, contextUtilization: null, contextSpace: null }] as const;
                    }
                })
            );
            if (active) setComparisonMetrics(Object.fromEntries(entries) as typeof comparisonMetrics);
        }, 3000);
        return () => { active = false; clearTimeout(timer); };
    }, []);

    const handleAnalyze = useCallback((text: string, topK: number) => {
        lastTextRef.current = text;
        viz.analyze(text, topK);
    }, [viz.analyze]);

    const nGramData = viz.data;
    const diagnostics = nGramData?.visualization.diagnostics ?? null;
    const training = nGramData?.visualization.training ?? null;
    const activeSlice = nGramData?.visualization.active_slice;
    const contextDistributions = nGramData?.visualization.context_distributions;
    const vocabForScalability = diagnostics?.vocab_size ?? nGramData?.metadata.vocab_size ?? 96;
    const fallbackCurrent = contextDistributions?.current;
    const fallbackSliceMatrix = useMemo(() =>
        fallbackCurrent?.probabilities
            ? {
                shape: [1, fallbackCurrent.probabilities.length],
                data: [fallbackCurrent.probabilities],
                row_labels: [fallbackCurrent.context || "current"],
                col_labels: fallbackCurrent.row_labels ?? Array.from({ length: fallbackCurrent.probabilities.length }, (_, i) => `#${i}`),
            }
            : null,
        [fallbackCurrent]
    );

    /* ÔöÇÔöÇÔöÇÔöÇÔöÇ Educational Mode: full narrative ÔöÇÔöÇÔöÇÔöÇÔöÇ */
    if (isEdu) {
        return (
            <LabShell>
                <NgramNarrative
                    contextSize={viz.contextSize}
                    vocabSize={vocabForScalability}
                    comparisonMetrics={comparisonMetrics}
                />
            </LabShell>
        );
    }

    /* ÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉ
       FREE LAB MODE
       ÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉÔòÉ */

    const heroStats = diagnostics ? [
        {
            label: "Unique Contexts",
            value: training?.unique_contexts?.toLocaleString() ?? "?",
            icon: Activity,
            desc: t("models.ngram.hero.stats.uniqueContexts.desc"),
            color: "cyan"
        },
        {
            label: "Vocabulary",
            value: diagnostics.vocab_size?.toString() ?? "?",
            icon: Database,
            desc: "Unique characters",
            color: "blue"
        },
        {
            label: "Context Space",
            value: diagnostics.estimated_context_space?.toLocaleString() ?? "?",
            icon: Hash,
            desc: `|V|^${diagnostics.context_size}`,
            color: "purple"
        },
        {
            label: "Training Tokens",
            value: training ? `${(training.total_tokens / 1000).toFixed(1)}k` : "?",
            icon: FlaskConical,
            desc: "Total tokens seen",
            color: "emerald"
        },
    ] : undefined;

    const hasLossHistory = training?.loss_history && training.loss_history.length > 1;

    return (
        <LabShell>
            <div className="max-w-7xl mx-auto pb-24 relative">

                {/* HERO */}
                <ModelHero
                    title="N-Gram Language Model"
                    description="A character-level statistical language model with variable context size. Visualize how increasing the context window sharpens predictions at the cost of exponential sparsity."
                    customStats={heroStats}
                    showExplanationCta={false}
                />

                {/* Lab mode badge */}
                <motion.div
                    initial={{ opacity: 0 }}
                    animate={{ opacity: 1 }}
                    className="max-w-6xl mx-auto px-6 mb-8 flex items-center gap-3"
                >
                    <Zap className="w-4 h-4 text-cyan-400" />
                    <p className="text-xs uppercase tracking-[0.15em] text-cyan-300/60 font-bold">
                        Free Lab Mode ┬À Full instrument access
                    </p>
                </motion.div>

                {/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ
                   ROW 1: Context Controller (full width)
                   ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */}
                <div className="max-w-6xl mx-auto px-6 mb-8">
                    <ContextControl
                        value={viz.contextSize}
                        onChange={viz.setContextSize}
                        disabled={viz.loading}
                    />
                </div>

                {/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ
                   ROW 2: Transition Matrix + Sparsity / Metrics
                   ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */}
                <div className="max-w-6xl mx-auto px-6 mb-8 grid grid-cols-1 lg:grid-cols-5 gap-6">
                    {/* Matrix ÔÇö wide */}
                    <div className="lg:col-span-3">
                        <LabSection
                            icon={Eye}
                            title="Transition Probabilities"
                            description={viz.contextSize === 1
                                ? "Full bigram matrix P(next | current)"
                                : `Slice P(next | "${activeSlice?.context_tokens?.join("") ?? "..."}")`
                            }
                        >
                            {viz.contextSize >= 5 ? (
                                <NgramFiveGramScale vocabSize={vocabForScalability} />
                            ) : (
                                <AnimatePresence mode="wait">
                                    <motion.div
                                        key={`matrix-${viz.contextSize}`}
                                        initial={{ opacity: 0 }}
                                        animate={{ opacity: 1 }}
                                        exit={{ opacity: 0 }}
                                        transition={{ duration: 0.3 }}
                                    >
                                        <TransitionMatrix
                                            data={
                                                viz.contextSize === 1
                                                    ? nGramData?.visualization.transition_matrix ?? null
                                                    : activeSlice?.matrix ?? fallbackSliceMatrix
                                            }
                                            activeContext={viz.contextSize > 1 ? activeSlice?.context_tokens : undefined}
                                            accent="cyan"
                                        />
                                        {viz.contextSize > 1 && (
                                            <div className="mt-3 flex items-center gap-2 px-1 text-cyan-300/60">
                                                <span className="text-[10px] uppercase tracking-[0.15em] font-bold">Conditioned on:</span>
                                                <span className="font-mono text-sm text-white/70">
                                                    &quot;{activeSlice?.context_tokens?.join("") ?? fallbackCurrent?.context ?? "..."}&quot;
                                                </span>
                                            </div>
                                        )}
                                    </motion.div>
                                </AnimatePresence>
                            )}
                        </LabSection>
                    </div>

                    {/* Right column: sparsity + model metrics */}
                    <div className="lg:col-span-2 space-y-6">
                        <LabSection
                            icon={BarChart3}
                            title="Data Sparsity"
                            description="How much of the context space is actually observed"
                            accent="red"
                        >
                            <SparsityIndicator training={training} diagnostics={diagnostics} />
                        </LabSection>

                        <LabSection
                            icon={Layers}
                            title="Model Comparison"
                            description="Metrics across N=1..5 from backend"
                            accent="violet"
                        >
                            <ComparisonDashboard metrics={comparisonMetrics} currentN={viz.contextSize} />
                        </LabSection>
                    </div>
                </div>

                {/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ
                   ROW 3: Loss Chart (if available)
                   ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */}
                {hasLossHistory && (
                    <div className="max-w-6xl mx-auto px-6 mb-8">
                        <LabSection
                            icon={TrendingDown}
                            title="Training Quality"
                            description={`Loss curve for the N=${viz.contextSize} model during training`}
                            accent="emerald"
                        >
                            <LossChart
                                lossHistory={training!.loss_history!}
                                perplexity={training!.perplexity}
                                finalLoss={training!.final_loss}
                            />
                        </LabSection>
                    </div>
                )}

                {/* ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ
                   ROW 4: Inference + Stepwise + Generation
                   ÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇÔöÇ */}
                <div className="max-w-6xl mx-auto px-6 mb-8">
                    {viz.contextSize >= 5 ? (
                        <motion.div
                            initial={{ opacity: 0 }}
                            animate={{ opacity: 1 }}
                            className="p-6 rounded-2xl border border-red-500/25 bg-gradient-to-br from-red-950/20 to-black/60 flex items-center gap-4"
                        >
                            <AlertTriangle className="w-5 h-5 text-red-400 shrink-0" />
                            <div>
                                <p className="text-sm text-red-100/80 font-medium">Combinatorial threshold exceeded</p>
                                <p className="text-xs text-red-200/50 mt-0.5">
                                    N=5 produces over {Math.pow(vocabForScalability, 5).toLocaleString()} possible contexts.
                                    Reduce N to 1ÔÇô4 for live inference, stepwise prediction, and generation.
                                </p>
                            </div>
                        </motion.div>
                    ) : (
                        <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
                            {/* Left: Next-token prediction */}
                            <LabSection
                                icon={Type}
                                title="Next-Token Prediction"
                                description="Type text and see the probability distribution over next characters"
                            >
                                <InferenceConsole
                                    onAnalyze={handleAnalyze}
                                    predictions={nGramData?.predictions ?? null}
                                    inferenceMs={nGramData?.metadata.inference_time_ms}
                                    device={nGramData?.metadata.device}
                                    loading={viz.loading}
                                    error={viz.error}
                                />
                            </LabSection>

                            {/* Right: Stepwise prediction */}
                            <LabSection
                                icon={Activity}
                                title="Stepwise Prediction"
                                description="Trace the context window sliding character by character"
                                accent="violet"
                            >
                                <StepwisePrediction
                                    onPredict={stepwise.predict}
                                    steps={stepwise.data?.steps ?? null}
                                    finalPrediction={stepwise.data?.final_prediction ?? null}
                                    loading={stepwise.loading}
                                    error={stepwise.error}
                                />
                            </LabSection>
                        </div>
                    )}
                </div>

                {/* Generation (full width, below inference) */}
                {viz.contextSize < 5 && (
                    <div className="max-w-6xl mx-auto px-6 mb-8">
                        <LabSection
                            icon={Sparkles}
                            title="Text Generation"
                            description="Generate text auto-regressively using the current N-gram model"
                            accent="amber"
                        >
                            <GenerationPlayground
                                onGenerate={generation.generate}
                                generatedText={generation.data?.generated_text ?? null}
                                loading={generation.loading}
                                error={generation.error}
                            />
                        </LabSection>
                    </div>
                )}

                {/* Footer */}
                <motion.div
                    initial={{ opacity: 0 }}
                    whileInView={{ opacity: 1 }}
                    viewport={{ once: true }}
                    className="mt-16 flex items-center justify-center gap-2 text-[10px] font-mono uppercase tracking-widest text-cyan-300/15"
                >
                    <FlaskConical className="h-3 w-3" />
                    LM-Lab ┬À Scientific Instrument v1.0
                </motion.div>
            </div>
        </LabShell>
    );
}

export default function NgramPage() {
    return (
        <LabModeProvider>
            <NgramPageContent />
        </LabModeProvider>
    );
}
