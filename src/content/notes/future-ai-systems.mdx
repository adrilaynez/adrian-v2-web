---
title: "The Future of AI Systems"
date: "2024-05-20"
description: "Thoughts on scaling laws, sparse attention, and the next generation of inference engines."
tags: ["AI", "Systems", "Thought"]
---

# The Future of AI Systems

As models grow larger, the bottleneck inevitably shifts from **compute** to **memory bandwidth**.

In this note, I explore how sparse attention mechanisms might solve this.

## The Problem with Dense Attention

The standard attention mechanism is $O(N^2)$ in complexity.

$$
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

This is expensive for long sequences.
