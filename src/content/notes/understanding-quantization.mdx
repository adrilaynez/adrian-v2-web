---
title: "Understanding Quantization"
date: "2024-03-10"
description: "Mathematical foundations of INT8 and FP4 quantization for LLMs."
tags: ["AI", "Math", "Optimization"]
---

# Understanding Quantization

Quantization reduces the precision of model weights to save memory.

## Affine Quantization

We map a floating point value $x$ to an integer $q$:

$$
q = round(\frac{x}{S} + Z)
$$

Where $S$ is the scale and $Z$ is the zero-point.

## Symmetric Quantization

If we constrain $Z=0$, we get symmetric quantization:

$$
q = round(\frac{x}{S})
$$

This is faster for SIMD implementations.

## Impact on Perplexity

Lowering precision requires careful calibration to minimize the divergence $D_{KL}(P || Q)$.
