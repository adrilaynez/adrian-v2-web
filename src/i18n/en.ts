export const en = {
    common: {
        language: "Language",
        loading: "Loading...",
        error: "Error",
        comingSoon: "Coming Soon",
        backToProjects: "Back to Projects",
        toggleLanguage: "Toggle Language",
        code: "Code",
        liveDemo: "Live Demo",
        viewCaseStudy: "View Case Study",
    },
    nav: {
        home: "Home",
        projects: "Projects",
        lab: "Lab",
        notes: "Notes",
    },
    lab: {
        bigram: "Bigram",
        ngram: "N-Gram",
        mlp: "MLP",
        transformer: "Transformer",
        neuralNetworks: "Neural Nets",
        shell: {
            allModels: "Back to Lab",
        },
        active: "Lab Active",
        waking: "Waking Up",
        serverWarning: {
            title: "BACKEND COLD-START DETECTED",
            subtitle: "CONTAINMENT PROTOCOL ACTIVE",
            message: "The server is waking up from hibernation. Free-tier Render instances spin down after inactivity — yes, I'm hosting this on a free server because I'm a broke student.",
            donate: "If the 30s wait is too painful, feel free to sponsor my coffee fund so I can afford a real server. Or just wait, it's free entertainment.",
            status: "ATTEMPTING CONNECTION",
            dismiss: "I'LL SURVIVE",
            connected: "SIGNAL ACQUIRED",
        },
        mode: {
            educational: "Educational",
            educationalDescription: "Guided learning experience with story-driven explanations and progressive reveals.",
            freeLab: "Free Lab",
            freeLabDescription: "Full access to all tools and visualizations for manual experimentation and analysis.",
            selectViewingMode: "Select Viewing Mode",
            availableModels: "Available Models",
        },
        status: {
            ready: "Ready",
            coming: "Coming",
        },
        models: {
            bigram: {
                name: "Bigram Explorer",
                description: "First-order analysis and transition matrices.",
            },
            ngram: {
                name: "N-Gram Lab",
                description: "Variable contexts and combinatorial explosion.",
            },
            mlp: {
                name: "MLP Neural",
                description: "Dense representations and neural perceptrons.",
            },
            transformer: {
                name: "Transformer",
                description: "Attention mechanisms and modern LLM architecture.",
            },
        },
        dashboard: {
            chip: "Model Interpretability Lab",
            suite: "Suite",
            description1: "Explore the inner workings of language models through interactive visualizations.",
            description2: "Follow a guided path or experiment freely in the sandbox.",
            launchUnit: "LAUNCH UNIT",
            secureLock: "SECURE LOCK",
            footerCopyright: "© 2026 LM-LAB INSTRUMENTS",
            footerSystem: "INTERPRETABILITY_SYSTEM",
            secureConnection: "Secure Connection",
            hardwareMock: "Hardware: v4-8 TPU MOCK",
        },
        placeholders: {
            mlp: {
                title: "MLP Explorer",
                description: "Multi-Layer Perceptron language model explorer. Currently under development - check back soon.",
            },
            transformer: {
                title: "Transformer Explorer",
                description: "Attention-based transformer model explorer. Currently under development - check back soon.",
            },
        },
        landing: {
            hero: {
                badge: "Research Unit",
                subtitle: "Interactive Interpretability Lab",
                description: "Demystifying Language Models through first-principles engineering and visual proof.",
                subDescription: "This unit focuses on mechanistic interpretability: the reverse-engineering of neural weights into understandable human concepts.",
                start: "Initialize Base Model",
                recommended: "Recommended for beginners",
            },
            highlights: {
                visualizations: "Interactive Viz",
                inference: "Live Inference",
                guided: "Guided Path",
                backend: "PyTorch Backend",
            },
            learningPath: {
                title: "Learning Path",
                status: {
                    soon: "Developing",
                    ready: "Unit Active",
                },
            },
            modes: {
                title: "Laboratory Protocols",
                educational: {
                    title: "Educational Mode",
                    subtitle: "Guided Discovery",
                    description: "Step-by-step narrative explaining the 'why' behind the math. Best for conceptual learning.",
                },
                freeLab: {
                    title: "Free Lab Mode",
                    subtitle: "Sandbox Environment",
                    description: "Full access to all visualization tools and generation parameters. For advanced experimentation.",
                },
            },
            availableModels: {
                title: "Biological Units Available",
                enter: "Enter Lab",
                locked: "Protocol Restricted",
            },
            footer: {
                text: "Scientific Instrument v2.2 // Build 2026",
            },
        },
    },
    training: {
        title: "Training Insights",
        noData: "Run inference to view training data",
        tooltip: {
            lossTitle: "What is Loss?",
            lossErrorPrefix: "Prediction Error:",
            lossError: "Loss measures how 'surprised' the model is. A high loss means it's guessing wrong frequently.",
            lossBenchmarkPrefix: "The Benchmark:",
            lossBenchmark: "Pure random guessing would give a loss of ~4.56 (-ln(1/96)). Anything lower means the model has actually learned something!",
            lossCurve: "The descending curve shows the model slowly discovering patterns in your text.",
        },
        stats: {
            finalLoss: "Final Loss",
            steps: "Steps",
            batchSize: "Batch Size",
            learningRate: "Learning Rate",
            parameters: "Parameters",
            tooltips: {
                finalLoss: "The error level. At the end of training, it should be as low as possible.",
                steps: "How many times the model practiced to improve its predictions.",
                batchSize: "The amount of information pieces the model processes at once.",
                learningRate: "The learning speed. Neither too fast to avoid missing, nor too slow to avoid taking too long.",
                parameters: "The size of the neural network or 'brain' of the model.",
            },
        },
    },
    ngram: {
        training: {
            title: "Training Insights",
            stats: {
                totalTokens: "Total Tokens",
                uniqueContexts: "Unique Contexts",
                utilization: "Context Utilization",
                sparsity: "Sparsity",
                transitionDensity: "Transition Density",
                subs: {
                    possiblePrefix: "of",
                    possibleSuffix: "possible",
                    fractionObserved: "Fraction of contexts observed",
                    unseen: "Unseen context fraction",
                },
            },
        },
    },
    landing: {
        hero: {
            status: "System Online :: v2.2",
            role: "Research & Engineering",
            title: "ADRIAN LAYNEZ ORTIZ",
            tagline1: "Mathematics & Computer Science.",
            tagline2: "Mechanistic Interpretability · High-Performance Engineering.",
            cta: {
                lab: "View Lab Work",
                notes: "Read Notes",
            },
        },
        metrics: {
            research: "Years of Research",
            repos: "Open Source Repos",
            projects: "Active Projects",
            curiosity: "Curiosity",
        },
        about: {
            badge: "About",
            building: "Currently Building",
            projectTitle: "Deep Learning Engine — CUDA / C++",
            projectDesc: "Custom kernels for matrix operations and backpropagation",
            bio: {
                titlePrefix: "Bridging Abstract Mathematics",
                titleSuffix: "& Machine Intelligence",
                p1: "I am pursuing a double degree in <strong class='text-foreground'>Mathematics and Computer Science</strong> at the Universidad Complutense de Madrid. My research focuses on understanding neural networks at their deepest level — from gradient dynamics to kernel-level optimization.",
                p2: "I specialize in <strong class='text-foreground'>Mechanistic Interpretability</strong> — the science of reverse-engineering how neural networks represent and process information internally. Rather than treating models as black boxes, I decompose their circuits to understand <em class='text-foreground/80'>why they work</em>.",
                mission: "My mission: make AI systems transparent through rigorous mathematical analysis and low-level engineering.",
            },
        },
        skills: {
            title: "Technical Proficiencies",
            linearAlgebra: "Linear Algebra",
            topology: "Topology",
            convexOpt: "Convex Optimization",
        },
        work: {
            badge: "Selected Work",
            titlePrefix: "Engineering from",
            titleSuffix: "First Principles",
            description: "Every project begins with a question. From reimplementing seminal papers to writing bare-metal GPU kernels, each one is an exercise in deep understanding.",
            viewAll: "View All Projects",
            items: {
                nanoTransformer: {
                    title: "Nano-Transformer",
                    desc: "Ground-up reproduction of 'Attention Is All You Need' in PyTorch — Multi-Head Attention, Positional Encodings, and LayerNorm implemented without pre-built Transformer modules.",
                },
                cudaKernels: {
                    title: "CUDA Matrix Kernels",
                    desc: "Handwritten CUDA kernels exploring SGEMM optimization — from naive implementations to tiled shared-memory strategies, benchmarked against cuBLAS.",
                },
                autograd: {
                    title: "Autograd Engine",
                    desc: "Lightweight reverse-mode automatic differentiation library. Dynamically constructs computation graphs and propagates gradients via the chain rule.",
                },
                mathDl: {
                    title: "The Mathematics of Deep Learning",
                    desc: "Interactive articles exploring the rigorous theory behind modern AI — SGD convergence analysis, the linear algebra of LoRA, and differential geometry on neural manifolds.",
                },
                distributed: {
                    title: "Distributed Inference",
                    desc: "Architectural explorations in data-parallel training, model sharding, and optimized inference pipelines for large-scale neural networks.",
                },
            },
        },
        contact: {
            badge: "Open to Opportunities",
            titlePrefix: "Let's Build",
            titleMiddle: "Something",
            titleSuffix: "Together",
            description: "Whether it's a research collaboration, an internship opportunity, or just a conversation about the mathematics of intelligence — I'd love to hear from you.",
            email: "Get in Touch",
            github: "GitHub Profile",
        },
    },
    projects: {
        hero: {
            badge: "Research & Development",
            titlePrefix: "Constructing the",
            titleSuffix: "Digital Frontier.",
            description: "A curated collection of my work in distributed systems, AI infrastructure, and high-performance computing.",
        },
        flagship: {
            badge: "Flagship Project",
            featured: "Featured",
            liveDemo: "Live Demo Available",
            title: "LM-Lab",
            description: "An interactive platform for exploring language model architectures from first principles. Visualize transition matrices, probe inference dynamics, and generate text — all powered by a live FastAPI backend with PyTorch.",
            highlights: {
                inference: {
                    title: "Live Inference",
                    desc: "Real-time next-character prediction with probability distributions",
                },
                matrix: {
                    title: "Transition Matrix",
                    desc: "Interactive canvas heatmap of the learned bigram probabilities",
                },
                generation: {
                    title: "Text Generation",
                    desc: "Generate text with configurable temperature and step-by-step tracing",
                },
            },
            cta: {
                explorer: "Open Lab",
                architecture: "View Architecture",
                demo: "Run Interactive Demo",
            },
        },
        experiments: {
            title: "Selected Experiments",
            items: {
                distriKv: {
                    title: "Distri-KV",
                    desc: "A distributed key-value store implemented in Go, featuring Raft consensus and sharding.",
                },
                neuroVis: {
                    title: "NeuroVis",
                    desc: "Interactive visualization tool for neural network activations in real-time.",
                },
                autoAgent: {
                    title: "Auto-Agent",
                    desc: "A lightweight autonomous agent framework focused on coding tasks.",
                },
            },
        },
    },
    notes: {
        hero: {
            est: "EST. 2024",
            archive: "RESEARCH ARCHIVE",
            titlePrefix: "The Engineering",
            titleSuffix: "Logbook",
            description: "Explorations in <strong class='text-primary'>distributed intelligence</strong>, high-dimensional topology, and the mechanics of modern software.",
        },
        featured: {
            badge: "LATEST RESEARCH",
            readTime: "{minutes} min read",
            figure: "Figure 1.0: Latent Space Visualization",
        },
        grid: {
            title: "Previous Entries",
        },
        backToNotes: "Back to Notes",
        noteNotFound: "Note Not Found",
    },
    footer: {
        builtBy: "Built by",
        sourceAvailable: "The source code is available on",
    },
    datasetExplorer: {
        title: "Corpus Evidence",
        subtitle: "Why did the model learn '{context}' -> '{next}'?",
        scanning: "Scanning training corpus...",
        occurrencesFound: "Occurrences Found",
        source: "Source",
        contextSnippets: "Context Snippets",
        noExamples: "No examples found for this transition.",
        fetchError: "Failed to fetch dataset examples",
        explorerTitle: "Corpus Explorer",
        searching: "Searching Dataset...",
        querySequence: "Query Sequence",
        found: "Found {count} occurrences",
        exampleContexts: "Example Contexts",
        noExamplesValidation: "No examples found in the validation snippet.",
    },
    models: {
        bigram: {
            title: "Bigram Language Model",
            description: "The fundamental building block of sequence modeling. A probabilistic model that predicts the next character based solely on the immediate predecessor.",
            params: "Parameters",
            vocab: "Vocabulary",
            trainingData: "Training Data",
            loss: "Final Loss",
            unknown: "Unknown",
            tooltips: {
                params: "They are like the brain's connections. This model is simple, so it doesn't need many.",
                vocab: "It's the set of letters and symbols the model knows, like its own alphabet.",
                trainingData: "The amount of text the model read to learn how to write.",
                loss: "It's the 'error' score. The lower it is, the better the model knows which letter comes next.",
            },
            sections: {
                visualization: {
                    title: "Visualization: Transition Matrix",
                    description: "This is where the model's 'knowledge' lives. For a Bigram model, this grid represents which letters typically follow others.",
                },
                inference: {
                    title: "Inference and Generation",
                    description: "Interact with the model in real-time. Watch how it 'guesses' the next character based on learned probabilities.",
                },
                architecture: {
                    title: "Model Architecture",
                    description: "A technical look at the 'neurons' and layers that process information.",
                },
                training: {
                    title: "Training Insights",
                    description: "Observing the learning process. These metrics show how the model optimized its parameters by reducing prediction error (loss) over 5000 iterations.",
                },
            },
            hero: {
                scientificInstrument: "Scientific Instrument v1.0",
                explanationButton: "Need an intuitive explanation?",
                explanationSub: "Understand the core idea before diving into the math and visualizations.",
            },
            matrix: {
                title: "Transition Matrix",
                activeSlice: "Active Slice Transition",
                tryIt: {
                    label: "Try it:",
                    text: "Click any colored cell in the matrix to see",
                    highlight: "real training examples",
                },
                searchPlaceholder: "Highlight character…",
                runInference: "Run inference to generate the transition matrix",
                tooltip: {
                    title: "How to read this chart?",
                    desc: "Rows represent the current character and columns represent the next character. Brighter cells indicate higher transition probability.",
                    rows: "Rows (Y):",
                    rowsDesc: "The letter the model just wrote.",
                    cols: "Columns (X):",
                    colsDesc: "The letter the model is trying to guess.",
                    brightness: "Brightness:",
                    brightnessDesc: "The brighter a square is, the more likely that pair of letters appears in the text.",
                    example: "Example: If the row is 'q' and the 'u' column shines brightly, it means the model knows that after 'q' almost always comes 'u'.",
                },
                slice: "Slice:",
                datasetMeta: {
                    learnedFrom: "Learned from",
                    summarizes: "summarizes",
                    rawChars: "raw characters",
                    inTrain: "in training split",
                    vocab: "across",
                    symbols: "unique symbols",
                    corpus: "Corpus Name:",
                    rawText: "Total Raw Text:",
                    trainingSplit: "Training Data:",
                    vocabulary: "Vocabulary Size:",
                    charTokens: "characters",
                },
            },
            inference: {
                title: "Inference Console",
                probDist: "1. Probability Distribution",
                probDistDesc: "Type a phrase to see the top-k most likely next characters.",
                tooltip: {
                    title: "What is Inference?",
                    process: "The Process:",
                    processDesc: "The model takes your text, looks at the",
                    processHighlight: "last character",
                    processEnd: ", and looks up the probabilities for what comes next in its brain (the Matrix).",
                    topK: "Top-K:",
                    topKDesc: "We only show the top winners. If K=5, you see the 5 most likely candidates.",
                    note: "Note: This model is \"deterministic\" in its probabilities but \"stochastic\" (random) when it actually picks a character to generate text.",
                },
                form: {
                    input: "Input Text",
                    placeholder: "Type text to analyze...",
                    topK: "Top-K Predictions",
                    analyze: "Analyze",
                    analyzing: "Analyzing...",
                },
            },
            stepwise: {
                title: "Stepwise Prediction",
                mainTitle: "2. Stepwise Prediction",
                description: "Watch the model predict a sequence character-by-character.",
                form: {
                    input: "Input Text",
                    placeholder: "Starting text...",
                    steps: "Prediction Steps",
                    predict: "Predict Steps",
                    predicting: "Predicting...",
                },
                table: {
                    step: "Step",
                    char: "Char",
                    prob: "Probability",
                },
                result: "Result:",
            },
            generation: {
                title: "Generation Playground",
                mainTitle: "3. Text Generation",
                description: "Let the model hallucinate text by sampling from the distribution.",
                tooltip: {
                    title: "How is text generated?",
                    sampling: "Sampling:",
                    samplingDesc: "The model doesn't just pick the #1 answer. It \"rolls a dice\" weighted by probabilities. This is why it can generate different text every time.",
                    temp: "Temperature:",
                    tempDesc: "Higher values make the dice roll \"wilder\" (more random). Lower values make it \"safer\" and more repetitive.",
                    note: "Try temperature 2.0 to see complete gibberish, or 0.1 to see it get stuck in loops!",
                },
                form: {
                    startChar: "Start Character",
                    numTokens: "Number of Tokens",
                    temp: "Temperature",
                    generate: "Generate",
                    generating: "Generating...",
                },
                copyToClipboard: "Copy generated text",
            },
            architecture: {
                title: "Technical Specification",
                subtitle: "Detailed breakdown of the model's internal mechanism, capabilities, and constraints.",
                mechanism: "Inference Mechanism",
                capabilities: "Capabilities",
                constraints: "Constraints",
                modelCard: {
                    title: "Model Card",
                    type: "Architecture Type",
                    complexity: "Complexity Rating",
                    useCases: "Primary Use Cases",
                    description: "Description",
                },
                tooltips: {
                    matrixW: {
                        title: "What is Matrix W?",
                        desc: "It's essentially a lookup table of 9216 numbers (96x96 characters in the vocab). Each number represents the \"unnormalized score\" of how likely one character follows another.",
                    },
                    softmax: {
                        title: "What is Softmax?",
                        desc: "Softmax takes raw scores (logits) and squashes them into a probability distribution. All numbers become positive and add up to 1 (100%).",
                    },
                    loss: {
                        title: "What is Loss (Cross-Entropy)?",
                        desc: "Loss measures the distance between the model's prediction and the truth. If the truth is 'n' and the model gave 'n' a 0.1% chance, the loss will be very high. Training is the process of tuning weights to minimize this distance.",
                    },
                },
                steps: {
                    predicts: "Predicts next character via:",
                    optimizes: "Optimizes parameters using:",
                },
            },
            guide: {
                badge: "Guide for Non-Technical Explorers",
                title: "How does this \"Brain\" work?",
                subtitle: "Explaining the Bigram model so even my mom can understand it (with lots of love).",
                switchHint: "Switch to Educational Mode to see the conceptual guide",
                cards: {
                    memory: {
                        title: "Goldfish Memory",
                        desc: "A **Bigram** model has the shortest memory in the world: it only remembers the **last letter** it wrote. To decide which letter comes next, it can only look at the previous one. It has no context of entire words or phrases.",
                    },
                    darts: {
                        title: "Darts Throw",
                        desc: "The model doesn't \"read\". It just has a giant table that says: \"If the last letter was 'a', there's a 10% probability that the next is 'n'\". Throwing the dart (sampling) is what generates text in a random but coherent way.",
                    },
                    heatmap: {
                        title: "The Heatmap",
                        desc: "The colored grid (Matrix) is the **heart** of the model. The bright squares are the most frequent \"routes\" the model found in the books it read during its training.",
                    },
                },
            },
            educationalOverlay: {
                visualGuideTitle: "Visualization Guide",
                visualGuideDescription: "Each cell in this matrix represents P(next | current) - the probability that one character follows another. Brighter cells indicate more frequent character pairings found in the training corpus.",
                probabilityAnalysisTitle: "Probability Analysis",
                probabilityAnalysisDescription: "Type any text to see which characters the model predicts will come next, ranked by learned probability. The model only looks at the very last character - it has no memory of earlier context.",
                generationLabTitle: "Generation Lab",
                generationLabDescription: "Text generation works by repeatedly sampling from the probability distribution. The temperature parameter controls how random each sample is - lower values produce more predictable output, higher values produce creative (or nonsensical) sequences.",
            },
        },
        ngram: {
            title: "N-Gram Language Model",
            description: "A character-level statistical language model with variable context size. Visualize how increasing the context window sharpens predictions at the cost of exponential sparsity.",
            sections: {
                context: {
                    title: "Context Size",
                    description: "Adjust the context size (N) to condition predictions on more history.",
                },
                slice: {
                    title: "Active Slice",
                    descriptionN1: "For N=1 (Bigram), we visualize the simple Markov transition matrix P(next | current).",
                    descriptionNPlus: "For N>1, we visualize the conditional slice P(next | context). Click cells to trace examples.",
                },
                inference: {
                    title: "Inference & Generation",
                    description: "Interact with the model in real-time. Observe how it selects the next token based on the learned probabilities.",
                    distribution: {
                        title: "Probability Distribution",
                        desc: "Type a phrase to see the top-k most likely next characters.",
                    },
                    stepwise: {
                        title: "Stepwise Prediction",
                        desc: "Watch the model predict a sequence character-by-character.",
                    },
                    generation: {
                        title: "Text Generation",
                        desc: "Let the model hallucinate text by sampling from the distribution.",
                    },
                },
            },
            hero: {
                stats: {
                    uniqueContexts: { label: "Unique Contexts", desc: "Observed n-grams" },
                    vocab: { label: "Vocabulary", desc: "Unique characters" },
                    contextSpace: { label: "Context Space", desc: "|V|^{n}" },
                    tokens: { label: "Training Tokens", desc: "Total tokens seen" },
                },
            },
            viz: {
                hint: {
                    label: "Try it:",
                    text: "Click any colored cell in the matrix to see <strong class='text-white font-semibold'>real training examples</strong>.",
                },
            },
            controls: {
                contextSize: "Context Size (N)",
                contextDesc: "Number of previous characters to condition on",
                bigram: "Bigram (1)",
                trigram: "Trigram (2)",
                fourgram: "4-Gram",
                fivegram: "5-Gram",
                explosion: "Explosion (5+)",
            },
            training: {
                title: "Training Insights",
                stats: {
                    totalTokens: "Total Tokens",
                    uniqueContexts: "Unique Contexts",
                    utilization: "Utilization",
                    sparsity: "Sparsity",
                    transitionDensity: "Transition Matrix Density",
                    subs: {
                        possiblePrefix: "of",
                        possibleSuffix: "possible",
                        fractionObserved: "fraction of possible contexts observed",
                        unseen: "of contexts never seen",
                    },
                },
            },
            historical: {
                title: "Historical Significance & Context",
                learnMore: "Learn More",
                description: "Description",
                limitations: "Key Limitations",
                evolution: "Evolution to Modern AI",
            },
            explosion: {
                title: "Context Too Large — Combinatorial Explosion",
                description: "As valid N increases, the number of possible contexts grows exponentially (|V|^N). For this vocabulary size, calculating the full transition matrix becomes computationally impractical and requires an enormous dataset to avoid sparsity.",
                complexity: "|V|^N = Space Complexity",
                limit: "Classical Limit Reached",
            },
            diagnostics: {
                vocabSize: "Vocabulary",
                contextSize: "Context Size (N)",
                contextSpace: "Context Space (|V|^N)",
                sparsity: "Sparsity",
                sub: {
                    observed: "{count} observed",
                    possible: "Possible Contexts",
                    utilized: "{percent}% utilized",
                },
            },
            educationalOverlay: {
                contextControlTitle: "Context Size Control",
                contextControlDescription: "Increasing N lets the model condition on more history - but the number of possible contexts grows as |V|^N. This exponential blowup is the central tension of n-gram models: more context means sharper predictions but also more data sparsity.",
                sliceVisualizationTitle: "Matrix Slice View",
                sliceVisualizationDescription: "For N > 1 the full transition tensor is too large to display. Instead, we fix the current context and show the resulting probability row - a slice through the high-dimensional table.",
                probabilityDistributionTitle: "Probability Distribution",
                probabilityDistributionDescription: "The model looks at the last N characters of your input, finds the matching context in its lookup table, and returns the probability distribution over all possible next characters.",
                generationPredictionTitle: "Generation & Prediction",
                generationPredictionDescription: "In educational mode we focus on understanding how a single next token is chosen. Switch to Free Lab to unlock the full stepwise tracer and text generation playground.",
                simplifiedSimulation: "Full stepwise prediction and generation available in Free Lab mode.",
            },
        },
    },
    bigramNarrative: {
        hero: {
            eyebrow: "Understanding Language Models",
            titlePrefix: "The Bigram",
            titleSuffix: "Model",
            description: "A first-principles exploration of the simplest statistical language model — and why it still matters."
        },
        problem: {
            title: "The Problem of Prediction",
            lead: "Language is fundamentally sequential. Every word you read right now is informed by the words that came before it.",
            p1: "This property — that each token in a sequence carries ",
            p1Highlight: "expectations about what follows",
            p2: " — is what makes language both expressive and predictable. It's also what makes it so hard to model computationally.",
            p3: "The central challenge of language modeling is deceptively simple to state:",
            quote: "Given what we have already seen, what should come next?",
            p4: "This question has driven decades of research in ",
            h1: "computational linguistics",
            h2: "information theory",
            h3: "deep learning",
            p5: ". To build a model that can answer it, we need a way to capture the statistical structure of language. Let's start with the simplest possible approach.",
            label: "Foundation"
        },
        coreIdea: {
            label: "Core Idea",
            title: "The Simplest Statistical Idea",
            lead: "What if, instead of trying to understand meaning, we simply observed patterns?",
            p1: "Specifically: ",
            h1: "how often does one character follow another?",
            p2: " This is the core insight behind the Bigram model. It ignores grammar, semantics, and long-range dependencies entirely. It asks only one question: given the current token, what is the probability distribution over the next token?",
            caption: "The Bigram assumption: the next token depends only on the current one.",
            p3: "We model P(x_{t+1} | x_t) — the chance of seeing a particular next token given only the token we just observed. Nothing more, nothing less. This radical simplification is what makes the model both tractable and limited.",
            calloutTitle: "Key Insight",
            calloutP1: "The \"bi\" in Bigram means ",
            calloutH1: "two",
            calloutP2: ". The model considers pairs of tokens — the current one and the next one. It has zero memory of anything before the current token."
        },
        mechanics: {
            label: "Mechanics",
            title: "Building a Transition Table",
            lead: "To learn these probabilities, the model scans through a training corpus and counts every pair of consecutive tokens.",
            p1: "For each token A, it records how often each possible token B appears immediately after it. These counts form a ",
            h1: "matrix",
            p2: " — a two-dimensional table where rows represent the current token and columns represent the next token. Each cell holds the number of times that specific transition was observed in the training data.",
            p3: "The visualization below is a live rendering of this transition matrix. Brighter cells indicate more frequent pairings — patterns the model has learned from real text.",
            calloutTitle: "Reading the Matrix",
            calloutP1: "Each row represents a \"given\" character. Each column represents a \"next\" character. The brightness of a cell encodes how likely that transition is. Notice how some rows are nearly uniform (the model is unsure) while others have sharp peaks (strong preferences)."
        },
        normalization: {
            label: "Normalization",
            title: "From Counts to Probabilities",
            lead: "Raw counts alone don't tell us much. To make predictions, we need to convert them into probabilities.",
            p1: "We do this by ",
            h1: "normalizing each row",
            p2: " of the count matrix — dividing every count by the total number of transitions from that row's token. After normalization, each row sums to 1.0, forming a valid probability distribution.",
            p3: "The model can now make concrete statements: \"After the letter h, there is a 32% chance the next character is e, a 15% chance it's a, and so on.\"",
            p4: "Try it yourself below. Type any text to see what the model predicts will come next — based ",
            h2: "solely on the very last character",
            p5: " of your input."
        },
        sampling: {
            label: "Sampling",
            title: "Generating New Text",
            lead: "Once we have a probability distribution, we can do something remarkable: generate entirely new text.",
            p1: "The process is called ",
            h1: "autoregressive sampling",
            p2: ". Start with a seed character, sample the next one from its probability distribution, then use that new character as the seed for the next step. Repeat indefinitely.",
            calloutTitle: "Temperature",
            calloutP1: "The ",
            calloutH1: "temperature",
            calloutP2: " parameter controls how \"creative\" the generation is. At ",
            calloutH2: "low temperatures",
            calloutP3: ", the model almost always picks the most likely next token. At ",
            calloutH3: "high temperatures",
            calloutP4: ", it samples more uniformly — producing surprising and often nonsensical output.",
            p3: "Generate some text below and observe how a model with ",
            h2: "only one character of memory",
            p4: " produces output that is statistically plausible at the character level, yet meaningless at any higher level."
        },
        reflection: {
            label: "Reflection",
            title: "Power and Limitations",
            lead: "The Bigram model is powerful precisely because of its simplicity.",
            p1: "It requires very few parameters — just a V × V matrix, where V is the vocabulary size. It trains instantly. And it provides a clear ",
            h1: "probabilistic baseline",
            p2: " for language generation that every more sophisticated model must beat.",
            calloutTitle: "The Fundamental Limitation",
            calloutP1: "The model has ",
            calloutH1: "no memory beyond a single token",
            calloutP2: ". It cannot learn that \"th\" is often followed by \"e\", because by the time it sees \"h\", it has already forgotten the \"t\". It captures local co-occurrence but nothing about words, phrases, or meaning.",
            p3: "This limitation is exactly what motivates the progression to more sophisticated architectures: ",
            h2: "N-grams",
            p4: " extend the context window, ",
            h3: "MLPs",
            p5: " learn dense representations, and ",
            h4: "Transformers",
            p6: " attend to the entire sequence at once.",
            quote: "Each model in this lab builds on the same core question: given context, what comes next?"
        },
        tokens: {
            title: "Representing text",
            lead: "We split text into tokens.",
            charTitle: "Characters:",
            charDesc: "small vocab, easy to see.",
            wordTitle: "Words:",
            wordDesc: "richer, huge vocab.",
            note: "We use characters here.",
            charLevelTitle: "Character-level tokens",
            charLevelBody: "Small vocabulary, easy to visualize.",
            wordLevelTitle: "Word-level tokens",
            wordLevelBody: "More expressive; vocabulary can be huge."
        },
        counting: {
            title: "The Bigram idea",
            lead: "Count pairs: current -> next. More counts = more likely.",
            builderTitle: "Step-by-step builder",
            builderDesc: "Walk through text; each pair adds +1 to a cell."
        },
        matrix: {
            title: "The transition table",
            lead: "Rows = current token, columns = next.",
            desc: "Build below, then see the full matrix."
        },
        probabilities: {
            title: "Counts to probabilities",
            lead: "Normalize each row to 100%.",
            desc: "Model reads last token's row and samples the next.",
            overlayTitle: "Counts -> Probabilities -> Sampling",
            overlayDesc: "Pick token, normalize row, sample next.",
            step1: "1) Row values",
            step2: "2) Normalize",
            step3: "3) Sample next token",
            currentToken: "Current token",
            typeChar: "Type a character",
            normalizeSimple: "Simple normalize",
            softmax: "Softmax",
            sampleNext: "Sample next token",
            mostLikely: "Most likely:",
            remaining: "Remaining:",
            stochastic: "Sampling is random."
        },
        limitations: {
            title: "Limitations",
            lead: "Bigram has no memory—only the last token.",
            desc: "No long context. Hence N-grams and neural nets."
        },
        footer: {
            text: "Continue exploring the other models in the lab to see how each one addresses the limitations of its predecessor.",
            brand: "LM-Lab · Educational Mode"
        }
    },
    bigramBuilder: {
        description: "We build the bigram matrix by scanning the text character by character. For each pair of consecutive characters (current → next), we increment the cell [current, next]. This table captures how often each character is followed by another.",
        placeholder: "Type text here...",
        hint: "Enter some text to see how the bigram matrix is constructed.",
        buttons: {
            build: "Build Bigram Matrix",
            next: "Next Step",
            autoPlay: "Auto Play",
            pause: "Pause",
            instant: "Instant Complete",
            reset: "Reset Steps"
        },
        vocab: "Educational vocabulary",
        normalized: "Normalized text:",
        empty: "(empty after filtering)",
        skipped: "Showing the first {max} unique characters for clarity ({count} unique character(s) omitted).",
        step1: "Step",
        step2: "updates cell [",
        step3: "].",
        pressBuild: "Press Build Bigram Matrix and start stepping through character pairs.",
        table: {
            curnxt: "cur \\ nxt"
        }
    }
};
