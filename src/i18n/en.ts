export const en = {
    common: {
        language: "Language",
        loading: "Loading...",
        error: "Error",
        comingSoon: "Coming Soon",
        backToProjects: "Back to Projects",
        toggleLanguage: "Toggle Language",
        code: "Code",
        liveDemo: "Live Demo",
        viewCaseStudy: "View Case Study",
    },
    nav: {
        home: "Home",
        projects: "Projects",
        lab: "Lab",
        notes: "Notes",
    },
    lab: {
        bigram: "Bigram",
        ngram: "N-Gram",
        mlp: "MLP",
        transformer: "Transformer",
        neuralNetworks: "Neural Nets",
        shell: {
            allModels: "Back to Lab",
        },
        active: "Lab Active",
        waking: "Waking Up",
        serverWarning: {
            title: "BACKEND COLD-START DETECTED",
            subtitle: "CONTAINMENT PROTOCOL ACTIVE",
            message: "The server is waking up from hibernation. Free-tier Render instances spin down after inactivity — yes, I'm hosting this on a free server because I'm a broke student.",
            donate: "If the 30s wait is too painful, feel free to sponsor my coffee fund so I can afford a real server. Or just wait, it's free entertainment.",
            status: "ATTEMPTING CONNECTION",
            dismiss: "I'LL SURVIVE",
            connected: "SIGNAL ACQUIRED",
        },
        mode: {
            educational: "Educational",
            educationalDescription: "Guided learning experience with story-driven explanations and progressive reveals.",
            freeLab: "Free Lab",
            freeLabDescription: "Full access to all tools and visualizations for manual experimentation and analysis.",
            selectViewingMode: "Select Viewing Mode",
            availableModels: "Available Models",
        },
        status: {
            ready: "Ready",
            coming: "Coming",
        },
        models: {
            bigram: {
                name: "Bigram Explorer",
                subtitle: "Chapter 1 · Counting Pairs",
                description: "Start here. The simplest language model: count how often one character follows another, store it in a table, and sample the next character. You'll see exactly how a model can generate text from pure statistics — and why one character of memory isn't enough.",
            },
            ngram: {
                name: "N-Gram Lab",
                subtitle: "Chapter 2 · More Memory, More Problems",
                description: "What if the model remembers two characters? Three? Five? Predictions get sharper — but the table explodes exponentially and the model still can't generalize. Discover the fundamental limits of counting-based language models.",
            },
            neuralNetworks: {
                name: "Neural Networks",
                subtitle: "Chapter 3 · From Counting to Learning",
                description: "Counting hit a wall. Now we learn instead. Build intuition for perceptrons, activation functions, backpropagation, and gradient descent — the building blocks that let machines discover patterns on their own.",
            },
            mlp: {
                name: "MLP Language Model",
                subtitle: "Chapter 4 · Neural Language Modeling",
                description: "Apply everything you've learned: replace the N-gram lookup table with a neural network that uses dense embeddings and learned weights. See how an MLP generalizes beyond exact matches and produces better text with fewer parameters.",
            },
            transformer: {
                name: "Transformer Architecture",
                subtitle: "Chapter 5 · Attention Is All You Need",
                description: "The architecture behind GPT and modern AI. Self-attention lets the model dynamically focus on any part of the sequence, eliminating fixed context windows entirely. Coming soon.",
            },
        },
        dashboard: {
            chip: "Model Interpretability Lab",
            suite: "Suite",
            description1: "Explore the inner workings of language models through interactive visualizations.",
            description2: "Follow a guided path or experiment freely in the sandbox.",
            launchUnit: "LAUNCH UNIT",
            secureLock: "SECURE LOCK",
            footerCopyright: "© 2026 LM-LAB INSTRUMENTS",
            footerSystem: "INTERPRETABILITY_SYSTEM",
            secureConnection: "Secure Connection",
            hardwareMock: "Hardware: v4-8 TPU MOCK",
        },
        placeholders: {
            mlp: {
                title: "MLP Explorer",
                description: "Multi-Layer Perceptron language model explorer. Currently under development - check back soon.",
            },
            transformer: {
                title: "Transformer Explorer",
                description: "Attention-based transformer model explorer. Currently under development - check back soon.",
            },
        },
        landing: {
            hero: {
                badge: "Research Unit",
                subtitle: "Interactive Interpretability Lab",
                description: "Demystifying Language Models through first-principles engineering and visual proof.",
                subDescription: "This unit focuses on mechanistic interpretability: the reverse-engineering of neural weights into understandable human concepts.",
                start: "Initialize Base Model",
                recommended: "Recommended for beginners",
            },
            highlights: {
                visualizations: "Interactive Viz",
                inference: "Live Inference",
                guided: "Guided Path",
                backend: "PyTorch Backend",
            },
            learningPath: {
                title: "Learning Path",
                status: {
                    soon: "Developing",
                    ready: "Unit Active",
                },
            },
            modes: {
                title: "Laboratory Protocols",
                entryTitle: "Choose Your Experience",
                entrySubtitle: "Select how you want to explore the lab. You can change this at any time.",
                defaultNote: "Defaulting to Educational Mode",
                educational: {
                    title: "Educational Mode",
                    subtitle: "Guided Discovery",
                    description: "Step-by-step narrative explaining the 'why' behind the math. Best for conceptual learning.",
                    tag: "Recommended",
                    features: ["Narrative walkthroughs", "Progressive reveals", "Conceptual explanations"],
                },
                freeLab: {
                    title: "Free Lab Mode",
                    subtitle: "Sandbox Environment",
                    description: "Full access to all visualization tools and generation parameters. For advanced experimentation.",
                    tag: "Advanced",
                    features: ["All tools unlocked", "Raw parameter control", "No guided flow"],
                },
                cta: "Start with Bigram",
                ctaSubtext: "The simplest model — the best starting point",
                changeMode: "Change mode",
                selectedMode: "Selected",
            },
            availableModels: {
                title: "Biological Units Available",
                enter: "Enter Lab",
                locked: "Protocol Restricted",
            },
            footer: {
                text: "Scientific Instrument v2.2 // Build 2026",
            },
        },
    },
    training: {
        title: "Training Insights",
        noData: "Run inference to view training data",
        tooltip: {
            lossTitle: "What is Loss?",
            lossErrorPrefix: "Prediction Error:",
            lossError: "Loss measures how 'surprised' the model is. A high loss means it's guessing wrong frequently.",
            lossBenchmarkPrefix: "The Benchmark:",
            lossBenchmark: "Pure random guessing would give a loss of ~4.56 (-ln(1/96)). Anything lower means the model has actually learned something!",
            lossCurve: "The descending curve shows the model slowly discovering patterns in your text.",
        },
        stats: {
            finalLoss: "Final Loss",
            steps: "Steps",
            batchSize: "Batch Size",
            learningRate: "Learning Rate",
            parameters: "Parameters",
            tooltips: {
                finalLoss: "The error level. At the end of training, it should be as low as possible.",
                steps: "How many times the model practiced to improve its predictions.",
                batchSize: "The amount of information pieces the model processes at once.",
                learningRate: "The learning speed. Neither too fast to avoid missing, nor too slow to avoid taking too long.",
                parameters: "The size of the neural network or 'brain' of the model.",
            },
        },
    },
    ngram: {
        training: {
            title: "Training Insights",
            stats: {
                totalTokens: "Total Tokens",
                uniqueContexts: "Unique Contexts",
                utilization: "Context Utilization",
                sparsity: "Sparsity",
                transitionDensity: "Transition Density",
                subs: {
                    possiblePrefix: "of",
                    possibleSuffix: "possible",
                    fractionObserved: "Fraction of contexts observed",
                    unseen: "Unseen context fraction",
                },
            },
        },
    },
    landing: {
        hero: {
            status: "System Online :: v2.2",
            role: "Research & Engineering",
            title: "ADRIAN LAYNEZ ORTIZ",
            tagline1: "Mathematics & Computer Science.",
            tagline2: "Mechanistic Interpretability · High-Performance Engineering.",
            cta: {
                lab: "View Lab Work",
                notes: "Read Notes",
            },
        },
        metrics: {
            research: "Years of Research",
            repos: "Open Source Repos",
            projects: "Active Projects",
            curiosity: "Curiosity",
        },
        about: {
            badge: "About",
            building: "Currently Building",
            projectTitle: "Deep Learning Engine — CUDA / C++",
            projectDesc: "Custom kernels for matrix operations and backpropagation",
            bio: {
                titlePrefix: "Bridging Abstract Mathematics",
                titleSuffix: "& Machine Intelligence",
                p1: "I am pursuing a double degree in <strong class='text-foreground'>Mathematics and Computer Science</strong> at the Universidad Complutense de Madrid. My research focuses on understanding neural networks at their deepest level — from gradient dynamics to kernel-level optimization.",
                p2: "I specialize in <strong class='text-foreground'>Mechanistic Interpretability</strong> — the science of reverse-engineering how neural networks represent and process information internally. Rather than treating models as black boxes, I decompose their circuits to understand <em class='text-foreground/80'>why they work</em>.",
                mission: "My mission: make AI systems transparent through rigorous mathematical analysis and low-level engineering.",
            },
        },
        skills: {
            title: "Technical Proficiencies",
            linearAlgebra: "Linear Algebra",
            topology: "Topology",
            convexOpt: "Convex Optimization",
        },
        work: {
            badge: "Selected Work",
            titlePrefix: "Engineering from",
            titleSuffix: "First Principles",
            description: "Every project begins with a question. From reimplementing seminal papers to writing bare-metal GPU kernels, each one is an exercise in deep understanding.",
            viewAll: "View All Projects",
            items: {
                nanoTransformer: {
                    title: "Nano-Transformer",
                    desc: "Ground-up reproduction of 'Attention Is All You Need' in PyTorch — Multi-Head Attention, Positional Encodings, and LayerNorm implemented without pre-built Transformer modules.",
                },
                cudaKernels: {
                    title: "CUDA Matrix Kernels",
                    desc: "Handwritten CUDA kernels exploring SGEMM optimization — from naive implementations to tiled shared-memory strategies, benchmarked against cuBLAS.",
                },
                autograd: {
                    title: "Autograd Engine",
                    desc: "Lightweight reverse-mode automatic differentiation library. Dynamically constructs computation graphs and propagates gradients via the chain rule.",
                },
                mathDl: {
                    title: "The Mathematics of Deep Learning",
                    desc: "Interactive articles exploring the rigorous theory behind modern AI — SGD convergence analysis, the linear algebra of LoRA, and differential geometry on neural manifolds.",
                },
                distributed: {
                    title: "Distributed Inference",
                    desc: "Architectural explorations in data-parallel training, model sharding, and optimized inference pipelines for large-scale neural networks.",
                },
            },
        },
        contact: {
            badge: "Open to Opportunities",
            titlePrefix: "Let's Build",
            titleMiddle: "Something",
            titleSuffix: "Together",
            description: "Whether it's a research collaboration, an internship opportunity, or just a conversation about the mathematics of intelligence — I'd love to hear from you.",
            email: "Get in Touch",
            github: "GitHub Profile",
            githubShort: "GitHub",
            linkedin: "LinkedIn",
        },
    },
    projects: {
        hero: {
            badge: "Research & Development",
            titlePrefix: "Constructing the",
            titleSuffix: "Digital Frontier.",
            description: "A curated collection of my work in distributed systems, AI infrastructure, and high-performance computing.",
        },
        flagship: {
            badge: "Flagship Project",
            featured: "Featured",
            liveDemo: "Live Demo Available",
            title: "LM-Lab",
            description: "An interactive platform for exploring language model architectures from first principles. Visualize transition matrices, probe inference dynamics, and generate text — all powered by a live FastAPI backend with PyTorch.",
            highlights: {
                inference: {
                    title: "Live Inference",
                    desc: "Real-time next-character prediction with probability distributions",
                },
                matrix: {
                    title: "Transition Matrix",
                    desc: "Interactive canvas heatmap of the learned bigram probabilities",
                },
                generation: {
                    title: "Text Generation",
                    desc: "Generate text with configurable temperature and step-by-step tracing",
                },
            },
            cta: {
                explorer: "Open Lab",
                architecture: "View Architecture",
                demo: "Run Interactive Demo",
            },
        },
        experiments: {
            title: "Selected Experiments",
            items: {
                distriKv: {
                    title: "Distri-KV",
                    desc: "A distributed key-value store implemented in Go, featuring Raft consensus and sharding.",
                },
                neuroVis: {
                    title: "NeuroVis",
                    desc: "Interactive visualization tool for neural network activations in real-time.",
                },
                autoAgent: {
                    title: "Auto-Agent",
                    desc: "A lightweight autonomous agent framework focused on coding tasks.",
                },
            },
        },
    },
    notes: {
        hero: {
            est: "EST. 2024",
            archive: "RESEARCH ARCHIVE",
            titlePrefix: "The Engineering",
            titleSuffix: "Logbook",
            description: "Explorations in <strong class='text-primary'>distributed intelligence</strong>, high-dimensional topology, and the mechanics of modern software.",
        },
        featured: {
            badge: "LATEST RESEARCH",
            readTime: "{minutes} min read",
            figure: "Figure 1.0: Latent Space Visualization",
        },
        grid: {
            title: "Previous Entries",
        },
        backToNotes: "Back to Notes",
        noteNotFound: "Note Not Found",
    },
    footer: {
        builtBy: "Built by",
        sourceAvailable: "The source code is available on",
    },
    datasetExplorer: {
        title: "Corpus Evidence",
        subtitle: "Why did the model learn '{context}' -> '{next}'?",
        scanning: "Scanning training corpus...",
        occurrencesFound: "Occurrences Found",
        source: "Source",
        contextSnippets: "Context Snippets",
        noExamples: "No examples found for this transition.",
        fetchError: "Failed to fetch dataset examples",
        explorerTitle: "Corpus Explorer",
        searching: "Searching Dataset...",
        querySequence: "Query Sequence",
        found: "Found {count} occurrences",
        exampleContexts: "Example Contexts",
        noExamplesValidation: "No examples found in the validation snippet.",
    },
    models: {
        bigram: {
            title: "Bigram Language Model",
            description: "The fundamental building block of sequence modeling. A probabilistic model that predicts the next character based solely on the immediate predecessor.",
            params: "Parameters",
            vocab: "Vocabulary",
            trainingData: "Training Data",
            loss: "Final Loss",
            unknown: "Unknown",
            tooltips: {
                params: "They are like the brain's connections. This model is simple, so it doesn't need many.",
                vocab: "It's the set of letters and symbols the model knows, like its own alphabet.",
                trainingData: "The amount of text the model read to learn how to write.",
                loss: "It's the 'error' score. The lower it is, the better the model knows which letter comes next.",
            },
            sections: {
                visualization: {
                    title: "Visualization: Transition Matrix",
                    description: "This is where the model's 'knowledge' lives. For a Bigram model, this grid represents which letters typically follow others.",
                },
                inference: {
                    title: "Inference and Generation",
                    description: "Interact with the model in real-time. Watch how it 'guesses' the next character based on learned probabilities.",
                },
                architecture: {
                    title: "Model Architecture",
                    description: "A technical look at the 'neurons' and layers that process information.",
                },
                training: {
                    title: "Training Insights",
                    description: "Observing the learning process. These metrics show how the model optimized its parameters by reducing prediction error (loss) over 5000 iterations.",
                },
            },
            hero: {
                scientificInstrument: "Scientific Instrument v1.0",
                explanationButton: "Need an intuitive explanation?",
                explanationSub: "Understand the core idea before diving into the math and visualizations.",
            },
            matrix: {
                title: "Transition Matrix",
                activeSlice: "Active Slice Transition",
                tryIt: {
                    label: "Try it:",
                    text: "Click any colored cell in the matrix to see",
                    highlight: "real training examples",
                },
                searchPlaceholder: "Highlight character…",
                runInference: "Run inference to generate the transition matrix",
                tooltip: {
                    title: "How to read this chart?",
                    desc: "Rows represent the current character and columns represent the next character. Brighter cells indicate higher transition probability.",
                    rows: "Rows (Y):",
                    rowsDesc: "The letter the model just wrote.",
                    cols: "Columns (X):",
                    colsDesc: "The letter the model is trying to guess.",
                    brightness: "Brightness:",
                    brightnessDesc: "The brighter a square is, the more likely that pair of letters appears in the text.",
                    example: "Example: If the row is 'q' and the 'u' column shines brightly, it means the model knows that after 'q' almost always comes 'u'.",
                },
                slice: "Slice:",
                datasetMeta: {
                    learnedFrom: "Learned from",
                    summarizes: "summarizes",
                    rawChars: "raw characters",
                    inTrain: "in training split",
                    vocab: "across",
                    symbols: "unique symbols",
                    corpus: "Corpus Name:",
                    rawText: "Total Raw Text:",
                    trainingSplit: "Training Data:",
                    vocabulary: "Vocabulary Size:",
                    charTokens: "characters",
                },
                probFlow: {
                    badge: "Probability Flow Visualizer",
                    alreadyNormalized: "⚠ Matrix appears pre-normalized",
                    description: "Explore how raw counts become probabilities and how the model samples the next token. This interactive diagram shows the complete inference pipeline: from selecting a context character, to normalizing its row into a probability distribution, to stochastically sampling the next token.",
                    step1: "Step 1: Select Context",
                    step2: "Step 2: Normalize",
                    step3: "Step 3: Sample Next Token",
                    currentToken: "Current Token",
                    typeToChange: "Type to change context",
                    normalize: "Normalize",
                    softmax: "Softmax",
                    temperature: "Temperature",
                    educational: {
                        normTitle: "Simple Normalization",
                        normDesc: "Divide each count by the row sum. This converts raw frequencies into probabilities that sum to 1.0.",
                        softmaxTitle: "Softmax (Temperature-Scaled)",
                        softmaxDesc: "Exponentiates values and normalizes. Temperature controls sharpness: low temp → peaked distribution, high temp → uniform distribution.",
                        tempTitle: "Temperature",
                        tempDesc: "Controls the sharpness of the distribution. Low temperature (< 1) concentrates probability on the most likely tokens. High temperature (> 1) spreads it more evenly, producing more varied — and often surprising — output.",
                    },
                    tempLabel: "Temperature",
                    tempTooltip: "Controls randomness. Lower = more deterministic, Higher = more creative/random",
                    sampleButton: "Sample Next Token",
                    sample: "Sample Next Token",
                    sampling: "Sampling...",
                    result: "Sampled Result",
                    sampled: "Sampled",
                    topCandidate: "Top candidate",
                    mostLikely: "Most Likely",
                    probability: "Probability",
                    roll: "Random Roll",
                    explanation: "The model threw a weighted dice (roll = {roll}) and selected '{token}' with probability {prob}%",
                    stochasticNote: "Sampling is stochastic — each click may produce a different result even for the same context character.",
                },
                labModeGuide: "This is the full transition matrix trained on Paul Graham essays. Each row is a character; each column is the character that follows. Brighter cells = more frequent transitions. Click any cell to see real training examples from the corpus.",
                limitationGuide: "Notice the fundamental constraint: the model only looks at the last character. It cannot learn that 'th' is almost always followed by 'e', because by the time it sees 'h', the 't' is already forgotten. This single-token memory is exactly what N-gram and neural models overcome.",
                storySteps: {
                    problem: {
                        title: "The Problem",
                        body: "Language is sequential — every character depends on what came before. The challenge is to capture this structure computationally. How do we build a model that can predict what comes next in a stream of text?",
                    },
                    representation: {
                        title: "Representing Text",
                        body: "Before we can model language, we need to decide how to represent it. The choice of representation determines the vocabulary size, the model's capacity, and its limitations.",
                    },
                    solution: {
                        title: "The Bigram Solution",
                        body: "The simplest approach: count how often each character follows every other character in a large training corpus. These counts, once normalized into probabilities, form a complete statistical model of character-level language.",
                    },
                    matrix: {
                        title: "The Transition Matrix",
                        body: "Every count is stored in a V × V matrix (V = vocabulary size). Each row represents a current character; each column represents the next. The brightness of a cell encodes the transition probability learned from real text.",
                    },
                    probabilities: {
                        title: "From Counts to Probabilities",
                        body: "Raw counts are normalized row-by-row so each row sums to 1.0, forming a valid probability distribution. The model can then make concrete predictions: \"After 'h', there is a 34% chance the next character is 'e'.\"",
                    },
                    limitation: {
                        title: "The Fundamental Limitation",
                        body: "The bigram model has zero memory beyond the immediately preceding character. It cannot learn that 'th' is almost always followed by 'e', because by the time it sees 'h', the 't' is already forgotten. This single-token horizon is what motivates N-gram and neural models.",
                    },
                },
                representation: {
                    charTitle: "Character-level tokens",
                    charBody: "Small, fixed vocabulary (~96 printable ASCII characters). Every possible input is representable. Simple to implement and visualize — ideal for understanding the fundamentals.",
                    wordTitle: "Word-level tokens",
                    wordBody: "Richer semantic units, but vocabulary can reach 50,000–500,000 entries. Rare words cause sparsity; unseen words at inference time cause failures. Much harder to scale.",
                },
                builderLabel: "Step-by-step bigram builder",
            },
            inference: {
                title: "Inference Console",
                probDist: "1. Probability Distribution",
                probDistDesc: "Type a phrase to see the top-k most likely next characters.",
                tooltip: {
                    title: "What is Inference?",
                    process: "The Process:",
                    processDesc: "The model takes your text, looks at the",
                    processHighlight: "last character",
                    processEnd: ", and looks up the probabilities for what comes next in its brain (the Matrix).",
                    topK: "Top-K:",
                    topKDesc: "We only show the top winners. If K=5, you see the 5 most likely candidates.",
                    note: "Note: This model is \"deterministic\" in its probabilities but \"stochastic\" (random) when it actually picks a character to generate text.",
                },
                lastChar: "Last char:",
                form: {
                    input: "Input Text",
                    placeholder: "Type text to analyze...",
                    topK: "Top-K Predictions",
                    analyze: "Analyze",
                    analyzing: "Analyzing...",
                },
            },
            stepwise: {
                title: "Stepwise Prediction",
                mainTitle: "2. Stepwise Prediction",
                description: "Watch the model predict a sequence character-by-character.",
                form: {
                    input: "Input Text",
                    placeholder: "Starting text...",
                    steps: "Prediction Steps",
                    predict: "Predict Steps",
                    predicting: "Predicting...",
                },
                table: {
                    step: "Step",
                    char: "Char",
                    prob: "Probability",
                },
                result: "Result:",
            },
            generation: {
                title: "Generation Playground",
                mainTitle: "3. Text Generation",
                description: "Let the model hallucinate text by sampling from the distribution.",
                tooltip: {
                    title: "How is text generated?",
                    sampling: "Sampling:",
                    samplingDesc: "The model doesn't just pick the #1 answer. It \"rolls a dice\" weighted by probabilities. This is why it can generate different text every time.",
                    temp: "Temperature:",
                    tempDesc: "Higher values make the dice roll \"wilder\" (more random). Lower values make it \"safer\" and more repetitive.",
                    note: "Try temperature 2.0 to see complete gibberish, or 0.1 to see it get stuck in loops!",
                },
                form: {
                    startChar: "Start Character",
                    numTokens: "Number of Tokens",
                    temp: "Temperature",
                    generate: "Generate",
                    generating: "Generating...",
                },
                copyToClipboard: "Copy generated text",
            },
            architecture: {
                title: "Technical Specification",
                subtitle: "Detailed breakdown of the model's internal mechanism, capabilities, and constraints.",
                mechanism: "Inference Mechanism",
                capabilities: "Capabilities",
                constraints: "Constraints",
                modelCard: {
                    title: "Model Card",
                    type: "Architecture Type",
                    complexity: "Complexity Rating",
                    useCases: "Primary Use Cases",
                    description: "Description",
                },
                tooltips: {
                    matrixW: {
                        title: "What is Matrix W?",
                        desc: "It's essentially a lookup table of 9216 numbers (96x96 characters in the vocab). Each number represents the \"unnormalized score\" of how likely one character follows another.",
                    },
                    softmax: {
                        title: "What is Softmax?",
                        desc: "Softmax takes raw scores (logits) and squashes them into a probability distribution. All numbers become positive and add up to 1 (100%).",
                    },
                    loss: {
                        title: "What is Loss (Cross-Entropy)?",
                        desc: "Loss measures the distance between the model's prediction and the truth. If the truth is 'n' and the model gave 'n' a 0.1% chance, the loss will be very high. Training is the process of tuning weights to minimize this distance.",
                    },
                },
                stepsList: {
                    matrixW: "Look up the row of the weight matrix W corresponding to the current character's index. This row contains the raw unnormalized scores (logits) for every possible next character.",
                    softmax: "Apply softmax to the logit row to produce a valid probability distribution over the vocabulary. Every value becomes positive and the row sums to exactly 1.0.",
                    loss: "During training, compute cross-entropy loss between the predicted distribution and the true next character. Backpropagate gradients to update W and minimize future prediction error.",
                },
                analysis: {
                    strengths: [
                        "Exact closed-form solution — no gradient descent required. Counts are sufficient statistics.",
                        "Instant training on any corpus size. O(N) in the number of training tokens.",
                        "Fully interpretable: every cell in W is a directly readable probability.",
                    ],
                    limitations: [
                        "Zero context beyond the immediately preceding token — cannot model multi-character patterns.",
                        "No generalization: each character pair is treated independently with no notion of similarity.",
                        "Vocabulary scales as O(V²) — impractical for word-level models with large vocabularies.",
                    ],
                },
                steps: {
                    predicts: "Predicts next character via:",
                    optimizes: "Optimizes parameters using:",
                },
            },
            guide: {
                badge: "Guide for Non-Technical Explorers",
                title: "How does this \"Brain\" work?",
                subtitle: "Explaining the Bigram model so even my mom can understand it (with lots of love).",
                switchHint: "Switch to Educational Mode to see the conceptual guide",
                cards: {
                    memory: {
                        title: "Goldfish Memory",
                        desc: "A **Bigram** model has the shortest memory in the world: it only remembers the **last letter** it wrote. To decide which letter comes next, it can only look at the previous one. It has no context of entire words or phrases.",
                    },
                    darts: {
                        title: "Darts Throw",
                        desc: "The model doesn't \"read\". It just has a giant table that says: \"If the last letter was 'a', there's a 10% probability that the next is 'n'\". Throwing the dart (sampling) is what generates text in a random but coherent way.",
                    },
                    heatmap: {
                        title: "The Heatmap",
                        desc: "The colored grid (Matrix) is the **heart** of the model. The bright squares are the most frequent \"routes\" the model found in the books it read during its training.",
                    },
                },
            },
            historicalContext: {
                description: "The bigram model is the simplest instance of a Markov chain applied to language. First studied by Claude Shannon in his 1948 paper 'A Mathematical Theory of Communication', character-level bigrams demonstrated that even zero-context statistical models capture meaningful structure in natural language.",
                limitations: [
                    "Zero memory beyond the immediate predecessor — cannot learn multi-character patterns like 'th' → 'e'.",
                    "No generalization — each character pair is treated independently with no notion of similarity.",
                ],
                evolution: "The limitations of bigram models directly motivated N-gram extensions (longer context) and eventually neural approaches (learned representations). Every modern language model can trace its lineage back to this simple transition matrix.",
            },
            educationalOverlay: {
                visualGuideTitle: "Visualization Guide",
                visualGuideDescription: "Each cell in this matrix represents P(next | current) - the probability that one character follows another. Brighter cells indicate more frequent character pairings found in the training corpus.",
                probabilityAnalysisTitle: "Probability Analysis",
                probabilityAnalysisDescription: "Type any text to see which characters the model predicts will come next, ranked by learned probability. The model only looks at the very last character - it has no memory of earlier context.",
                generationLabTitle: "Generation Lab",
                generationLabDescription: "Text generation works by repeatedly sampling from the probability distribution. The temperature parameter controls how random each sample is - lower values produce more predictable output, higher values produce creative (or nonsensical) sequences.",
            },
        },
        ngram: {
            title: "N-Gram Language Model",
            description: "A character-level statistical language model with variable context size. Visualize how increasing the context window sharpens predictions at the cost of exponential sparsity.",
            sections: {
                context: {
                    title: "Context Size",
                    description: "Adjust the context size (N) to condition predictions on more history.",
                },
                slice: {
                    title: "Active Slice",
                    descriptionN1: "For N=1 (Bigram), we visualize the simple Markov transition matrix P(next | current).",
                    descriptionNPlus: "For N>1, we visualize the conditional slice P(next | context). Click cells to trace examples.",
                },
                inference: {
                    title: "Inference & Generation",
                    description: "Interact with the model in real-time. Observe how it selects the next token based on the learned probabilities.",
                    distribution: {
                        title: "Probability Distribution",
                        desc: "Type a phrase to see the top-k most likely next characters.",
                    },
                    stepwise: {
                        title: "Stepwise Prediction",
                        desc: "Watch the model predict a sequence character-by-character.",
                    },
                    generation: {
                        title: "Text Generation",
                        desc: "Let the model hallucinate text by sampling from the distribution.",
                    },
                },
            },
            hero: {
                stats: {
                    uniqueContexts: { label: "Unique Contexts", desc: "Observed n-grams" },
                    vocab: { label: "Vocabulary", desc: "Unique characters" },
                    contextSpace: { label: "Context Space", desc: "|V|^{n}" },
                    tokens: { label: "Training Tokens", desc: "Total tokens seen" },
                },
            },
            viz: {
                hint: {
                    label: "Try it:",
                    text: "Click any colored cell in the matrix to see <strong class='text-white font-semibold'>real training examples</strong>.",
                },
            },
            controls: {
                contextSize: "Context Size (N)",
                contextDesc: "Number of previous characters to condition on",
                unigram: "Unigram",
                bigram: "Bigram",
                trigram: "Trigram",
                fourgram: "4-gram",
                fivegram: "5-gram",
                explosion: "Explosion (5+)",
            },
            lab: {
                badge: "Free Lab Mode · Full instrument access",
                contextLevels: {
                    1: "No context — each character is predicted independently from the corpus frequency. Fastest but least accurate.",
                    2: "Conditions on 1 previous character. Simple Markov chain; low sparsity, moderate precision.",
                    3: "Conditions on 2 previous characters. Better predictions but context space grows to |V|².",
                    4: "Conditions on 3 characters. High precision on seen sequences; significant sparsity on unseen ones.",
                    5: "Maximum context. Very sharp predictions where data exists, but most contexts are unseen — combinatorial explosion imminent.",
                },
                flow: {
                    afterContext: "The matrix below shows the probability distribution learned from training data for the current N level.",
                    afterMatrix: "Use the inference console to query the model with your own text and observe how context size affects predictions.",
                    afterComparison: "The training quality chart below reflects how well the model fits the corpus at the selected N level.",
                },
                performanceSummary: {
                    title: "Performance Summary",
                    description: "Runtime and training metrics for the current model",
                    inferenceTime: "Inference Time",
                    device: "Device",
                    corpusSize: "Corpus Size",
                    trainingDuration: "Training Duration",
                    totalTokens: "Total Tokens",
                    perplexity: "Perplexity",
                    finalLoss: "Final NLL",
                    ms: "ms",
                    tokens: "tokens",
                },
                comparison: {
                    title: "Model Comparison",
                    description: "Metrics across N=1..5",
                    ppl: "PPL",
                    util: "Util",
                    space: "Space",
                    tooltipPpl: "Perplexity — lower means more confident predictions",
                    tooltipUtil: "Fraction of possible contexts seen during training",
                    tooltipSpace: "Total possible context combinations (|V|^N)",
                },
                sparsity: {
                    title: "Data Sparsity",
                    description: "How much of the context space is actually observed",
                    observedContexts: "Observed Contexts",
                    possibleSuffix: "possible",
                    avgTransitions: "Avg. Transitions / Context",
                    nextTokens: "next-tokens per observed context",
                    utilLabel: "Context utilization",
                    utilHint: "Fraction of possible contexts seen in training data",
                    sparsityLabel: "Table sparsity",
                    sparsityHint: "Fraction of (context, next-token) pairs never observed",
                },
                warning5: {
                    title: "Combinatorial threshold exceeded",
                    hint: "Reduce N to 1–4 for live inference, stepwise prediction, and generation. Lower N also reduces sparsity.",
                },
                sections: {
                    transitions: "Transition Probabilities",
                    transitionsDescN1: "Full unigram/bigram matrix P(next | current)",
                    transitionsDescNPlus: "Slice P(next | context)",
                    conditionedOn: "Conditioned on:",
                    sparsity: "Data Sparsity",
                    trainingQuality: "Training Quality",
                    trainingQualityDesc: "Loss curve for the N={n} model during training",
                    nextToken: "Next-Token Prediction",
                    nextTokenDesc: "Type text and see the probability distribution over next characters",
                    stepwise: "Stepwise Prediction",
                    stepwiseDesc: "Trace the context window sliding character by character",
                    generation: "Text Generation",
                    generationDesc: "Generate text auto-regressively using the current N-gram model",
                },
                hero: {
                    title: "N-Gram Language Model",
                    description: "A character-level statistical language model with variable context size. Visualize how increasing the context window sharpens predictions at the cost of exponential sparsity.",
                    uniqueContexts: "Unique Contexts",
                    vocabulary: "Vocabulary",
                    contextSpace: "Context Space",
                    trainingTokens: "Training Tokens",
                    uniqueChars: "Unique characters",
                    totalTokensSeen: "Total tokens seen",
                },
                lossChart: {
                    title: "Training loss (NLL)",
                    final: "Final:",
                    ppl: "PPL:",
                    start: "Start",
                    progress: "Training progress",
                    end: "End",
                    perplexity: "Perplexity",
                    perplexityHint: "Lower = more confident predictions",
                    finalNll: "Final NLL",
                    finalNllHint: "Negative log-likelihood on train data",
                },
                footer: "LM-Lab · Scientific Instrument v1.0",
            },
            training: {
                title: "Training Insights",
                stats: {
                    totalTokens: "Total Tokens",
                    uniqueContexts: "Unique Contexts",
                    utilization: "Utilization",
                    sparsity: "Sparsity",
                    transitionDensity: "Transition Matrix Density",
                    subs: {
                        possiblePrefix: "of",
                        possibleSuffix: "possible",
                        fractionObserved: "fraction of possible contexts observed",
                        unseen: "of contexts never seen",
                    },
                },
            },
            historical: {
                title: "Historical Significance & Context",
                learnMore: "Learn More",
                description: "Description",
                limitations: "Key Limitations",
                evolution: "Evolution to Modern AI",
            },
            explosion: {
                title: "Context Too Large — Combinatorial Explosion",
                description: "As valid N increases, the number of possible contexts grows exponentially (|V|^N). For this vocabulary size, calculating the full transition matrix becomes computationally impractical and requires an enormous dataset to avoid sparsity.",
                complexity: "|V|^N = Space Complexity",
                limit: "Classical Limit Reached",
            },
            diagnostics: {
                vocabSize: "Vocabulary",
                contextSize: "Context Size (N)",
                contextSpace: "Context Space (|V|^N)",
                sparsity: "Sparsity",
                sub: {
                    observed: "{count} observed",
                    possible: "Possible Contexts",
                    utilized: "{percent}% utilized",
                },
            },
            educationalOverlay: {
                contextControlTitle: "Context Size Control",
                contextControlDescription: "Increasing N lets the model condition on more history - but the number of possible contexts grows as |V|^N. This exponential blowup is the central tension of n-gram models: more context means sharper predictions but also more data sparsity.",
                sliceVisualizationTitle: "Matrix Slice View",
                sliceVisualizationDescription: "For N > 1 the full transition tensor is too large to display. Instead, we fix the current context and show the resulting probability row - a slice through the high-dimensional table.",
                probabilityDistributionTitle: "Probability Distribution",
                probabilityDistributionDescription: "The model looks at the last N characters of your input, finds the matching context in its lookup table, and returns the probability distribution over all possible next characters.",
                generationPredictionTitle: "Generation & Prediction",
                generationPredictionDescription: "In educational mode we focus on understanding how a single next token is chosen. Switch to Free Lab to unlock the full stepwise tracer and text generation playground.",
                simplifiedSimulation: "Full stepwise prediction and generation available in Free Lab mode.",
            },
        },
        mlp: {
            title: "MLP + Embeddings",
            description: "Explore 108 trained MLP configurations. Watch embeddings emerge from noise, compare training dynamics across architectures, and generate text from learned character-level representations.",
            hero: {
                badge: "Research Lab",
            },
            freeLab: {
                title: "MLP Configuration Lab",
                description: "Select any trained configuration from the Model Zoo, inspect training curves, explore the embedding space, and compare models side-by-side.",
            },
            page: {
                switchToEducational: "Switch to Educational Mode for the full guided narrative",
            },
            narrative: {
                hero: {
                    eyebrow: "Educational Narrative",
                    titlePrefix: "Beyond Tables:",
                    titleHighlight: "MLP + Embeddings",
                    description: "How multi-layer perceptrons and learned vector representations transformed language modeling — from counting co-occurrences to learning distributed representations of meaning.",
                },
                sections: {
                    s00: { number: "00", label: "Starting Point" },
                    s01: { number: "01", label: "Foundations" },
                    s02: { number: "02", label: "Language Modeling" },
                    s03: { number: "03", label: "Scalability Wall" },
                    s04: { number: "04", label: "The Breakthrough" },
                    s05: { number: "05", label: "Structural Limits" },
                    s06: { number: "06", label: "Empirical Exploration" },
                    s07: { number: "07", label: "Training Stability" },
                    s08: { number: "08", label: "Looking Forward" },
                },
                s00: {
                    heading: "Why N-grams Weren't Enough",
                    lead: "We already know N-grams can count. After working through bigrams and trigrams, one question naturally follows: what can't they do?",
                    p1: "N-gram models store a table of counts — how often each sequence of N characters appeared in training data. This works, but it has hard limits. If a 3-character sequence never appeared in training, the model assigns it",
                    p1H1: "zero probability",
                    p1End: ", even if it's completely reasonable. The table also grows exponentially: a vocabulary of 70 characters gives 70³ = 343,000 possible trigrams, and most are never observed. Larger N means exponentially more empty cells.",
                    p2: "Worse, N-grams have",
                    p2H1: "no notion of similarity",
                    p2End: ". The model treats every character as an isolated symbol. It has no way to know that \"a\" and \"e\" are both vowels, or that patterns learned for one context might transfer to a slightly different one. Every context is learned from scratch, in isolation.",
                    calloutTitle: "The core insight",
                    calloutText: "A table can only memorize. What we need is a function — something that can generalize from patterns it has seen to patterns it hasn't. A function can learn that vowels behave similarly, that certain character sequences share structure, and that unseen combinations aren't random. That function is a neural network.",
                },
                s01: {
                    heading: "What Is a Multi-Layer Perceptron?",
                    lead: "Before talking about layers and architectures, start with the smallest unit: a single artificial neuron. One neuron, one decision. Stack enough of them and you can approximate almost any pattern in data.",
                    p1: "A neuron takes a list of numbers as input, multiplies each by a",
                    p1H1: "weight",
                    p1Mid: "— a dial that says how important that input is — sums everything up, and passes the result through an",
                    p1H2: "activation function",
                    p1End: "that introduces a curve. Think of it as a tiny decision: \"given these inputs and these weights, how strongly should I fire?\" That's it. One neuron is unimpressive on its own. But the idea scales.",
                    calloutTitle1: "What is a weight?",
                    calloutText1: "A weight is a trainable number — a dial the network adjusts during learning. A large positive weight means \"pay close attention to this input.\" A weight near zero means \"mostly ignore it.\" Training is the process of finding the right dial settings by repeatedly checking how wrong a prediction was and nudging every weight slightly in the direction that reduces the mistake. This is called gradient descent.",
                    p2: "Arrange many neurons side by side, all reading the same input, and you get a",
                    p2H1: "layer",
                    p2Mid: ". Each neuron in that layer learns a different pattern. Stack two or more layers and the second layer reads the first layer's detections, not the raw input — it learns patterns of patterns. This stack is a",
                    p2H2: "Multi-Layer Perceptron (MLP)",
                    p2Tail: ".",
                    figLabel1: "MLP Architecture · Schematic",
                    figHint1: "A feedforward network with input, hidden, and output layers connected by learned weight matrices.",
                    p3: "Without the activation function between layers, stacking would be pointless — multiple linear transformations collapse into a single one. The non-linearity (Tanh, ReLU) is what makes depth meaningful. Each layer can bend and warp the representation in ways a single flat mapping never could. This is why deep networks can learn hierarchical features: strokes → letters → words → meaning.",
                    formulaCaption: "Every symbol here is just the 'weighted sum + squish' described above. σ is the activation function, W₁ and W₂ are weight matrices the network learns, and softmax converts the final numbers into probabilities that sum to 1.",
                    calloutTitle2: "Why depth matters",
                    calloutText2: "A single hidden layer can theoretically approximate any function, but in practice deeper networks learn hierarchical representations more efficiently — fewer parameters, better generalization. Each layer can build on abstractions learned by the previous one.",
                    figLabel2: "Interactive · Non-Linearity & Decision Boundaries",
                    figHint2: "Toggle between linear, shallow, and deep models to see how non-linear layers enable complex boundaries.",
                },
                s02: {
                    heading: "MLP Applied to Language (Without Embeddings)",
                    lead: "The simplest way to use an MLP for language: take the previous N tokens, convert each to a number vector, concatenate them, and feed the result into the network to predict the next token.",
                    p1: "To feed characters into a neural network, we first need to turn them into numbers. The most straightforward method is a",
                    p1H1: "one-hot vector",
                    p1Mid: ": a list of zeros the length of the vocabulary, with a single 1 in the slot for that character. For a vocabulary of 70 characters, \"a\" becomes [1, 0, 0, …, 0], \"b\" becomes [0, 1, 0, …, 0], and so on. With a context window of size N, we concatenate N such vectors to form an input of dimension",
                    p1H2: "N × V",
                    p1End: ", then pass it through one or more hidden layers to produce a probability distribution over the next token.",
                    formulaCaption: "The input to the MLP is a concatenation of N one-hot vectors, one per context token.",
                    calloutTitle: "What does loss actually mean?",
                    calloutP1: "Loss is a measure of surprise. After each prediction, the model compares what it said — a probability for every possible next character — with what actually came next.",
                    calloutP2: "If the model gave the correct character a probability of 0.9, it was confident and right. Low surprise. Low loss.",
                    calloutP3: "If it gave the correct character a probability of 0.02, it was nearly certain something else would appear. Big surprise. High loss.",
                    calloutP4: "Training is the process of reducing this average surprise — over millions of characters — until the model's predictions stop being shocking.",
                    figLabel1: "Interactive · Loss Intuition",
                    figHint1: "Drag the slider to set how confident the model is in the correct token. See how cross-entropy loss explodes as confidence approaches zero.",
                    p2: "This already represents a major step forward from N-gram tables. Instead of memorizing exact co-occurrence counts, the model",
                    p2H1: "learns a function",
                    p2End: "that maps context patterns to predictions. It can interpolate between seen patterns and generalize to novel combinations — at least in principle.",
                    p3: "The MLP can discover that certain character sequences behave similarly, even if it has never seen the exact N-gram before. This is because the hidden layers learn internal features that compress and abstract the raw input patterns.",
                    calloutTitle2: "Key improvement over N-grams",
                    calloutText2: "N-gram models assign zero probability to any context they never observed in training. An MLP, by contrast, can assign non-zero probability to unseen contexts because it learns a smooth function — not a lookup table.",
                    figLabel2: "Interactive · MLP Forward Pass",
                    figHint2: "Type a short seed and step through each stage of the forward pass — from raw tokens to final probability distribution.",
                },
                s03: {
                    heading: "The Problem With One-Hot Inputs",
                    lead: "One-hot encoding seems natural, but it creates severe scalability problems that become catastrophic as vocabularies grow.",
                    p1H1: "Input dimensionality explosion.",
                    p1: "With a character-level vocabulary of ~96 tokens and a context of 8 characters, the input vector has 768 dimensions — manageable. But with a word-level vocabulary of 50,000 tokens and a context of 5 words, the input jumps to 250,000 dimensions. The first weight matrix alone would have tens of billions of parameters.",
                    p2H1: "Massive first-layer weight matrices.",
                    p2: "The matrix W₁ connecting the input to the first hidden layer has shape (N·V) × H, where H is the hidden size. For large vocabularies, this single matrix dominates the entire parameter budget, making training slow and memory-prohibitive.",
                    p3H1: "No notion of similarity.",
                    p3: "In one-hot space, every token is equidistant from every other token. The vectors for \"cat\" and \"kitten\" are just as far apart as \"cat\" and \"quantum\". The model must learn every relationship from scratch, with no structural prior that semantically related tokens should behave similarly.",
                    formulaCaption: "All one-hot vectors are equidistant — the model gets zero information about semantic similarity from the encoding itself.",
                    calloutTitle: "The scalability wall",
                    calloutText: "These three problems — dimensional explosion, huge weight matrices, and orthogonal representations — together form a scalability wall. The naive one-hot MLP simply cannot scale to real-world vocabularies. We need a fundamentally better way to represent tokens.",
                },
                s04: {
                    heading: "The Game Changer: Word Embeddings",
                    lead: "Instead of representing each token as a sparse one-hot vector, we learn a dense, low-dimensional vector for every token in the vocabulary. These are called embeddings.",
                    p1: "An embedding is a lookup table — a matrix E of shape V × D, where V is the vocabulary size and D is the embedding dimension (typically 10–300). To get the representation of token t, we simply select the t-th row of E. This is equivalent to multiplying E by the one-hot vector, but much more efficient.",
                    formulaCaption: "An embedding lookup: selecting row t from the embedding matrix E gives a dense D-dimensional vector.",
                    p2: "The key insight is that",
                    p2H1: "each dimension of the embedding captures a latent semantic property",
                    p2End: ". These dimensions are not hand-designed — they emerge automatically from training. One dimension might encode something like \"animate vs. inanimate,\" another might capture \"verb tense,\" and others might encode more abstract patterns that humans cannot easily name.",
                    p3: "Because embeddings are dense and continuous, similar tokens naturally cluster together in embedding space. The model can leverage this structure to",
                    p3H1: "generalize across semantically related tokens",
                    p3End: ". If the model has learned something about \"cat,\" it can partially transfer that knowledge to \"kitten\" because their embedding vectors are close.",
                    p4: "This dramatically reduces the effective input dimensionality. Instead of N × V (potentially hundreds of thousands), the MLP now receives N × D (perhaps a few hundred) — orders of magnitude smaller, with richer information.",
                    pullQuote: "Embeddings transform tokens from isolated symbols into points in a continuous semantic space, where proximity encodes meaning. This single idea unlocked a new era of language modeling.",
                    figLabel1: "Illustrative · Embedding Space (Simplified)",
                    figHint1: "This is a pedagogical illustration — not real model data. Click tokens to explore how similar characters cluster together.",
                },
                s05: {
                    heading: "New Limitations of MLP + Embeddings",
                    lead: "Embeddings solve the representation problem, but the MLP architecture itself introduces structural limitations that no amount of tuning can overcome.",
                    p1H1: "Fixed-size context window.",
                    p1: "An MLP must receive a fixed number of input tokens. It cannot dynamically attend to longer or shorter contexts — every prediction uses exactly N previous tokens, no more, no less. Information outside this window is completely invisible to the model. This is not a training failure — it is a hard architectural constraint.",
                    p2: "The consequence is stark for language: pronouns, references, and topic continuity all depend on context that may be many tokens back. Drag the slider below to see how a window of 3 tokens blinds the model to who \"she\" is — even though the answer is right there in the sentence.",
                    figLabel1: "Interactive · Context Window Blindness",
                    figHint1: "Drag the slider to grow the context window. Watch when 'Mary' (the referent) comes into view — and notice how small the window must be to hide it entirely.",
                    p3H1: "Long-range dependencies are out of reach.",
                    p3: "The problem compounds over longer texts. In real language, a pronoun may refer to a noun introduced dozens of tokens earlier. No practically-sized fixed window can reliably bridge these gaps — and even when it can, the signal is buried in N−1 other tokens competing for the network's attention.",
                    figLabel2: "Demo · Long-Range Dependency Failure",
                    figHint2: "A 19-word sentence where the pronoun 'she' refers to 'scientist' 15 tokens back. Compare how the MLP's prediction changes as the window grows, versus a model with full context.",
                    p4H1: "Position-dependent token meaning.",
                    p4: "Because the MLP concatenates embeddings end to end, the same token at position 1 and position 3 occupies different slices of the input vector — and therefore activates different columns of W₁. The model learns entirely separate weights for \"the at position 1\" versus \"the at position 3.\" There is no shared, position-invariant representation of what a token means.",
                    figLabel3: "Interactive · Position Sensitivity",
                    figHint3: "Toggle 'the' between position 1 and position 3. The highlighted columns in W₁ show which parameters each instance activates — completely different sets.",
                    p5H1: "Parameter explosion and signal dilution.",
                    p5: "Even with embeddings, the first weight matrix W₁ has shape (N · D) × H. Doubling the context window doubles the size of this layer. For long contexts this becomes the dominant cost. At the same time, as N grows each token's embedding shrinks to a smaller fraction of the total input — from 100% at N=1 to just 6% at N=16 — diluting every signal without adding any mechanism to focus on the most informative tokens.",
                    figLabel4: "Interactive · Concatenation Bottleneck",
                    figHint4: "Switch between Parameter Growth and Signal Dilution views. Drag the context size slider and watch W₁ expand — and each token's share of the input shrink.",
                    calloutTitle: "The same root cause",
                    calloutText: "All four limitations share a common origin: the MLP treats its entire context as a single flat vector. It has no mechanism to reason about the structure, ordering, or relative importance of individual tokens. Overcoming this requires architectures that process sequences as sequences — not as concatenated blobs. That architecture is the Transformer.",
                },
                s06: {
                    heading: "Exploring MLP + Embedding Configurations",
                    lead: "With embeddings in place, the MLP language model has several key hyperparameters that control its capacity, efficiency, and behavior. Understanding their impact requires systematic experimentation.",
                    p1: "The core architectural choices include the",
                    p1H1: "embedding dimension",
                    p1Mid1: "(how many latent features per token), the",
                    p1H2: "hidden layer size",
                    p1Mid2: "(how many neurons in each hidden layer), the",
                    p1H3: "number of hidden layers",
                    p1End: "(depth of the network), and the context window size (how many previous tokens the model sees).",
                    p2: "To understand how these choices affect model behavior, we trained many MLP language models with different hyperparameter configurations on the same dataset. This systematic sweep reveals key trade-offs: larger embeddings capture richer semantics but risk overfitting on small data, wider hidden layers increase capacity but slow training, and deeper networks can learn more abstract features but are harder to optimize.",
                    hyperparamCards: {
                        embDim: { title: "Embedding Dimension", desc: "Controls the richness of token representations. Larger values capture more semantic nuance but require more data to train effectively." },
                        hiddenSize: { title: "Hidden Layer Size", desc: "Determines the model's computational width. Wider layers can detect more patterns per layer but increase memory and compute." },
                        numLayers: { title: "Number of Layers", desc: "Controls representational depth. Deeper models can compose features hierarchically but face training stability challenges." },
                        contextWindow: { title: "Context Window", desc: "How many previous tokens the model considers. Larger windows give more information but increase input dimensionality linearly." },
                    },
                    figLabel1: "Interactive · Softmax Temperature",
                    figHint1: "Adjust temperature to see how it sharpens or flattens the probability distribution over next tokens. Low temperature = deterministic; high = creative.",
                    calloutTitle: "Why systematic exploration matters",
                    calloutText: "There is no single \"best\" configuration — the optimal hyperparameters depend on the dataset size, vocabulary, and the computational budget. The only way to develop intuition is to explore the space empirically and observe how each choice affects loss, perplexity, and generation quality.",
                    figLabel2: "Interactive · Hyperparameter Explorer",
                    figHint2: "Adjust sliders to explore how embedding dimension, hidden size, and learning rate affect validation loss, compute cost, training dynamics, and learned embeddings.",
                    p3: "The interactive explorer above lets you compare models across these dimensions, visualizing validation loss, perplexity, training stability, compute cost, generated text quality, and the learned embedding space. Anomaly badges flag concerning patterns like overfitting or unstable gradients. This empirical approach is how practitioners develop real intuition about model design.",
                },
                s07: {
                    heading: "Deep Training Challenges for Large MLPs",
                    lead: "Making deep MLPs actually train well was one of the hardest practical problems in the history of neural networks. Without careful techniques, deep networks simply fail to learn.",
                    p1H1: "Weight initialization.",
                    p1: "If weights are initialized too large, activations explode through the layers. Too small, and gradients vanish before reaching the early layers. Proper initialization schemes (like Xavier or Kaiming) set the initial scale based on layer dimensions to maintain stable signal propagation.",
                    formulaCaption1: "Kaiming initialization: weights are drawn from a Gaussian scaled by the fan-in, keeping variance stable through ReLU layers.",
                    figLabel1: "Interactive · Initialization Sensitivity",
                    figHint1: "Compare loss curves under different initialization scales. Well-scaled initialization is critical for convergence.",
                    p2H1: "Vanishing and exploding gradients.",
                    p2: "During backpropagation, gradients are multiplied through each layer. In a deep network, if these multipliers are consistently less than 1, gradients shrink exponentially (vanishing). If greater than 1, they grow exponentially (exploding). Either way, the network fails to learn effectively.",
                    formulaCaption2: "The chain rule through L layers: gradients are products of per-layer Jacobians. If each factor is slightly < 1 or > 1, the product vanishes or explodes.",
                    figLabel2: "Interactive · Gradient Flow Across Layers",
                    figHint2: "Toggle between vanishing, stable, and exploding gradient regimes to see how gradient magnitude changes per layer.",
                    p3: "For many years, training networks deeper than 2–3 layers was extremely unreliable. Researchers discovered that the combination of activation function choice (ReLU replaced Sigmoid/Tanh for hidden layers), proper initialization, and normalization techniques was essential for stable training.",
                    p4H1: "Batch Normalization",
                    p4: "was a key breakthrough. By normalizing the activations within each layer to have zero mean and unit variance (across a mini-batch), it keeps the internal distributions stable as the network trains. This dramatically reduces the sensitivity to initialization and learning rate, enabling reliable training of much deeper networks.",
                    formulaCaption3: "Batch Normalization: normalize activations h using batch statistics (μ_B, σ²_B), then rescale with learned parameters γ and β.",
                    calloutTitle: "Why BatchNorm changed everything",
                    calloutText: "Before BatchNorm, training deep networks required meticulous hyperparameter tuning. After it, practitioners could reliably train 10, 20, or even 100+ layer networks. It acts as a stabilizer that smooths the loss landscape, allowing gradient descent to converge faster and more reliably.",
                    figLabel3: "Interactive · Batch Normalization Effect",
                    figHint3: "Toggle BatchNorm on and off to see how it stabilizes activation distributions across layers.",
                },
                s08: {
                    heading: "Final Limitations and the Path Ahead",
                    lead: "Even with embeddings, deep architectures, and modern training techniques, the MLP fundamentally operates on fixed-size windows — and this ceiling defines its era in the history of language modeling.",
                    p1: "The MLP processes each context window independently. It has no memory of what came before the window, no mechanism to dynamically attend to distant tokens, and no way to handle variable-length sequences without padding or truncation. Every prediction is made from the same fixed-size snapshot.",
                    p2: "This means MLPs cannot model long-range dependencies — the kind of structure that makes natural language coherent across sentences and paragraphs. A pronoun referring to a noun 50 tokens ago is simply out of reach for an MLP with a context of 8.",
                    p3: "These structural limitations motivated a series of architectural innovations that define the modern trajectory of language modeling:",
                    p3H1: "Recurrent Neural Networks (RNNs)",
                    p3Mid1: "introduced sequential memory, processing one token at a time while maintaining a hidden state.",
                    p3H2: "Convolutional architectures (like WaveNet)",
                    p3Mid2: "applied dilated convolutions to capture hierarchical patterns over sequences. And ultimately,",
                    p3H3: "Transformers",
                    p3End: "introduced self-attention — a mechanism that allows every token to directly attend to every other token, regardless of distance.",
                    pullQuote: "The MLP was the first architecture to show that neural networks could learn language — but its fixed window revealed that learning language requires architectures that understand sequences, not just snapshots.",
                    p4: "Despite these limitations, the MLP + Embeddings framework established concepts that remain foundational in every modern language model: learned token representations, non-linear feature hierarchies, and end-to-end gradient-based training. Every Transformer still uses embedding layers and feedforward MLP blocks — the ideas introduced here never went away; they evolved.",
                },
                cta: {
                    heading: "Continue Exploring",
                    freeLabTitle: "Open Free Lab",
                    freeLabDesc: "Experiment with MLP + Embedding models interactively. Train, visualize embeddings, and generate text with different hyperparameters.",
                    transformerTitle: "Next: Transformers",
                    transformerDesc: "Discover how self-attention overcomes the MLP's fixed-window limitation and enables truly contextual language understanding.",
                },
                footer: {
                    text: "From counting tables to learned representations — the MLP + Embeddings model marked the moment language modeling became truly neural.",
                    brand: "LM-Lab · MLP + Embeddings Narrative",
                },
                oneHot: {
                    title: "One-Hot Encoding",
                    sparse: "Sparse, high-dimensional. Every token is equally distant from every other. No notion of similarity.",
                    learnedTitle: "Learned Embeddings",
                    dense: "Dense, low-dimensional. Similar words (\"cat\" and \"mat\") get similar vectors — the model can generalize.",
                },
                mlpDiagram: {
                    input: "Input",
                    inputDesc: "Context tokens (one-hot or embeddings)",
                    hidden1: "Hidden 1",
                    hidden1Desc: "Learned features",
                    hidden2: "Hidden 2",
                    hidden2Desc: "Higher-order patterns",
                    output: "Output",
                    outputDesc: "Next-token probabilities",
                },
            },
            explorer: {
                loading: "Loading configurations…",
                errorPrefix: "Failed to load MLP grid:",
                noConfigs: "No MLP configurations available from backend.",
                sections: {
                    s01Title: "Model Zoo Overview",
                    s01Subtitle: "fully-trained configurations — click any dot to select it and sync the sliders below.",
                    s02Title: "Selected Configuration",
                    s02Subtitle: "Metric cards, anomaly flags, and a plain-English summary of this model's training quality.",
                    s03Title: "Embedding Space",
                    s03Subtitle: "Vocabulary tokens projected to 2D via PCA. Scrub through training snapshots to watch structure emerge from noise.",
                    s04Title: "Text Generation",
                    s04Subtitle: "Generate character sequences from the selected model. Adjust temperature and length to explore the output distribution.",
                    s05Title: "Advanced Training Diagnostics",
                    s05Subtitle: "Gradient flow, neuron health, and overfitting patterns across the full training run.",
                },
                zoo: {
                    expandableTitle: "Model Zoo · {count} Configurations",
                    description: "Start here. Each dot is one fully-trained model. Click any dot to select it and sync the sliders below. Use filters to find the strongest configs, the worst performers, or outliers worth investigating.",
                },
                sliders: {
                    embeddingDim: "Embedding Dim",
                    hiddenSize: "Hidden Size",
                    learningRate: "Learning Rate",
                },
                config: {
                    active: "Active:",
                    score: "score",
                },
                metrics: {
                    valLoss: "Val Loss",
                    trainLoss: "Train Loss",
                    loss: "Loss",
                    trainSmoothed: "Train (smoothed)",
                    perplexity: "Perplexity",
                    random: "random:",
                    trainValGap: "Train–Val Gap",
                    params: "Params",
                    compute: "Compute",
                    tooltips: {
                        valLoss: "Validation loss: how well the model predicts held-out data it never saw during training. Lower = better. This is the primary metric — unlike training loss, it cannot be gamed by memorization.",
                        trainLossOnly: "Training loss only — validation loss unavailable for this config. Training loss can be misleadingly optimistic because the model has seen these examples.",
                        trainSmoothed: "Mean of the last ~10% of logged training loss values. Smoothing removes per-batch noise, giving a stable estimate of where training loss settled.",
                        perplexity: "Perplexity ≈ exp(loss). Intuition: if perplexity = 20, the model is as uncertain as randomly choosing among 20 tokens. Lower = better. A random model has perplexity equal to the vocabulary size.",
                        randomPerplexity: "Perplexity a uniform random model would achieve. Any useful model should be well below this.",
                        trainValGap: "Train–Val Gap = val_loss − smoothed_train_loss. Positive = overfitting (model fits training data better than new data). Negative = healthy or underfitting. Values > 0.3 are a concern. Uses smoothed train loss for stability.",
                        paramsCount: "Total number of learnable weights and biases in this model configuration.",
                        compute: "Compute = parameters × training steps ({steps}k). This is a deterministic proxy for compute cost — machine-independent, unlike wall-clock time. Bar shows cost relative to the most expensive config in this grid.",
                        computeDetail: "{params} parameters × {steps}k steps. Larger models cost more to train but don't always generalise better.",
                        score: "Composite quality score — higher is better. Computed as how much this config improved over the random baseline, relative to the best config in the grid.",
                    },
                },
                anomalies: {
                    aboveRandom: "≥ Random",
                    overfitting: "Overfitting",
                    valLossUp: "Val Loss ↑",
                    noConvergence: "No Convergence",
                    unstableGrad: "Unstable ∇",
                    pplMismatch: "PPL ≠ exp(L)",
                    tooltips: {
                        aboveRandom: "Final loss is at or above the random baseline — the model may not have learned meaningful patterns from the data.",
                        overfitting: "The train–val gap exceeds 0.3 — the model memorizes training data better than it generalizes. Consider: smaller model, more data, or regularization.",
                        valLossUp: "Validation loss was still increasing at end of training — a sign of overfitting onset. The model may have trained too long.",
                        noConvergence: "Loss did not meaningfully decrease over training — the learning rate may be too high or too low for this architecture.",
                        unstableGrad: "Gradient norms varied by >1000× during training — indicates optimization instability, often caused by too-high learning rate.",
                        pplMismatch: "Reported perplexity does not match exp(loss). This may indicate a data pipeline issue or stale cached metrics.",
                    },
                },
                summaries: {
                    aboveRandom: "This model barely outperforms random guessing. It likely failed to learn meaningful patterns — check the learning rate and architecture.",
                    nonDecreasing: "Loss did not decrease during training. The model failed to converge — the learning rate may be too high or too low for this architecture.",
                    overfitting: "This model overfits — it memorizes training data better than it generalizes. The gap between training and validation loss is large.",
                    lossIncreasing: "Validation loss was still rising at the end of training — a sign of late-stage overfitting. The model trained too long for its capacity.",
                    unstableGradients: "Gradient norms varied wildly during training — the optimization was unstable. This often means the learning rate is too high.",
                    stillImproving: "Training was still improving at the final step. Given more compute, this model might converge further.",
                    balanced: "This model trains stably and generalizes well. A strong, balanced configuration.",
                    converged: "Training completed. The model converged with a moderate generalization gap.",
                },
                computeLabels: {
                    minimal: "Minimal",
                    low: "Low",
                    moderate: "Moderate",
                    high: "High",
                    veryHigh: "Very High",
                },
                timeline: {
                    title: "Training Timeline",
                    noData: "No timeline data available.",
                    pts: "pts",
                    every: "every",
                    steps: "steps",
                    total: "total",
                    nonUniform: "non-uniform",
                    trend: "Trend:",
                    variance: "Variance:",
                    converged: "Converged ~",
                    tooltips: {
                        chart: "Loss curves over training steps. Green = validation loss (on held-out data, primary). Purple = training loss. Dashed red = random baseline — a model guessing uniformly. Convergence means both curves flatten.",
                        pts: "Number of metric checkpoints logged during training. More points = smoother, more informative curves.",
                        interval: "A metric snapshot was saved every {interval} gradient update steps. Finer logging reveals more training dynamics but costs slightly more disk space.",
                        totalSteps: "Total gradient updates performed. All configs in this grid train for exactly {steps}k steps for fair comparison.",
                        trend: "Direction of validation loss in the second half of training. Decreasing = still learning. Flat = converged. Increasing = overfitting onset.",
                        variance: "Statistical variance of validation loss over the full training run. Near-zero = stable training. High variance = noisy or unstable optimization.",
                        convergenceStep: "Training step where validation loss first fell below 50% of its initial value. Earlier = faster learning.",
                    },
                    chart: {
                        randomBaseline: "random baseline",
                        train: "train",
                        valPrimary: "val (primary)",
                        trainingSteps: "Training Steps",
                    },
                },
                embeddingSpace: {
                    title: "PCA 2D · Training Drift",
                    tooltip: "Each point is one vocabulary token, projected from the learned embedding space to 2D via PCA. Use the snapshot slider to watch embeddings evolve from random noise to structured clusters during training.",
                },
                generation: {
                    title: "Generated Sample",
                    seedPlaceholder: "Seed text…",
                    generateButton: "Generate",
                    temp: "Temp",
                    tokens: "Tokens",
                    tempTooltip: "Temperature controls randomness. Low (0.1) = deterministic, always picks the most likely next character. High (2.0) = creative but chaotic, samples from a flatter distribution.",
                    tokensTooltip: "Maximum number of characters to generate. The model generates one character at a time; more tokens = longer output but slower.",
                    estPpl: "est. PPL ≈",
                    chars: "chars",
                    pressGenerate: "Press Generate to produce text from the selected model.",
                    pplTooltip: "Perplexity = exp(loss). This is the model's estimated perplexity on the training distribution — lower means more confident and fluent predictions.",
                },
                diagnostics: {
                    intro: "These diagnostics reveal internal training dynamics: gradient flow, neuron usage, and overfitting patterns. They help advanced users understand how the model learns beyond simple loss curves.",
                    gradNormLabel: "Gradient Norm over Steps",
                    deadNeuronLabel: "Dead Neuron Ratio over Steps",
                    gradNormSection: "ⓘ Gradient Norms per Layer",
                    activationSection: "ⓘ Activation Health over Training",
                    genGapSection: "ⓘ Generalization Gap Heatmap · All Configs",
                    tooltips: {
                        gradNorm: "Gradient norm is the overall magnitude of weight updates at each training step. Stable, moderate values indicate healthy optimization. Sudden spikes suggest instability; values collapsing to zero indicate vanishing gradients.",
                        deadNeuron: "Fraction of neurons that never activate during training. A 'dead' neuron always outputs zero, contributing nothing to learning. High values (> 20%) indicate wasted model capacity — often caused by large learning rates or poor initialization.",
                        gradNormLayer: "Shows gradient magnitudes per parameter group (C=embedding, W1/b1=hidden layer, W2/b2=output layer) across training snapshots. Balanced magnitudes across layers suggest stable, well-conditioned learning. Red cells indicate large gradients that may destabilize training.",
                        activationHealth: "Displays saturation and dead neuron statistics over training time. Saturation (left bar) means neurons are stuck near the tanh activation limits (±1) and contribute near-zero gradient. Dead fraction (right, pink) are neurons that never activate. Both indicate underutilized model capacity.",
                        genGap: "Difference between training loss and validation loss (val − train) across all configurations, averaged over learning rates. Green = model generalizes well to unseen data. Red = overfitting — the model memorizes training data but fails on new examples. Use this to identify which architecture choices cause overfitting.",
                    },
                },
                dataSource: "Real data from {count} trained configs · {steps}k steps each · logged every {interval} steps.",
                primaryValLoss: "Primary: validation loss.",
                primaryTrainLoss: "Primary: training loss (val unavailable).",
            },
            compareMode: {
                needMore: "Need at least 2 configurations to compare.",
                needAtLeastTwoConfigs: "Need at least 2 configurations to compare.",
                description: "Select two configurations to compare side-by-side. The seed text from the main generator is synced to both.",
                selectTwoConfigsToCompare: "Select two configurations to compare side-by-side. The seed text from the main generator is synced to both.",
                configLabel: "Config {label}",
                config: "Config",
                configA: "Config A",
                configB: "Config B",
                title: "Side-by-Side Comparison",
                seed: "Seed:",
                editSeedHint: "(edit in main generator above)",
                editInMainGeneratorAbove: "(edit in main generator above)",
                selectAConfig: "Select a config…",
                selectConfigA: "Select Config A",
                selectConfigB: "Select Config B above",
                diffTitle: "B vs A — differences",
                diffSummary: "B vs A — differences",
                noTimelineData: "No timeline data.",
                steps: "Steps",
                train: "train",
                val: "val",
                trainingLoss: "Training Loss",
                embeddingSpace: "Embedding Space",
                generatedText: "Generated Text",
                temperature: "T",
                generate: "Generate",
                generating: "Generating…",
                seedTextAboveWillBeUsed: "Seed text above will be used.",
                metrics: {
                    valLoss: "Val Loss",
                    perplexity: "Perplexity",
                    gap: "Gap",
                    genGap: "Gen Gap",
                    score: "Score",
                    params: "Params",
                },
                panel: {
                    trainingLoss: "Training Loss",
                    embeddingSpace: "Embedding Space",
                    generatedText: "Generated Text",
                    seedUsed: "Seed text above will be used.",
                    noTimeline: "No timeline data.",
                    tempLabel: "T=",
                    generateButton: "Generate",
                },
            },
            scatterPlot: {
                description: "Each dot is one trained model. X = parameter count (cost), Y = final validation loss (lower is better). Color = embedding dimension. The dashed line is the",
                paretoFrontier: "Pareto frontier",
                paretoDesc: "— best loss for each compute level.",
                highlighted: "highlighted",
                filters: {
                    all: "All",
                    allTip: "Show all configurations",
                    best: "Best ★",
                    bestTip: "Top 25% by composite score",
                    worst: "Worst",
                    worstTip: "Bottom 25% by score — highest loss, lowest quality",
                    anomalies: "Anomalies",
                    anomaliesTip: "Configs with generalization gap > 0.3 or score < 0.2",
                },
                legend: {
                    paretoLine: "Pareto frontier",
                },
                footer: "{count} configurations · Click any dot to select · Lower-right = more compute, less payoff",
                axisX: "Parameters",
                axisY: "Val Loss",
            },
            embeddingDrift: {
                snapshotLabel: "Training snapshot",
                trainingSnapshot: "Training snapshot",
                stepLabel: "Step",
                step: "Step",
                loading: "Loading embeddings…",
                phaseText: {
                    p0: "Random initialization — embeddings have no structure yet.",
                    p1: "Early training — clusters are beginning to form.",
                    p2: "Early-mid training — character categories becoming distinct.",
                    p3: "Mid training — embedding space shows clear structure. Similar tokens cluster together.",
                    p4: "Late training — structure consolidating, noise reducing.",
                    p5: "Final checkpoint — fully trained embeddings. This is what the model uses for prediction.",
                },
                phases: {
                    "0": "Random initialization — embeddings have no structure yet.",
                    "1": "Early training — clusters are beginning to form.",
                    "2": "Early-mid training — character categories becoming distinct.",
                    "3": "Mid training — embedding space shows clear structure. Similar tokens cluster together.",
                    "4": "Late training — structure consolidating, noise reducing.",
                    "5": "Final checkpoint — fully trained embeddings. This is what the model uses for prediction.",
                },
                snapshotUnavailable: "Snapshot unavailable — showing nearest available checkpoint.",
            },
            embeddingViz: {
                loading: "Loading embeddings…",
                waiting: "Waiting for embedding data…",
                dim1: "Dimension 1 (PCA)",
                dim2: "Dimension 2 (PCA)",
                tokens: "tokens",
                clickInfo: "Click any token to highlight its nearest neighbors. Similar tokens cluster together in the learned embedding space.",
                clickToHighlight: "Click any token to highlight its nearest neighbors. Similar tokens cluster together in the learned embedding space.",
                deselectInfo: "Click another token or click \"{token}\" again to deselect. Dashed lines connect to the 4 nearest neighbors in embedding space.",
                clickToDeselect: "Click another token or click \"{token}\" again to deselect. Dashed lines connect to the 4 nearest neighbors in embedding space.",
                categories: {
                    vowel: "Vowels",
                    vowels: "Vowels",
                    consonant: "Consonants",
                    consonants: "Consonants",
                    digit: "Digits",
                    digits: "Digits",
                    punctuation: "Punctuation",
                    whitespace: "Space / Special",
                    spaceSpecial: "Space / Special",
                },
            },
            nearestNeighbors: {
                title: "Nearest Neighbors (Cosine Similarity)",
                loading: "Loading neighbor data…",
                neighborsOf: "Neighbors of",
                noNeighborData: "No neighbor data for this token.",
                noData: "No neighbor data for this token.",
                selectPrompt: "Select a token above to see its nearest neighbors by cosine similarity in embedding space.",
            },
            snapshotDiagnostics: {
                noSnapshotData: "No snapshot data available.",
                noGradData: "No gradient norm data in snapshots.",
                noSatData: "No activation saturation data in snapshots.",
                gradLegend: "Green = small gradients · Yellow/Red = large gradients · Consistent magnitudes across layers indicate healthy training.",
                saturatedLeft: "Saturated activations (left)",
                deadRight: "Dead neurons (right)",
                satLegend: "High saturation means many neurons are pinned at tanh extremes (±1). Dead neurons never activate. Both waste capacity.",
                satNote: "High saturation means many neurons are pinned at tanh extremes (±1). Dead neurons never activate. Both waste capacity.",
                stepHeader: "Step",
                step: "Step",
            },
            genGapHeatmap: {
                header: "hidden ↓ / emb →",
                axisLabel: "hidden ↓ / emb →",
                gapLabel: "Gap:",
                legend: {
                    healthy: "< 0 (healthy)",
                    low: "0–0.1",
                    mid: "0.1–0.2",
                    medium: "0.1–0.2",
                    high: "0.2–0.3",
                    overfit: "> 0.3 (overfit)",
                },
                note: "Each cell averages the train–val gap across all learning rates for that (emb_dim, hidden_size) pair. Red = overfitting. Green = healthy generalization.",
                description: "Each cell averages the train–val gap across all learning rates for that (emb_dim, hidden_size) pair. Red = overfitting. Green = healthy generalization.",
                configs: "configs (across LRs)",
                avgGap: "avg gap=",
                bestLoss: "best loss=",
            },
        },
        neuralNetworks: {
            title: "Neural Networks & Deep Learning",
            description: "A first-principles exploration of artificial neural networks — from the perceptron to backpropagation. Understand how learned parameters replace counting and why dense representations generalize where N-grams fail.",
            hero: {
                badge: "Neural Computation",
            },
            freeLab: {
                title: "Neural Network Playground",
                description: "Experiment freely with perceptrons, activation functions, weight updates, and training dynamics.",
            },
            guidedExperiments: {
                title: "Guided Experiments",
                subtitle: "Five quick exercises to build intuition",
                handVsTraining: {
                    title: "Build Your First Neuron — By Hand, Then By Training",
                    doThis: "Set x₁=1, x₂=0.5. Manually adjust w₁, w₂, b until output ≈ 0.8. Then reset and train with target=0.8.",
                    observeThis: "Training finds similar values automatically. Gradient descent replaces manual guessing.",
                },
                activationComparison: {
                    title: "Compare Activation Functions",
                    doThis: "Switch between Linear, ReLU, Sigmoid, and Tanh. Set x₁=−2, x₂=2, w₁=1, w₂=1, b=0.",
                    observeThis: "Linear passes negatives unchanged. ReLU zeros them. Sigmoid and Tanh squash to bounded ranges.",
                },
                learningRateExtremes: {
                    title: "Break Training with Extreme Learning Rates",
                    doThis: "Switch the model to Linear. Set target=0.8. Train with η=0.05 (slow), then η=1.0 (normal), then η=2.0 (aggressive).",
                    observeThis: "Low η converges slowly. Higher η can overshoot and oscillate. The sweet spot depends on the model and data.",
                },
                convergenceBehavior: {
                    title: "Watch Loss Converge Over Many Steps",
                    doThis: "Set target=0.9, η=1.0. Click Auto-Train ×10 repeatedly and watch the loss chart.",
                    observeThis: "Loss drops fast at first, then plateaus. Early steps matter most. Diminishing returns set in quickly.",
                },
                randomInitialization: {
                    title: "See Why Random Starts Matter",
                    doThis: "Train to target=0.5 with default params. Reset. Change w₁ to 2.0, train again. Compare final loss.",
                    observeThis: "Different starting points lead to different solutions. Neural nets are non-convex; initialization matters.",
                },
            },
            sections: {
                artificialNeuron: { number: "01", label: "The Neuron" },
                nonLinearity: { number: "02", label: "Non-Linearity" },
                howItLearns: { number: "03", label: "Learning" },
                watchingItLearn: { number: "04", label: "Training" },
                bridge: { number: "05", label: "Connection" },
                powerAndLimits: { number: "06", label: "Reflection" },
                playground: {
                    inputs: {
                        title: "Inputs",
                        desc: "Feature values fed into the perceptron. Each input is multiplied by its corresponding weight before being summed.",
                        x1: "First input feature value (x₁). Multiplied by weight w₁ before entering the sum node.",
                        x2: "Second input feature value (x₂). Multiplied by weight w₂ before entering the sum node.",
                    },
                    weights: {
                        title: "Parameters",
                        desc: "Learnable parameters that scale each input. The bias shifts the activation threshold independently of the inputs.",
                        w1: "Weight for input x₁. Controls how strongly x₁ influences the output. Updated by gradient descent during training.",
                        w2: "Weight for input x₂. Controls how strongly x₂ influences the output. Updated by gradient descent during training.",
                        bias: "Bias term (b). Shifts the weighted sum, allowing the neuron to activate even when all inputs are zero.",
                    },
                    activation: {
                        title: "Activation Function",
                        desc: "Non-linear transformation applied after the weighted sum. Without it, stacking layers would collapse into a single linear function.",
                        linear: "No transformation — output equals the weighted sum z. Useful as a baseline but cannot model non-linear patterns.",
                        relu: "Rectified Linear Unit. Outputs max(0, z). Sparse, efficient, and widely used in deep networks.",
                        sigmoid: "Squashes output to (0, 1). Useful for binary probability outputs, but can cause vanishing gradients.",
                        tanh: "Squashes output to (−1, 1). Zero-centered, often preferred over sigmoid for hidden layers.",
                    },
                    training: {
                        title: "Training",
                        desc: "Adjust the target and learning rate, then step through gradient descent to minimize the loss.",
                        target: "The desired output value (y). The model tries to minimize the squared difference between its prediction and this target.",
                        learningRate: "Learning rate (η). Controls the step size during gradient descent. Too high causes instability; too low slows convergence.",
                        step: "Run one gradient descent step: compute gradients and update w₁, w₂, and b by −η × gradient.",
                        auto: "Run 10 gradient descent steps in sequence to observe how parameters and loss evolve over multiple iterations.",
                        reset: "Reset all parameters and training history to their initial values.",
                        random: "Randomize weights and bias to explore a different region of the loss landscape.",
                        steps: "Total number of gradient descent steps taken so far in this training session.",
                        stepIndex: "Step number in the training history log.",
                        noData: "No training data yet",
                        noDataHint: "Click \"Train 1 Step\" or \"Auto-Train ×10\" to begin",
                        insightsTitle: "Training Insights",
                        runInference: "Run inference to view training data",
                        stats: {
                            finalLoss: { label: "Final Loss", desc: "The error level at the end of training. Lower is better." },
                            steps: { label: "Steps", desc: "How many times the model updated its parameters during training." },
                            batchSize: { label: "Batch Size", desc: "Number of examples processed per gradient update step." },
                            learningRate: { label: "Learning Rate", desc: "Step size for gradient descent. Too high causes instability; too low slows convergence." },
                            parameters: { label: "Parameters", desc: "Total number of learnable weights in the model." },
                        },
                    },
                    visualization: {
                        sum: "Weighted sum node (Σ). Computes z = w₁x₁ + w₂x₂ + b before the activation function is applied.",
                        output: "Final prediction ŷ = activation(z). This is the value the network outputs after applying the non-linearity.",
                        loss: "Mean squared error loss: L = (ŷ − target)². Measures how far the prediction is from the desired target.",
                        activationNode: "Activation function node. Applies the selected non-linearity to the weighted sum z.",
                        activationCurve: "The activation function curve. The dot shows the current input z and its corresponding output f(z).",
                        equation: "Full forward-pass equation: multiply each input by its weight, add the bias, then apply the activation function.",
                        lossCurve: "Loss over training steps. A descending curve indicates the model is learning — parameters are converging toward the target.",
                        lossCurveLabel: "Loss over training steps",
                        lossTooltipTitle: "What is Loss?",
                        lossTooltipErrorLabel: "Prediction Error",
                        lossTooltipError: "Loss measures how \"surprised\" the model is. A high loss means it's guessing wrong frequently.",
                        lossTooltipBenchmarkLabel: "The Benchmark",
                        lossTooltipBenchmark: "Pure random guessing gives a loss of ~4.56 (−ln(1/96)). Anything lower means the model has actually learned something.",
                        lossTooltipCaption: "The descending curve shows the model slowly discovering patterns in your text.",
                    },
                    tabs: {
                        perceptron: "Visualize the single-neuron forward pass: inputs are scaled by weights, summed with a bias, then passed through an activation.",
                        activation: "Explore how the chosen activation function transforms the weighted sum z into the final prediction ŷ.",
                        gradients: "Inspect the chain-rule gradient flow and see exactly how each parameter will be updated in the next training step.",
                        training: "Track loss and parameter evolution across training steps to observe gradient descent in action.",
                    },
                    gradients: {
                        forwardPass: "Forward pass: compute z, apply activation, and calculate the loss from the current prediction and target.",
                        forwardPassLabel: "Forward Pass",
                        chainRule: "Backpropagation via the chain rule: decompose ∂L/∂w into a product of local gradients through each node.",
                        chainRuleLabel: "Gradients (Chain Rule)",
                        weightUpdate: "Proposed parameter update: new value = old value − η × gradient. Applied when you click Train 1 Step.",
                        weightUpdateLabel: "Proposed Weight Update",
                        linearSum: "Linear pre-activation: z = w₁x₁ + w₂x₂ + b. The raw weighted sum before the activation function.",
                        linearSumLabel: "Linear sum",
                        prediction: "Prediction ŷ = activation(z). The output of the neuron after applying the non-linear activation function.",
                        predictionLabel: "Prediction",
                        loss: "Loss L = (ŷ − target)². Squared error between the prediction and the desired target value.",
                        lossLabel: "Loss",
                    },
                    buttons: {
                        trainStep: "Train 1 Step",
                        autoTrain: "Auto-Train ×10",
                        reset: "Reset",
                        random: "Random",
                    },
                    tabLabels: {
                        perceptron: "Perceptron",
                        activation: "Activation",
                        gradients: "Gradients",
                        training: "Training",
                    },
                    diagram: {
                        caption: "Adjust inputs, weights, and bias to see how the perceptron transforms them into an output.",
                        inputX1: "Input x₁",
                        inputX2: "Input x₂",
                        weightW1: "Weight w₁",
                        weightW2: "Weight w₂",
                        biasB: "Bias b",
                    },
                },
            },
        },
    },
    bigramNarrative: {
        hero: {
            eyebrow: "Understanding Language Models",
            titlePrefix: "The Bigram",
            titleSuffix: "Model",
            description: "A first-principles exploration of the simplest statistical language model — and why it still matters."
        },
        problem: {
            title: "The Problem of Prediction",
            lead: "Language is fundamentally sequential. Every word you read right now is informed by the words that came before it.",
            p1: "This property — that each token in a sequence carries ",
            p1Highlight: "expectations about what follows",
            p2: " — is what makes language both expressive and predictable. It's also what makes it so hard to model computationally.",
            p3: "The central challenge of language modeling is deceptively simple to state:",
            quote: "Given what we have already seen, what should come next?",
            p4: "This question has driven decades of research in ",
            h1: "computational linguistics",
            h2: "information theory",
            h3: "deep learning",
            p5: ". To build a model that can answer it, we need a way to capture the statistical structure of language. Let's start with the simplest possible approach.",
            label: "Foundation"
        },
        coreIdea: {
            label: "Core Idea",
            title: "The Simplest Statistical Idea",
            lead: "What if, instead of trying to understand meaning, we simply observed patterns?",
            p1: "Specifically: ",
            h1: "how often does one character follow another?",
            p2: " This is the core insight behind the Bigram model. It ignores grammar, semantics, and long-range dependencies entirely. It asks only one question: given the current token, what is the probability distribution over the next token?",
            caption: "The Bigram assumption: the next token depends only on the current one.",
            p3: "We model P(x_{t+1} | x_t) — the chance of seeing a particular next token given only the token we just observed. Nothing more, nothing less. This radical simplification is what makes the model both tractable and limited.",
            calloutTitle: "Key Insight",
            calloutP1: "The \"bi\" in Bigram means ",
            calloutH1: "two",
            calloutP2: ". The model considers pairs of tokens — the current one and the next one. It has zero memory of anything before the current token."
        },
        mechanics: {
            label: "Mechanics",
            title: "Building a Transition Table",
            lead: "To learn these probabilities, the model scans through a training corpus and counts every pair of consecutive tokens.",
            p1: "For each token A, it records how often each possible token B appears immediately after it. These counts form a ",
            h1: "matrix",
            p2: " — a two-dimensional table where rows represent the current token and columns represent the next token. Each cell holds the number of times that specific transition was observed in the training data.",
            p3: "The visualization below is a live rendering of this transition matrix. Brighter cells indicate more frequent pairings — patterns the model has learned from real text.",
            calloutTitle: "Reading the Matrix",
            calloutP1: "Each row represents a \"given\" character. Each column represents a \"next\" character. The brightness of a cell encodes how likely that transition is. Notice how some rows are nearly uniform (the model is unsure) while others have sharp peaks (strong preferences)."
        },
        normalization: {
            label: "Normalization",
            title: "From Counts to Probabilities",
            lead: "Raw counts alone don't tell us much. To make predictions, we need to convert them into probabilities.",
            p1: "We do this by ",
            h1: "normalizing each row",
            p2: " of the count matrix — dividing every count by the total number of transitions from that row's token. After normalization, each row sums to 1.0, forming a valid probability distribution.",
            p3: "The model can now make concrete statements: \"After the letter h, there is a 32% chance the next character is e, a 15% chance it's a, and so on.\"",
            p4: "Try it yourself below. Type any text to see what the model predicts will come next — based ",
            h2: "solely on the very last character",
            p5: " of your input."
        },
        sampling: {
            label: "Sampling",
            title: "Generating New Text",
            lead: "Once we have a probability distribution, we can do something remarkable: generate entirely new text.",
            p1: "The process is called ",
            h1: "autoregressive sampling",
            p2: ". Start with a seed character, sample the next one from its probability distribution, then use that new character as the seed for the next step. Repeat indefinitely.",
            calloutTitle: "Temperature",
            calloutP1: "The ",
            calloutH1: "temperature",
            calloutP2: " parameter controls how \"creative\" the generation is. At ",
            calloutH2: "low temperatures",
            calloutP3: ", the model almost always picks the most likely next token. At ",
            calloutH3: "high temperatures",
            calloutP4: ", it samples more uniformly — producing surprising and often nonsensical output.",
            tempP1: "How you sample matters as much as what you learned. A single ",
            tempH1: "temperature",
            tempP2: " parameter scales the logits before the final softmax step. Below 1.0, the distribution sharpens — the model almost always picks its top-ranked character. Above 1.0, the distribution flattens — every character gets a fairer shot, at the cost of coherence.",
            tempBridge: "Now try the generation playground below. The same bigram model produces markedly different text at temperature 0.1 versus 2.5 — not because its knowledge changed, but because its sampling strategy did.",
            softmaxFigureLabel: "Softmax Temperature · Conceptual",
            softmaxFigureHint: "Drag the slider to see how temperature reshapes the same probability distribution.",
            playgroundLabel: "Generation Playground",
            playgroundHint: "Adjust temperature and observe how it affects the creativity of the generated text.",
            p3: "Generate some text below and observe how a model with ",
            h2: "only one character of memory",
            p4: " produces output that is statistically plausible at the character level, yet meaningless at any higher level."
        },
        reflection: {
            label: "Reflection",
            title: "Power and Limitations",
            lead: "The Bigram model is powerful precisely because of its simplicity.",
            p1: "It requires very few parameters — just a V × V matrix, where V is the vocabulary size. It trains instantly. And it provides a clear ",
            h1: "probabilistic baseline",
            p2: " for language generation that every more sophisticated model must beat.",
            calloutTitle: "The Fundamental Limitation",
            calloutP1: "The model has ",
            calloutH1: "no memory beyond a single token",
            calloutP2: ". It cannot learn that \"th\" is often followed by \"e\", because by the time it sees \"h\", it has already forgotten the \"t\". It captures local co-occurrence but nothing about words, phrases, or meaning.",
            p3: "This limitation is exactly what motivates the progression to more sophisticated architectures: ",
            h2: "N-grams",
            p4: " extend the context window, ",
            h3: "neural networks",
            p5: " learn from data instead of counting, ",
            h4: "MLPs",
            p6: " apply that learning to language, and transformers attend to the entire sequence at once.",
            quote: "Each model in this lab builds on the same core question: given context, what comes next?"
        },
        tokens: {
            label: "Representation",
            title: "Representing text",
            lead: "We split text into tokens.",
            charTitle: "Characters:",
            charDesc: "small vocab, easy to see.",
            wordTitle: "Words:",
            wordDesc: "richer, huge vocab.",
            note: "We use characters here.",
            charLevelTitle: "Character-level tokens",
            charLevelBody: "Small, fixed vocabulary of ~96 printable ASCII symbols. Every possible input is representable. Simple to implement, easy to visualize — ideal for understanding the fundamentals of language modeling.",
            wordLevelTitle: "Word-level tokens",
            wordLevelBody: "Richer semantic units that carry more meaning per token. But vocabulary can reach 50,000–500,000 entries, making the transition matrix enormous. Rare words cause sparsity; words unseen during training cause complete failures at inference time.",
            charLimitations: "Character-level models have a small, manageable vocabulary — but they must learn everything from scratch. There are no pre-built notions of words, morphology, or meaning. The model must discover that 't', 'h', 'e' together form a common word purely from co-occurrence statistics.",
            wordLimitations: "Word-level models are more expressive but face a fundamental scalability problem. English has over 170,000 words in common use. A bigram model at the word level would need a 170,000 × 170,000 transition matrix — nearly 29 billion cells — most of which would be empty (never observed in training). This sparsity problem is one of the core motivations for neural language models.",
            whyCharHere: "For this lab, we use character-level tokens. The vocabulary stays small enough to visualize the entire transition matrix at once, making the model's learned knowledge directly inspectable. Every design decision you see here scales directly to word-level and subword-level models — only the vocabulary size changes."
        },
        counting: {
            title: "The Bigram idea",
            lead: "Count pairs: current -> next. More counts = more likely.",
            builderTitle: "Step-by-step builder",
            builderDesc: "Walk through text; each pair adds +1 to a cell.",
            p1: "The core operation is almost embarrassingly simple: scan through the training text one character at a time, and for every consecutive pair (current character → next character), increment a counter. That's it. After scanning millions of characters, these counts encode the statistical structure of the language — which characters tend to follow which others, and how strongly.",
            p2: "The step-by-step builder below makes this concrete. Watch how each character pair in the input text adds exactly one count to the corresponding cell in the matrix. By the end, the matrix is a complete record of every transition observed in the training data.",
            calloutTitle: "Why counting works",
            calloutText: "The Law of Large Numbers guarantees that as training data grows, the observed frequencies converge to the true underlying probabilities of the language. With enough text, the bigram counts become a reliable statistical portrait of character-level patterns."
        },
        matrix: {
            title: "The transition table",
            lead: "Rows = current token, columns = next.",
            desc: "Build below, then see the full matrix."
        },
        probabilities: {
            title: "Counts to probabilities",
            lead: "Normalize each row to 100%.",
            desc: "Model reads last token's row and samples the next.",
            inferenceIntro: "The tool below lets you walk through this inference pipeline step by step: pick any context character, choose how to normalize its row — plain division or softmax — then sample to see which character the model would predict next. Try a few characters and notice how the distribution changes shape depending on what the model saw most often in training.",
            overlayTitle: "Counts -> Probabilities -> Sampling",
            overlayDesc: "Pick token, normalize row, sample next.",
            step1: "1) Row values",
            step2: "2) Normalize",
            step3: "3) Sample next token",
            currentToken: "Current token",
            typeChar: "Type a character",
            normalizeSimple: "Simple normalize",
            softmax: "Softmax",
            sampleNext: "Sample next token",
            mostLikely: "Most likely:",
            remaining: "Remaining:",
            stochastic: "Sampling is random."
        },
        limitations: {
            title: "Limitations",
            lead: "Bigram has no memory—only the last token.",
            desc: "No long context. Hence N-grams and neural nets."
        },
        textToNumbers: {
            label: "How Computers See Text",
            title: "Turning Text Into Numbers",
            lead: "A computer can't read letters like you do. It needs numbers it can store and compare.",
            p1: "We give each character a number, like a seat number in a theater. Now the model can count which numbered seat tends to come next.",
            bridge: "Once text becomes numbers, we can build a big table of counts. Then we can turn that table into chances for the next letter.",
        },
        cliffhanger: {
            label: "The Trap",
            title: "One Big Problem",
            lead: "This model only remembers one letter. That is a tiny memory.",
            p1: "If you see 't' and then 'h', you want the model to remember both. But a bigram model forgets the 't' as soon as it sees 'h'.",
            hookLine: "So what happens if we let the model remember more than one letter?",
        },
        cta: {
            freeLabButton: "Explore in Free Lab",
            nextTitle: "What If We Remember More?",
            nextDesc: "Meet the N-gram model — a next-letter guessing machine that looks at the last few characters, not just one.",
        },
        footer: {
            text: "Next, we'll grow this idea into an N-gram model — an extended version that remembers more than one previous character.",
            brand: "LM-Lab · Educational Mode"
        }
    },
    bigramBuilder: {
        description: "We build the bigram matrix by scanning the text character by character. For each pair of consecutive characters (current → next), we increment the cell [current, next]. This table captures how often each character is followed by another.",
        placeholder: "Type text here...",
        hint: "Enter some text to see how the bigram matrix is constructed.",
        buttons: {
            build: "Build Bigram Matrix",
            next: "Next Step",
            autoPlay: "Auto Play",
            pause: "Pause",
            instant: "Instant Complete",
            reset: "Reset Steps"
        },
        vocab: "Educational vocabulary",
        normalized: "Normalized text:",
        empty: "(empty after filtering)",
        skipped: "Showing the first {max} unique characters for clarity ({count} unique character(s) omitted).",
        step1: "Step",
        step2: "updates cell [",
        step3: "].",
        currentStep: "Current Step",
        updatingCell: "Updating cell at row",
        updatingCellCol: "col",
        pressBuild: "Press Build Bigram Matrix and start stepping through character pairs.",
        table: {
            curnxt: "cur \\ nxt"
        }
    },
    ngramNarrative: {
        hero: {
            eyebrow: "Understanding Language Models",
            titlePrefix: "What If We",
            titleSuffix: "Remember More?",
            description: "The bigram model could only see one character behind. What happens when we give it two? Three? Five? The answer is both thrilling and devastating.",
        },
        moreContext: {
            label: "More Context",
            title: "Beyond a Single Character",
            lead: "You saw that the bigram model can only look at one character behind. What if we let it look at two? Three? Five?",
            p1: "An N-gram model looks at the",
            p1Highlight: "previous N characters",
            p1End: " before it guesses the next one. Example: N=2 means it can see two characters of context.",
            p2: "More context makes guesses smarter. After \"th\", the model can strongly expect \"e\" — it has seen that pattern many times.",
            p3: "But more memory has a hidden cost. We are about to watch that cost grow faster than your intuition expects.",
            calloutTitle: "The N-gram Assumption",
            calloutText: "The key assumption: the next character depends only on the previous N characters. Everything before that is forgotten. It's like a sliding window — and the question is: how big should it be?",
        },
        contextWindow: {
            label: "Context Window",
            title: "Seeing More of the Past",
            lead: "The context window is how many previous characters the model can \"see\" before it makes a guess.",
            caption: "As the window gets larger, the model can use richer patterns. But the number of possible windows grows extremely fast.",
            hint: "Watch how the context grows as N increases.",
            p1: "Each step up in N gives the model more clues. It also creates many more situations the model might need to remember later.",
        },
        howItWorks: {
            label: "Mechanics",
            title: "Counting with Context",
            lead: "The counting process is the same as in bigrams — but now instead of counting pairs, we count longer groups: the N-character context plus what comes next.",
            p1: "For each spot in the training text, the model takes the",
            p1Highlight: " N-character context",
            p1End: " and counts what character follows. Later, it uses those counts like a lookup table: find the context, then read the usual next character.",
            p2: "As N grows, the counting table gets more dimensions. With N=1 (bigram), it's a simple grid. With N=2, imagine a stack of grids — one for each possible two-character context. The table grows in every direction.",
        },
        improvement: {
            label: "Improvement",
            title: "The Prediction Gets Better",
            lead: "Now for the payoff. When the model sees more context, its guesses become much more confident and correct.",
            example: "After \"h\", the next letter is unclear. After \"th\", \"e\" becomes very likely. After \"the\", a space becomes very likely.",
        },
        statistical: {
            label: "Statistical Nature",
            title: "A Purely Statistical Model",
            lead: "N-gram models have no understanding of language. They are sophisticated counting machines.",
            p1: "Every prediction is a",
            p1Highlight: "table lookup",
            p1End: " — the model finds the matching context in its table and returns the stored probability distribution. There are no learned parameters, no gradients, no optimization.",
            p2: "This makes N-grams extremely fast at inference and trivially interpretable: you can always ask \"why did the model predict X?\" and trace the answer back to exact training examples.",
            calloutTitle: "No Generalization",
            calloutText: "If the model has never seen a particular context in training, it has zero information about what comes next. Unlike neural networks, N-grams cannot generalize from similar contexts — each context is treated as completely independent.",
        },
        complexity: {
            label: "Complexity",
            title: "The Price of Memory",
            lead: "Here's where the math turns against us.",
            p1: "With 96 possible characters, every extra character of context multiplies the table by 96. N=1: 96 contexts. N=2: 9,216. N=3: 884,736. N=4: 85 million. N=5: over 8 billion.",
            p1Highlight: " 884,736",
            p1End: ". A 5-gram has over 84 million. Most of these contexts will never appear in any realistic training corpus.",
            p2: "Most of those contexts never appear in real text. That means most of the table is empty — this is called sparsity — and empty rows cannot guide predictions.",
            vocabCalloutTitle: "And it gets much worse with words",
            vocabCalloutText: "This lab uses characters (~96 possible). Real language models use words instead. With 50,000 words, even a bigram matrix needs 2.5 billion cells. A trigram table would need 125 trillion. The math turns catastrophic extremely fast.",
            comparisonLabel: "N-Gram Comparison · Live backend metrics",
            comparisonHint: "Compare perplexity, context utilization, and state space across different values of N.",
            metricsLegend: {
                perplexity: "Perplexity means \"how surprised the model is\" on average; lower means better guesses and more confidence.",
                utilization: "Context utilization means how much of the huge table was actually filled by the training text; low utilization means many contexts were never seen.",
                contextSpace: "Context space means how many different contexts could exist in theory; it grows extremely fast as N increases.",
            },
        },
        vocabulary: {
            label: "Vocabulary",
            title: "Characters vs. Words",
            lead: "We use character-level tokens in this lab, but real-world N-grams often operate on words — making the explosion even worse.",
            p1: "With a word vocabulary of 50,000 tokens, even a",
            p1Highlight: " bigram matrix needs 2.5 billion cells",
            p1End: ". A trigram table would require 125 trillion entries. This is why word-level N-grams beyond N=3 are essentially impractical without aggressive smoothing and pruning.",
            p2: "Character-level models keep the vocabulary small (~96), making it feasible to visualize and explore the full table. But the tradeoff is that individual characters carry almost no semantic meaning.",
        },
        noUnderstanding: {
            label: "Limitations",
            title: "No True Understanding",
            lead: "N-gram models capture local co-occurrence patterns but have no notion of meaning, grammar, or long-range coherence.",
            p1: "The model treats \"the cat sat on the\" and \"the dog sat on the\" as completely unrelated contexts (for N < full sentence length). It cannot recognize that both involve an animal sitting on something.",
            p2: "This inability to",
            p2Highlight: "generalize across similar contexts",
            p2End: " is what ultimately limits N-gram models. No matter how much data you collect, there will always be valid contexts the model has never seen.",
            p3: "This fundamental limitation is exactly what motivates the transition to neural approaches — models that learn dense, continuous representations capable of recognizing similarity between contexts.",
        },
        deeperProblem: {
            label: "Limitations",
            title: "The Deeper Problem",
            lead: "The explosion is a practical problem. But there's a philosophical one that's even worse.",
            p1: "Imagine the text starts with \"the cat sat on the\". If the model has seen that exact context, it can predict what comes next from memory.",
            p2: "Now change one word: \"the dog sat on the\". A human sees it is almost the same. The N-gram model treats it like a totally new situation.",
            p3: "N-grams have no concept of 'similar.' The contexts 'the cat' and 'the dog' are as different to the model as 'the cat' and 'xyzq'. Each is a separate row in the table, with no connection.",
            calloutTitle: "No Generalization",
            calloutText: "If the model has never seen a particular sequence in training, it has nothing to say. It can't guess. It can't reason by analogy. It just shrugs.",
        },
        endOfCounting: {
            label: "Reflection",
            title: "The End of Counting",
            lead: "We've reached the end of what counting can do.",
            p1: "We started with bigrams, which remember one character. We pushed to N-grams, which remember more, and we watched predictions improve.",
            p2: "Then we hit two walls. The explosion wall: the table grows too fast to fill. More memory multiplies the table again and again.",
            p3: "The generalization wall: each context is an island. The model cannot share knowledge between similar contexts, so it fails on new phrases.",
            quote: "The era of counting is over. The era of learning begins.",
            hookLine: "In the next chapter, we stop counting. We start learning.",
        },
        conclusion: {
            label: "Reflection",
            title: "The Bridge to Neural Models",
            lead: "N-gram models push statistical language modeling to its logical extreme — and reveal why a fundamentally different approach is needed.",
            p1: "We have seen that increasing context improves predictions but triggers an exponential explosion in the state space. This is not a bug — it is an inherent property of discrete, count-based models.",
            p2: "The core problem is representation: N-grams represent each context as an isolated point in a vast discrete space. There is no notion of similarity between contexts, no way to share statistical strength between related patterns.",
            p3: "Neural language models solve this by mapping discrete tokens into continuous vector spaces where similar contexts live near each other. This allows them to generalize from seen examples to unseen but similar contexts.",
            p4: "The progression from Bigram → N-gram → Neural Network is not just historical — it reflects a deepening understanding of what it means to model language computationally.",
            quote: "The curse of dimensionality is not a failure of N-grams — it is the reason neural representations were invented.",
        },
        cta: {
            title: "Continue Exploring",
            labButton: "Open Free Lab",
            labDesc: "Switch to Free Lab mode to change N, test your own phrases, and see where the model becomes silent.",
            neuralButton: "Next: From Counting to Learning",
            neuralDesc: "We've pushed counting to its limit. Now we build something that learns.",
        },
        footer: {
            text: "The statistical era is complete. You've seen what counting can do — and where it breaks. Next: models that learn.",
            brand: "LM-Lab · Educational Mode",
        },
    },
    neuralNetworkNarrative: {
        hero: {
            eyebrow: "Chapter 3 · From Counting to Learning",
            titlePrefix: "Neural",
            titleSuffix: "Networks",
            description: "Counting hit a wall. N-gram tables grow too fast to fill, and they cannot share knowledge between similar patterns. What if, instead of a lookup table, we used a small set of adjustable numbers — learning parameters — that improve with every example? A system that discovers rules, not just memorizes facts. That is generalization. That is a neural network.",
        },
        history: {
            title: "The History of Neural Networks",
            summary: "From 1943 to modern deep learning — a story of breakthroughs, winters, and persistence.",
            p1: "In 1943, McCulloch and Pitts proposed that a brain cell could be modeled as a logical gate: receive signals, and if strong enough, fire. They showed networks of these units could compute anything.",
            p2: "In 1958, Frank Rosenblatt built the Mark I Perceptron at Cornell — the first machine that learned from experience. It adjusted its own parameters to recognize simple shapes.",
            p3: "In 1969, Minsky and Papert proved single-layer perceptrons had fundamental limits. They could not learn XOR. Funding dried up. The AI Winter lasted nearly two decades.",
            p4: "In 1986, Rumelhart, Hinton, and Williams published their paper on backpropagation — the algorithm that showed multi-layer networks could learn. The thaw began.",
            p5: "It took another 25 years, massive datasets, and the GPU revolution before deep learning conquered the world. But the seed planted in 1943 never stopped growing.",
        },
        artificialNeuron: {
            title: "The Artificial Neuron",
            lead: "Every neural network is built from one tiny unit: the artificial neuron. It takes numbers in, scales them, sums them up, and fires a result. Master this, and the rest follows.",
            p1: "A neuron receives inputs — just numbers. Each input passes through a",
            p1Highlight: "weight",
            p1End: ", a dial that controls how much that input matters. Think of a mixing board: each channel has its own volume knob. Turn a knob up and that input gets louder. Turn it to zero and the input is muted. Turn it negative and the signal flips.",
            p2: "After scaling every input by its weight, the neuron sums them and adds one more number: the bias. The bias is the neuron's baseline tendency — how eager it is to fire before seeing any input. A positive bias means \"I lean toward yes.\" A negative bias means \"convince me.\"",
            formulaCaption: "Multiply each input by its weight, sum everything, add the bias, then apply an activation function f. This is the atomic operation of every neural network.",
            p3: "With the right weights and bias, a single neuron can",
            p3Highlight: "draw a straight line through data and classify everything on one side as A and the other as B",
            p3End: ". Training is how the neuron finds those right numbers — starting from random guesses and improving step by step.",
            calloutTitle: "What are parameters?",
            calloutText: "Weights and biases together are called parameters. At the start of training they are random noise. By the end they encode everything the network has learned — stored as nothing more than a list of decimal numbers. The process of finding good values is called training.",
        },
        nonLinearity: {
            title: "Why Non-Linearity?",
            lead: "A neuron computes a weighted sum — a straight-line operation. Stack two straight-line operations and you still get a straight line. Without a curve, depth is useless.",
            p1: "If every layer just multiplied and added, stacking ten layers would collapse into one. A hundred layers would still act like one. A linear function of a linear function is still linear. To learn curves, thresholds, and context-dependent patterns, we need something to bend the signal.",
            p2: "That something is the activation function. It sits at every neuron's output and introduces a non-linear shape. The neuron computes its weighted sum, and the activation function decides how to express it. Explore the curves below — drag the slider to see how each function transforms the input.",
            p3: "Different functions have different personalities.",
            p3Highlight: "ReLU passes positives unchanged and kills negatives. Sigmoid squashes everything into 0–1. Tanh squashes into −1 to 1.",
            p3End: "The choice matters: ReLU made deep networks practical by avoiding a training problem called vanishing gradients, where error signals shrink to near-zero in early layers.",
        },
        howItLearns: {
            title: "How a Network Learns",
            lead: "A network starts as random noise. It learns by making predictions, measuring how wrong they are, and tracing that error backward to fix every weight. This is backpropagation.",
            p1: "Learning begins with failure. The network sees an input, makes a guess, and we measure the mistake with a",
            p1Highlight: "loss function — a single number that says how wrong we are",
            p1End: ". Higher loss means worse predictions. The goal of training is to shrink this number as much as possible.",
            workedTitle: "A Concrete Example",
            workedIntro: "Let us trace one full training step with real numbers.",
            workedForward: "Forward pass: input x = 1.0, weight w = 0.50, bias b = −0.20. Weighted sum: z = 0.50 × 1.0 + (−0.20) = 0.30. After sigmoid: ŷ = σ(0.30) = 0.5744.",
            workedLoss: "Target is 0.80. Loss = (0.5744 − 0.80)² = 0.0509.",
            workedBackward: "Backward pass (chain rule, each step): ∂L/∂ŷ = 2(0.5744 − 0.80) = −0.4512. ∂ŷ/∂z = 0.5744 × (1 − 0.5744) = 0.2445. Multiply: ∂L/∂z = −0.4512 × 0.2445 = −0.1103. Since ∂z/∂w = x = 1.0, the weight gradient is ∂L/∂w = −0.1103.",
            workedUpdate: "Update: w_new = 0.50 − 1.0 × (−0.1103) = 0.6103. The weight moved toward the target. One step done.",
            p2: "That chain of multiplications is the chain rule.",
            p2Highlight: "Backpropagation is the chain rule applied efficiently from output to input.",
            p2End: " Each node passes its local gradient backward, and we multiply through the entire graph.",
            formulaCaption: "The update rule in shorthand: subtract the learning rate η times the gradient. This one line captures the entire worked example above.",
            p3: "The learning rate η controls step size. Too large and you overshoot the optimum. Too small and training crawls. Each cycle — forward, loss, backward, update — nudges every weight toward better predictions. Repeat thousands of times and random noise crystallizes into knowledge.",
        },
        watchingItLearn: {
            title: "Watching It Learn",
            lead: "Theory is one thing. Watching it happen is another. The demo below runs real training steps on a single neuron.",
            p1: "Press the training button and observe. The loss should drop. The prediction should creep toward the target. Each click runs one cycle of forward pass, backpropagation, and weight update — the same process described in the previous section, but live.",
            p2: "Pay attention to how the weights change. Early steps produce large swings because the gradients are steep. Later steps produce tiny refinements as the neuron settles into a good solution. This is gradient descent in action.",
        },
        bridge: {
            title: "The Bridge: Tables to Parameters",
            lead: "Here is the payoff. A single-layer neural network trained to predict the next character converges to the exact same probabilities as a bigram counting table. Counting and learning arrive at the same answer.",
            p1: "A bigram model stores one count per character pair. A neural network stores shared parameters that encode knowledge about all pairs at once. When you add a hidden layer, the network goes further: it learns that vowels behave similarly, that certain consonant clusters share patterns. Knowledge about \"a\" transfers to \"e\" because they activate similar neurons.",
            p2: "The visualization below shows both systems side by side.",
            p2Highlight: "Watch the neural network's predictions converge toward the bigram table",
            p2End: " — then surpass it, because learned parameters generalize where raw counts cannot.",
            insightTitle: "From Tables to Representations",
            insightText: "A bigram table stores 9,216 independent counts. A neural network with a small hidden layer stores fewer parameters — but organized so that similar characters share structure. This is the seed of the idea that becomes word embeddings, attention, and modern language models.",
            p3: "This bridge — from counting to learning — is the most important conceptual leap in language modeling. Everything that follows builds on it.",
        },
        powerAndLimits: {
            title: "Power, Limits, and What Comes Next",
            lead: "A single neuron draws straight lines. That is both its power and its limit.",
            p1: "Consider the XOR problem: four points on a 2D plane where opposite corners share a label. No single straight line can separate them.",
            p1Highlight: "A single neuron will fail at XOR no matter how long you train it.",
            p1End: "This was proven in 1969, and it froze neural network research for nearly two decades.",
            p2: "The fix is simple: stack neurons into layers. One hidden layer of just two neurons can solve XOR. The first layer splits the space into curved regions; the second combines them. A network with one large-enough hidden layer can approximate any continuous function — the Universal Approximation Theorem.",
            p3: "But a single neuron or a shallow network still processes a fixed-size input. It has no memory across time steps and no way to focus on the most relevant parts of its context. For language, this matters: pronouns refer to nouns many tokens back. A fixed window cannot reliably bridge that gap.",
            p4: "The building blocks you have learned — weighted sums, activations, backpropagation, gradient descent — are the same blocks inside every modern AI system. The next step is to stack them into a multi-layer perceptron and apply them directly to language.",
            calloutTitle: "Next: MLP Language Model",
            calloutText: "In the next chapter, we replace the N-gram lookup table with a neural network that uses dense embeddings and learned weights. You will see an MLP generalize beyond exact matches and produce better text with fewer parameters.",
        },
        cta: {
            title: "Continue the Journey",
            subtitle: "You understand the building blocks. Next: apply them to language.",
            labButton: "Open Free Lab",
            labDesc: "Experiment with perceptrons, activations, and training in the interactive playground.",
            mlpButton: "Next: Building a Language Model",
            mlpDesc: "Stack neurons into layers, add embeddings, and build a real character-level language model.",
        },
        footer: {
            text: "From counting to learning — you now understand the core building blocks of neural networks. Next: stack them into layers and apply them to language.",
            brand: "LM-LAB · Educational Mode"
        }
    }
};
