export const en = {
    common: {
        language: "Language",
        loading: "Loading...",
        error: "Error",
        comingSoon: "Coming Soon",
        backToProjects: "Back to Projects",
        toggleLanguage: "Toggle Language",
        code: "Code",
        liveDemo: "Live Demo",
        viewCaseStudy: "View Case Study",
    },
    nav: {
        home: "Home",
        projects: "Projects",
        lab: "Lab",
        notes: "Notes",
    },
    lab: {
        bigram: "Bigram",
        ngram: "N-Gram",
        mlp: "MLP",
        transformer: "Transformer",
        neuralNetworks: "Neural Nets",
        shell: {
            allModels: "Back to Lab",
        },
        active: "Lab Active",
        waking: "Waking Up",
        serverWarning: {
            title: "BACKEND COLD-START DETECTED",
            subtitle: "CONTAINMENT PROTOCOL ACTIVE",
            message: "The server is waking up from hibernation. Free-tier Render instances spin down after inactivity — yes, I'm hosting this on a free server because I'm a broke student.",
            donate: "If the 30s wait is too painful, feel free to sponsor my coffee fund so I can afford a real server. Or just wait, it's free entertainment.",
            status: "ATTEMPTING CONNECTION",
            dismiss: "I'LL SURVIVE",
            connected: "SIGNAL ACQUIRED",
        },
        mode: {
            educational: "Educational",
            educationalDescription: "Guided learning experience with story-driven explanations and progressive reveals.",
            freeLab: "Free Lab",
            freeLabDescription: "Full access to all tools and visualizations for manual experimentation and analysis.",
            selectViewingMode: "Select Viewing Mode",
            availableModels: "Available Models",
        },
        status: {
            ready: "Ready",
            coming: "Coming",
        },
        models: {
            bigram: {
                name: "Bigram Explorer",
                subtitle: "The Simplest Statistical Model",
                description: "Explore the foundational building block of language modeling. Learn how character-to-character transition probabilities form a complete probabilistic model with zero memory beyond the immediate predecessor. Visualize the transition matrix, run live inference, and generate text through autoregressive sampling.",
            },
            ngram: {
                name: "N-Gram Lab",
                subtitle: "Variable Context Windows",
                description: "Extend the Bigram model by conditioning on N previous characters instead of just one. Observe how increasing context size improves predictions but triggers exponential growth in the state space (|V|^N). Experience the fundamental tradeoff between expressiveness and data sparsity that motivated neural approaches.",
            },
            mlp: {
                name: "MLP Neural Network",
                subtitle: "From Counting to Learning",
                description: "Transition from discrete lookup tables to continuous learned representations. Discover how multi-layer perceptrons use dense embeddings and non-linear activations to generalize beyond exact n-gram matches. Understand backpropagation, gradient descent, and the shift from statistical to neural language modeling.",
            },
            transformer: {
                name: "Transformer Architecture",
                subtitle: "Attention is All You Need",
                description: "Explore the architecture that revolutionized modern AI. Learn how self-attention mechanisms allow the model to dynamically weight the importance of every token in the sequence, eliminating fixed context windows. Visualize attention patterns, positional encodings, and the multi-head attention mechanism that powers GPT and BERT.",
            },
            neuralNetworks: {
                name: "Neural Networks",
                subtitle: "Foundations of Deep Learning",
                description: "Build intuition for artificial neural networks from first principles. Understand perceptrons, activation functions, weight matrices, and how gradient-based optimization enables learning. See how simple mathematical units combine to form powerful function approximators capable of modeling complex patterns in data.",
            },
        },
        dashboard: {
            chip: "Model Interpretability Lab",
            suite: "Suite",
            description1: "Explore the inner workings of language models through interactive visualizations.",
            description2: "Follow a guided path or experiment freely in the sandbox.",
            launchUnit: "LAUNCH UNIT",
            secureLock: "SECURE LOCK",
            footerCopyright: "© 2026 LM-LAB INSTRUMENTS",
            footerSystem: "INTERPRETABILITY_SYSTEM",
            secureConnection: "Secure Connection",
            hardwareMock: "Hardware: v4-8 TPU MOCK",
        },
        placeholders: {
            mlp: {
                title: "MLP Explorer",
                description: "Multi-Layer Perceptron language model explorer. Currently under development - check back soon.",
            },
            transformer: {
                title: "Transformer Explorer",
                description: "Attention-based transformer model explorer. Currently under development - check back soon.",
            },
        },
        landing: {
            hero: {
                badge: "Research Unit",
                subtitle: "Interactive Interpretability Lab",
                description: "Demystifying Language Models through first-principles engineering and visual proof.",
                subDescription: "This unit focuses on mechanistic interpretability: the reverse-engineering of neural weights into understandable human concepts.",
                start: "Initialize Base Model",
                recommended: "Recommended for beginners",
            },
            highlights: {
                visualizations: "Interactive Viz",
                inference: "Live Inference",
                guided: "Guided Path",
                backend: "PyTorch Backend",
            },
            learningPath: {
                title: "Learning Path",
                status: {
                    soon: "Developing",
                    ready: "Unit Active",
                },
            },
            modes: {
                title: "Laboratory Protocols",
                entryTitle: "Choose Your Experience",
                entrySubtitle: "Select how you want to explore the lab. You can change this at any time.",
                defaultNote: "Defaulting to Educational Mode",
                educational: {
                    title: "Educational Mode",
                    subtitle: "Guided Discovery",
                    description: "Step-by-step narrative explaining the 'why' behind the math. Best for conceptual learning.",
                    tag: "Recommended",
                    features: ["Narrative walkthroughs", "Progressive reveals", "Conceptual explanations"],
                },
                freeLab: {
                    title: "Free Lab Mode",
                    subtitle: "Sandbox Environment",
                    description: "Full access to all visualization tools and generation parameters. For advanced experimentation.",
                    tag: "Advanced",
                    features: ["All tools unlocked", "Raw parameter control", "No guided flow"],
                },
                cta: "Start with Bigram",
                ctaSubtext: "The simplest model — the best starting point",
                changeMode: "Change mode",
                selectedMode: "Selected",
            },
            availableModels: {
                title: "Biological Units Available",
                enter: "Enter Lab",
                locked: "Protocol Restricted",
            },
            footer: {
                text: "Scientific Instrument v2.2 // Build 2026",
            },
        },
    },
    training: {
        title: "Training Insights",
        noData: "Run inference to view training data",
        tooltip: {
            lossTitle: "What is Loss?",
            lossErrorPrefix: "Prediction Error:",
            lossError: "Loss measures how 'surprised' the model is. A high loss means it's guessing wrong frequently.",
            lossBenchmarkPrefix: "The Benchmark:",
            lossBenchmark: "Pure random guessing would give a loss of ~4.56 (-ln(1/96)). Anything lower means the model has actually learned something!",
            lossCurve: "The descending curve shows the model slowly discovering patterns in your text.",
        },
        stats: {
            finalLoss: "Final Loss",
            steps: "Steps",
            batchSize: "Batch Size",
            learningRate: "Learning Rate",
            parameters: "Parameters",
            tooltips: {
                finalLoss: "The error level. At the end of training, it should be as low as possible.",
                steps: "How many times the model practiced to improve its predictions.",
                batchSize: "The amount of information pieces the model processes at once.",
                learningRate: "The learning speed. Neither too fast to avoid missing, nor too slow to avoid taking too long.",
                parameters: "The size of the neural network or 'brain' of the model.",
            },
        },
    },
    ngram: {
        training: {
            title: "Training Insights",
            stats: {
                totalTokens: "Total Tokens",
                uniqueContexts: "Unique Contexts",
                utilization: "Context Utilization",
                sparsity: "Sparsity",
                transitionDensity: "Transition Density",
                subs: {
                    possiblePrefix: "of",
                    possibleSuffix: "possible",
                    fractionObserved: "Fraction of contexts observed",
                    unseen: "Unseen context fraction",
                },
            },
        },
    },
    landing: {
        hero: {
            status: "System Online :: v2.2",
            role: "Research & Engineering",
            title: "ADRIAN LAYNEZ ORTIZ",
            tagline1: "Mathematics & Computer Science.",
            tagline2: "Mechanistic Interpretability · High-Performance Engineering.",
            cta: {
                lab: "View Lab Work",
                notes: "Read Notes",
            },
        },
        metrics: {
            research: "Years of Research",
            repos: "Open Source Repos",
            projects: "Active Projects",
            curiosity: "Curiosity",
        },
        about: {
            badge: "About",
            building: "Currently Building",
            projectTitle: "Deep Learning Engine — CUDA / C++",
            projectDesc: "Custom kernels for matrix operations and backpropagation",
            bio: {
                titlePrefix: "Bridging Abstract Mathematics",
                titleSuffix: "& Machine Intelligence",
                p1: "I am pursuing a double degree in <strong class='text-foreground'>Mathematics and Computer Science</strong> at the Universidad Complutense de Madrid. My research focuses on understanding neural networks at their deepest level — from gradient dynamics to kernel-level optimization.",
                p2: "I specialize in <strong class='text-foreground'>Mechanistic Interpretability</strong> — the science of reverse-engineering how neural networks represent and process information internally. Rather than treating models as black boxes, I decompose their circuits to understand <em class='text-foreground/80'>why they work</em>.",
                mission: "My mission: make AI systems transparent through rigorous mathematical analysis and low-level engineering.",
            },
        },
        skills: {
            title: "Technical Proficiencies",
            linearAlgebra: "Linear Algebra",
            topology: "Topology",
            convexOpt: "Convex Optimization",
        },
        work: {
            badge: "Selected Work",
            titlePrefix: "Engineering from",
            titleSuffix: "First Principles",
            description: "Every project begins with a question. From reimplementing seminal papers to writing bare-metal GPU kernels, each one is an exercise in deep understanding.",
            viewAll: "View All Projects",
            items: {
                nanoTransformer: {
                    title: "Nano-Transformer",
                    desc: "Ground-up reproduction of 'Attention Is All You Need' in PyTorch — Multi-Head Attention, Positional Encodings, and LayerNorm implemented without pre-built Transformer modules.",
                },
                cudaKernels: {
                    title: "CUDA Matrix Kernels",
                    desc: "Handwritten CUDA kernels exploring SGEMM optimization — from naive implementations to tiled shared-memory strategies, benchmarked against cuBLAS.",
                },
                autograd: {
                    title: "Autograd Engine",
                    desc: "Lightweight reverse-mode automatic differentiation library. Dynamically constructs computation graphs and propagates gradients via the chain rule.",
                },
                mathDl: {
                    title: "The Mathematics of Deep Learning",
                    desc: "Interactive articles exploring the rigorous theory behind modern AI — SGD convergence analysis, the linear algebra of LoRA, and differential geometry on neural manifolds.",
                },
                distributed: {
                    title: "Distributed Inference",
                    desc: "Architectural explorations in data-parallel training, model sharding, and optimized inference pipelines for large-scale neural networks.",
                },
            },
        },
        contact: {
            badge: "Open to Opportunities",
            titlePrefix: "Let's Build",
            titleMiddle: "Something",
            titleSuffix: "Together",
            description: "Whether it's a research collaboration, an internship opportunity, or just a conversation about the mathematics of intelligence — I'd love to hear from you.",
            email: "Get in Touch",
            github: "GitHub Profile",
            githubShort: "GitHub",
            linkedin: "LinkedIn",
        },
    },
    projects: {
        hero: {
            badge: "Research & Development",
            titlePrefix: "Constructing the",
            titleSuffix: "Digital Frontier.",
            description: "A curated collection of my work in distributed systems, AI infrastructure, and high-performance computing.",
        },
        flagship: {
            badge: "Flagship Project",
            featured: "Featured",
            liveDemo: "Live Demo Available",
            title: "LM-Lab",
            description: "An interactive platform for exploring language model architectures from first principles. Visualize transition matrices, probe inference dynamics, and generate text — all powered by a live FastAPI backend with PyTorch.",
            highlights: {
                inference: {
                    title: "Live Inference",
                    desc: "Real-time next-character prediction with probability distributions",
                },
                matrix: {
                    title: "Transition Matrix",
                    desc: "Interactive canvas heatmap of the learned bigram probabilities",
                },
                generation: {
                    title: "Text Generation",
                    desc: "Generate text with configurable temperature and step-by-step tracing",
                },
            },
            cta: {
                explorer: "Open Lab",
                architecture: "View Architecture",
                demo: "Run Interactive Demo",
            },
        },
        experiments: {
            title: "Selected Experiments",
            items: {
                distriKv: {
                    title: "Distri-KV",
                    desc: "A distributed key-value store implemented in Go, featuring Raft consensus and sharding.",
                },
                neuroVis: {
                    title: "NeuroVis",
                    desc: "Interactive visualization tool for neural network activations in real-time.",
                },
                autoAgent: {
                    title: "Auto-Agent",
                    desc: "A lightweight autonomous agent framework focused on coding tasks.",
                },
            },
        },
    },
    notes: {
        hero: {
            est: "EST. 2024",
            archive: "RESEARCH ARCHIVE",
            titlePrefix: "The Engineering",
            titleSuffix: "Logbook",
            description: "Explorations in <strong class='text-primary'>distributed intelligence</strong>, high-dimensional topology, and the mechanics of modern software.",
        },
        featured: {
            badge: "LATEST RESEARCH",
            readTime: "{minutes} min read",
            figure: "Figure 1.0: Latent Space Visualization",
        },
        grid: {
            title: "Previous Entries",
        },
        backToNotes: "Back to Notes",
        noteNotFound: "Note Not Found",
    },
    footer: {
        builtBy: "Built by",
        sourceAvailable: "The source code is available on",
    },
    datasetExplorer: {
        title: "Corpus Evidence",
        subtitle: "Why did the model learn '{context}' -> '{next}'?",
        scanning: "Scanning training corpus...",
        occurrencesFound: "Occurrences Found",
        source: "Source",
        contextSnippets: "Context Snippets",
        noExamples: "No examples found for this transition.",
        fetchError: "Failed to fetch dataset examples",
        explorerTitle: "Corpus Explorer",
        searching: "Searching Dataset...",
        querySequence: "Query Sequence",
        found: "Found {count} occurrences",
        exampleContexts: "Example Contexts",
        noExamplesValidation: "No examples found in the validation snippet.",
    },
    models: {
        bigram: {
            title: "Bigram Language Model",
            description: "The fundamental building block of sequence modeling. A probabilistic model that predicts the next character based solely on the immediate predecessor.",
            params: "Parameters",
            vocab: "Vocabulary",
            trainingData: "Training Data",
            loss: "Final Loss",
            unknown: "Unknown",
            tooltips: {
                params: "They are like the brain's connections. This model is simple, so it doesn't need many.",
                vocab: "It's the set of letters and symbols the model knows, like its own alphabet.",
                trainingData: "The amount of text the model read to learn how to write.",
                loss: "It's the 'error' score. The lower it is, the better the model knows which letter comes next.",
            },
            sections: {
                visualization: {
                    title: "Visualization: Transition Matrix",
                    description: "This is where the model's 'knowledge' lives. For a Bigram model, this grid represents which letters typically follow others.",
                },
                inference: {
                    title: "Inference and Generation",
                    description: "Interact with the model in real-time. Watch how it 'guesses' the next character based on learned probabilities.",
                },
                architecture: {
                    title: "Model Architecture",
                    description: "A technical look at the 'neurons' and layers that process information.",
                },
                training: {
                    title: "Training Insights",
                    description: "Observing the learning process. These metrics show how the model optimized its parameters by reducing prediction error (loss) over 5000 iterations.",
                },
            },
            hero: {
                scientificInstrument: "Scientific Instrument v1.0",
                explanationButton: "Need an intuitive explanation?",
                explanationSub: "Understand the core idea before diving into the math and visualizations.",
            },
            matrix: {
                title: "Transition Matrix",
                activeSlice: "Active Slice Transition",
                tryIt: {
                    label: "Try it:",
                    text: "Click any colored cell in the matrix to see",
                    highlight: "real training examples",
                },
                searchPlaceholder: "Highlight character…",
                runInference: "Run inference to generate the transition matrix",
                tooltip: {
                    title: "How to read this chart?",
                    desc: "Rows represent the current character and columns represent the next character. Brighter cells indicate higher transition probability.",
                    rows: "Rows (Y):",
                    rowsDesc: "The letter the model just wrote.",
                    cols: "Columns (X):",
                    colsDesc: "The letter the model is trying to guess.",
                    brightness: "Brightness:",
                    brightnessDesc: "The brighter a square is, the more likely that pair of letters appears in the text.",
                    example: "Example: If the row is 'q' and the 'u' column shines brightly, it means the model knows that after 'q' almost always comes 'u'.",
                },
                slice: "Slice:",
                datasetMeta: {
                    learnedFrom: "Learned from",
                    summarizes: "summarizes",
                    rawChars: "raw characters",
                    inTrain: "in training split",
                    vocab: "across",
                    symbols: "unique symbols",
                    corpus: "Corpus Name:",
                    rawText: "Total Raw Text:",
                    trainingSplit: "Training Data:",
                    vocabulary: "Vocabulary Size:",
                    charTokens: "characters",
                },
                probFlow: {
                    badge: "Probability Flow Visualizer",
                    alreadyNormalized: "⚠ Matrix appears pre-normalized",
                    description: "Explore how raw counts become probabilities and how the model samples the next token. This interactive diagram shows the complete inference pipeline: from selecting a context character, to normalizing its row into a probability distribution, to stochastically sampling the next token.",
                    step1: "Step 1: Select Context",
                    step2: "Step 2: Normalize",
                    step3: "Step 3: Sample Next Token",
                    currentToken: "Current Token",
                    typeToChange: "Type to change context",
                    normalize: "Normalize",
                    softmax: "Softmax",
                    temperature: "Temperature",
                    educational: {
                        normTitle: "Simple Normalization",
                        normDesc: "Divide each count by the row sum. This converts raw frequencies into probabilities that sum to 1.0.",
                        softmaxTitle: "Softmax (Temperature-Scaled)",
                        softmaxDesc: "Exponentiates values and normalizes. Temperature controls sharpness: low temp → peaked distribution, high temp → uniform distribution.",
                        tempTitle: "Temperature",
                        tempDesc: "Controls the sharpness of the distribution. Low temperature (< 1) concentrates probability on the most likely tokens. High temperature (> 1) spreads it more evenly, producing more varied — and often surprising — output.",
                    },
                    tempLabel: "Temperature",
                    tempTooltip: "Controls randomness. Lower = more deterministic, Higher = more creative/random",
                    sampleButton: "Sample Next Token",
                    sample: "Sample Next Token",
                    sampling: "Sampling...",
                    result: "Sampled Result",
                    sampled: "Sampled",
                    topCandidate: "Top candidate",
                    mostLikely: "Most Likely",
                    probability: "Probability",
                    roll: "Random Roll",
                    explanation: "The model threw a weighted dice (roll = {roll}) and selected '{token}' with probability {prob}%",
                    stochasticNote: "Sampling is stochastic — each click may produce a different result even for the same context character.",
                },
                labModeGuide: "This is the full transition matrix trained on Paul Graham essays. Each row is a character; each column is the character that follows. Brighter cells = more frequent transitions. Click any cell to see real training examples from the corpus.",
                limitationGuide: "Notice the fundamental constraint: the model only looks at the last character. It cannot learn that 'th' is almost always followed by 'e', because by the time it sees 'h', the 't' is already forgotten. This single-token memory is exactly what N-gram and neural models overcome.",
                storySteps: {
                    problem: {
                        title: "The Problem",
                        body: "Language is sequential — every character depends on what came before. The challenge is to capture this structure computationally. How do we build a model that can predict what comes next in a stream of text?",
                    },
                    representation: {
                        title: "Representing Text",
                        body: "Before we can model language, we need to decide how to represent it. The choice of representation determines the vocabulary size, the model's capacity, and its limitations.",
                    },
                    solution: {
                        title: "The Bigram Solution",
                        body: "The simplest approach: count how often each character follows every other character in a large training corpus. These counts, once normalized into probabilities, form a complete statistical model of character-level language.",
                    },
                    matrix: {
                        title: "The Transition Matrix",
                        body: "Every count is stored in a V × V matrix (V = vocabulary size). Each row represents a current character; each column represents the next. The brightness of a cell encodes the transition probability learned from real text.",
                    },
                    probabilities: {
                        title: "From Counts to Probabilities",
                        body: "Raw counts are normalized row-by-row so each row sums to 1.0, forming a valid probability distribution. The model can then make concrete predictions: \"After 'h', there is a 34% chance the next character is 'e'.\"",
                    },
                    limitation: {
                        title: "The Fundamental Limitation",
                        body: "The bigram model has zero memory beyond the immediately preceding character. It cannot learn that 'th' is almost always followed by 'e', because by the time it sees 'h', the 't' is already forgotten. This single-token horizon is what motivates N-gram and neural models.",
                    },
                },
                representation: {
                    charTitle: "Character-level tokens",
                    charBody: "Small, fixed vocabulary (~96 printable ASCII characters). Every possible input is representable. Simple to implement and visualize — ideal for understanding the fundamentals.",
                    wordTitle: "Word-level tokens",
                    wordBody: "Richer semantic units, but vocabulary can reach 50,000–500,000 entries. Rare words cause sparsity; unseen words at inference time cause failures. Much harder to scale.",
                },
                builderLabel: "Step-by-step bigram builder",
            },
            inference: {
                title: "Inference Console",
                probDist: "1. Probability Distribution",
                probDistDesc: "Type a phrase to see the top-k most likely next characters.",
                tooltip: {
                    title: "What is Inference?",
                    process: "The Process:",
                    processDesc: "The model takes your text, looks at the",
                    processHighlight: "last character",
                    processEnd: ", and looks up the probabilities for what comes next in its brain (the Matrix).",
                    topK: "Top-K:",
                    topKDesc: "We only show the top winners. If K=5, you see the 5 most likely candidates.",
                    note: "Note: This model is \"deterministic\" in its probabilities but \"stochastic\" (random) when it actually picks a character to generate text.",
                },
                lastChar: "Last char:",
                form: {
                    input: "Input Text",
                    placeholder: "Type text to analyze...",
                    topK: "Top-K Predictions",
                    analyze: "Analyze",
                    analyzing: "Analyzing...",
                },
            },
            stepwise: {
                title: "Stepwise Prediction",
                mainTitle: "2. Stepwise Prediction",
                description: "Watch the model predict a sequence character-by-character.",
                form: {
                    input: "Input Text",
                    placeholder: "Starting text...",
                    steps: "Prediction Steps",
                    predict: "Predict Steps",
                    predicting: "Predicting...",
                },
                table: {
                    step: "Step",
                    char: "Char",
                    prob: "Probability",
                },
                result: "Result:",
            },
            generation: {
                title: "Generation Playground",
                mainTitle: "3. Text Generation",
                description: "Let the model hallucinate text by sampling from the distribution.",
                tooltip: {
                    title: "How is text generated?",
                    sampling: "Sampling:",
                    samplingDesc: "The model doesn't just pick the #1 answer. It \"rolls a dice\" weighted by probabilities. This is why it can generate different text every time.",
                    temp: "Temperature:",
                    tempDesc: "Higher values make the dice roll \"wilder\" (more random). Lower values make it \"safer\" and more repetitive.",
                    note: "Try temperature 2.0 to see complete gibberish, or 0.1 to see it get stuck in loops!",
                },
                form: {
                    startChar: "Start Character",
                    numTokens: "Number of Tokens",
                    temp: "Temperature",
                    generate: "Generate",
                    generating: "Generating...",
                },
                copyToClipboard: "Copy generated text",
            },
            architecture: {
                title: "Technical Specification",
                subtitle: "Detailed breakdown of the model's internal mechanism, capabilities, and constraints.",
                mechanism: "Inference Mechanism",
                capabilities: "Capabilities",
                constraints: "Constraints",
                modelCard: {
                    title: "Model Card",
                    type: "Architecture Type",
                    complexity: "Complexity Rating",
                    useCases: "Primary Use Cases",
                    description: "Description",
                },
                tooltips: {
                    matrixW: {
                        title: "What is Matrix W?",
                        desc: "It's essentially a lookup table of 9216 numbers (96x96 characters in the vocab). Each number represents the \"unnormalized score\" of how likely one character follows another.",
                    },
                    softmax: {
                        title: "What is Softmax?",
                        desc: "Softmax takes raw scores (logits) and squashes them into a probability distribution. All numbers become positive and add up to 1 (100%).",
                    },
                    loss: {
                        title: "What is Loss (Cross-Entropy)?",
                        desc: "Loss measures the distance between the model's prediction and the truth. If the truth is 'n' and the model gave 'n' a 0.1% chance, the loss will be very high. Training is the process of tuning weights to minimize this distance.",
                    },
                },
                stepsList: {
                    matrixW: "Look up the row of the weight matrix W corresponding to the current character's index. This row contains the raw unnormalized scores (logits) for every possible next character.",
                    softmax: "Apply softmax to the logit row to produce a valid probability distribution over the vocabulary. Every value becomes positive and the row sums to exactly 1.0.",
                    loss: "During training, compute cross-entropy loss between the predicted distribution and the true next character. Backpropagate gradients to update W and minimize future prediction error.",
                },
                analysis: {
                    strengths: [
                        "Exact closed-form solution — no gradient descent required. Counts are sufficient statistics.",
                        "Instant training on any corpus size. O(N) in the number of training tokens.",
                        "Fully interpretable: every cell in W is a directly readable probability.",
                    ],
                    limitations: [
                        "Zero context beyond the immediately preceding token — cannot model multi-character patterns.",
                        "No generalization: each character pair is treated independently with no notion of similarity.",
                        "Vocabulary scales as O(V²) — impractical for word-level models with large vocabularies.",
                    ],
                },
                steps: {
                    predicts: "Predicts next character via:",
                    optimizes: "Optimizes parameters using:",
                },
            },
            guide: {
                badge: "Guide for Non-Technical Explorers",
                title: "How does this \"Brain\" work?",
                subtitle: "Explaining the Bigram model so even my mom can understand it (with lots of love).",
                switchHint: "Switch to Educational Mode to see the conceptual guide",
                cards: {
                    memory: {
                        title: "Goldfish Memory",
                        desc: "A **Bigram** model has the shortest memory in the world: it only remembers the **last letter** it wrote. To decide which letter comes next, it can only look at the previous one. It has no context of entire words or phrases.",
                    },
                    darts: {
                        title: "Darts Throw",
                        desc: "The model doesn't \"read\". It just has a giant table that says: \"If the last letter was 'a', there's a 10% probability that the next is 'n'\". Throwing the dart (sampling) is what generates text in a random but coherent way.",
                    },
                    heatmap: {
                        title: "The Heatmap",
                        desc: "The colored grid (Matrix) is the **heart** of the model. The bright squares are the most frequent \"routes\" the model found in the books it read during its training.",
                    },
                },
            },
            historicalContext: {
                description: "The bigram model is the simplest instance of a Markov chain applied to language. First studied by Claude Shannon in his 1948 paper 'A Mathematical Theory of Communication', character-level bigrams demonstrated that even zero-context statistical models capture meaningful structure in natural language.",
                limitations: [
                    "Zero memory beyond the immediate predecessor — cannot learn multi-character patterns like 'th' → 'e'.",
                    "No generalization — each character pair is treated independently with no notion of similarity.",
                ],
                evolution: "The limitations of bigram models directly motivated N-gram extensions (longer context) and eventually neural approaches (learned representations). Every modern language model can trace its lineage back to this simple transition matrix.",
            },
            educationalOverlay: {
                visualGuideTitle: "Visualization Guide",
                visualGuideDescription: "Each cell in this matrix represents P(next | current) - the probability that one character follows another. Brighter cells indicate more frequent character pairings found in the training corpus.",
                probabilityAnalysisTitle: "Probability Analysis",
                probabilityAnalysisDescription: "Type any text to see which characters the model predicts will come next, ranked by learned probability. The model only looks at the very last character - it has no memory of earlier context.",
                generationLabTitle: "Generation Lab",
                generationLabDescription: "Text generation works by repeatedly sampling from the probability distribution. The temperature parameter controls how random each sample is - lower values produce more predictable output, higher values produce creative (or nonsensical) sequences.",
            },
        },
        ngram: {
            title: "N-Gram Language Model",
            description: "A character-level statistical language model with variable context size. Visualize how increasing the context window sharpens predictions at the cost of exponential sparsity.",
            sections: {
                context: {
                    title: "Context Size",
                    description: "Adjust the context size (N) to condition predictions on more history.",
                },
                slice: {
                    title: "Active Slice",
                    descriptionN1: "For N=1 (Bigram), we visualize the simple Markov transition matrix P(next | current).",
                    descriptionNPlus: "For N>1, we visualize the conditional slice P(next | context). Click cells to trace examples.",
                },
                inference: {
                    title: "Inference & Generation",
                    description: "Interact with the model in real-time. Observe how it selects the next token based on the learned probabilities.",
                    distribution: {
                        title: "Probability Distribution",
                        desc: "Type a phrase to see the top-k most likely next characters.",
                    },
                    stepwise: {
                        title: "Stepwise Prediction",
                        desc: "Watch the model predict a sequence character-by-character.",
                    },
                    generation: {
                        title: "Text Generation",
                        desc: "Let the model hallucinate text by sampling from the distribution.",
                    },
                },
            },
            hero: {
                stats: {
                    uniqueContexts: { label: "Unique Contexts", desc: "Observed n-grams" },
                    vocab: { label: "Vocabulary", desc: "Unique characters" },
                    contextSpace: { label: "Context Space", desc: "|V|^{n}" },
                    tokens: { label: "Training Tokens", desc: "Total tokens seen" },
                },
            },
            viz: {
                hint: {
                    label: "Try it:",
                    text: "Click any colored cell in the matrix to see <strong class='text-white font-semibold'>real training examples</strong>.",
                },
            },
            controls: {
                contextSize: "Context Size (N)",
                contextDesc: "Number of previous characters to condition on",
                unigram: "Unigram",
                bigram: "Bigram",
                trigram: "Trigram",
                fourgram: "4-gram",
                fivegram: "5-gram",
                explosion: "Explosion (5+)",
            },
            lab: {
                badge: "Free Lab Mode · Full instrument access",
                contextLevels: {
                    1: "No context — each character is predicted independently from the corpus frequency. Fastest but least accurate.",
                    2: "Conditions on 1 previous character. Simple Markov chain; low sparsity, moderate precision.",
                    3: "Conditions on 2 previous characters. Better predictions but context space grows to |V|².",
                    4: "Conditions on 3 characters. High precision on seen sequences; significant sparsity on unseen ones.",
                    5: "Maximum context. Very sharp predictions where data exists, but most contexts are unseen — combinatorial explosion imminent.",
                },
                flow: {
                    afterContext: "The matrix below shows the probability distribution learned from training data for the current N level.",
                    afterMatrix: "Use the inference console to query the model with your own text and observe how context size affects predictions.",
                    afterComparison: "The training quality chart below reflects how well the model fits the corpus at the selected N level.",
                },
                performanceSummary: {
                    title: "Performance Summary",
                    description: "Runtime and training metrics for the current model",
                    inferenceTime: "Inference Time",
                    device: "Device",
                    corpusSize: "Corpus Size",
                    trainingDuration: "Training Duration",
                    totalTokens: "Total Tokens",
                    perplexity: "Perplexity",
                    finalLoss: "Final NLL",
                    ms: "ms",
                    tokens: "tokens",
                },
                comparison: {
                    title: "Model Comparison",
                    description: "Metrics across N=1..5",
                    ppl: "PPL",
                    util: "Util",
                    space: "Space",
                    tooltipPpl: "Perplexity — lower means more confident predictions",
                    tooltipUtil: "Fraction of possible contexts seen during training",
                    tooltipSpace: "Total possible context combinations (|V|^N)",
                },
                sparsity: {
                    title: "Data Sparsity",
                    description: "How much of the context space is actually observed",
                    observedContexts: "Observed Contexts",
                    possibleSuffix: "possible",
                    avgTransitions: "Avg. Transitions / Context",
                    nextTokens: "next-tokens per observed context",
                    utilLabel: "Context utilization",
                    utilHint: "Fraction of possible contexts seen in training data",
                    sparsityLabel: "Table sparsity",
                    sparsityHint: "Fraction of (context, next-token) pairs never observed",
                },
                warning5: {
                    title: "Combinatorial threshold exceeded",
                    hint: "Reduce N to 1–4 for live inference, stepwise prediction, and generation. Lower N also reduces sparsity.",
                },
                sections: {
                    transitions: "Transition Probabilities",
                    transitionsDescN1: "Full unigram/bigram matrix P(next | current)",
                    transitionsDescNPlus: "Slice P(next | context)",
                    conditionedOn: "Conditioned on:",
                    sparsity: "Data Sparsity",
                    trainingQuality: "Training Quality",
                    trainingQualityDesc: "Loss curve for the N={n} model during training",
                    nextToken: "Next-Token Prediction",
                    nextTokenDesc: "Type text and see the probability distribution over next characters",
                    stepwise: "Stepwise Prediction",
                    stepwiseDesc: "Trace the context window sliding character by character",
                    generation: "Text Generation",
                    generationDesc: "Generate text auto-regressively using the current N-gram model",
                },
                hero: {
                    title: "N-Gram Language Model",
                    description: "A character-level statistical language model with variable context size. Visualize how increasing the context window sharpens predictions at the cost of exponential sparsity.",
                    uniqueContexts: "Unique Contexts",
                    vocabulary: "Vocabulary",
                    contextSpace: "Context Space",
                    trainingTokens: "Training Tokens",
                    uniqueChars: "Unique characters",
                    totalTokensSeen: "Total tokens seen",
                },
                lossChart: {
                    title: "Training loss (NLL)",
                    final: "Final:",
                    ppl: "PPL:",
                    start: "Start",
                    progress: "Training progress",
                    end: "End",
                    perplexity: "Perplexity",
                    perplexityHint: "Lower = more confident predictions",
                    finalNll: "Final NLL",
                    finalNllHint: "Negative log-likelihood on train data",
                },
                footer: "LM-Lab · Scientific Instrument v1.0",
            },
            training: {
                title: "Training Insights",
                stats: {
                    totalTokens: "Total Tokens",
                    uniqueContexts: "Unique Contexts",
                    utilization: "Utilization",
                    sparsity: "Sparsity",
                    transitionDensity: "Transition Matrix Density",
                    subs: {
                        possiblePrefix: "of",
                        possibleSuffix: "possible",
                        fractionObserved: "fraction of possible contexts observed",
                        unseen: "of contexts never seen",
                    },
                },
            },
            historical: {
                title: "Historical Significance & Context",
                learnMore: "Learn More",
                description: "Description",
                limitations: "Key Limitations",
                evolution: "Evolution to Modern AI",
            },
            explosion: {
                title: "Context Too Large — Combinatorial Explosion",
                description: "As valid N increases, the number of possible contexts grows exponentially (|V|^N). For this vocabulary size, calculating the full transition matrix becomes computationally impractical and requires an enormous dataset to avoid sparsity.",
                complexity: "|V|^N = Space Complexity",
                limit: "Classical Limit Reached",
            },
            diagnostics: {
                vocabSize: "Vocabulary",
                contextSize: "Context Size (N)",
                contextSpace: "Context Space (|V|^N)",
                sparsity: "Sparsity",
                sub: {
                    observed: "{count} observed",
                    possible: "Possible Contexts",
                    utilized: "{percent}% utilized",
                },
            },
            educationalOverlay: {
                contextControlTitle: "Context Size Control",
                contextControlDescription: "Increasing N lets the model condition on more history - but the number of possible contexts grows as |V|^N. This exponential blowup is the central tension of n-gram models: more context means sharper predictions but also more data sparsity.",
                sliceVisualizationTitle: "Matrix Slice View",
                sliceVisualizationDescription: "For N > 1 the full transition tensor is too large to display. Instead, we fix the current context and show the resulting probability row - a slice through the high-dimensional table.",
                probabilityDistributionTitle: "Probability Distribution",
                probabilityDistributionDescription: "The model looks at the last N characters of your input, finds the matching context in its lookup table, and returns the probability distribution over all possible next characters.",
                generationPredictionTitle: "Generation & Prediction",
                generationPredictionDescription: "In educational mode we focus on understanding how a single next token is chosen. Switch to Free Lab to unlock the full stepwise tracer and text generation playground.",
                simplifiedSimulation: "Full stepwise prediction and generation available in Free Lab mode.",
            },
        },
        mlp: {
            title: "MLP + Embeddings",
            description: "Explore 108 trained MLP configurations. Watch embeddings emerge from noise, compare training dynamics across architectures, and generate text from learned character-level representations.",
            hero: {
                badge: "Research Lab",
            },
            freeLab: {
                title: "MLP Configuration Lab",
                description: "Select any trained configuration from the Model Zoo, inspect training curves, explore the embedding space, and compare models side-by-side.",
            },
        },
        neuralNetworks: {
            title: "Neural Networks & Deep Learning",
            description: "A first-principles exploration of artificial neural networks — from the perceptron to backpropagation. Understand how learned parameters replace counting and why dense representations generalize where N-grams fail.",
            hero: {
                badge: "Neural Computation",
            },
            freeLab: {
                title: "Neural Network Playground",
                description: "Experiment freely with perceptrons, activation functions, weight updates, and training dynamics.",
            },
            sections: {
                historicalOrigins: { number: "01", label: "History" },
                countingToLearning: { number: "02", label: "Learning" },
                perceptron: { number: "03", label: "Perceptron" },
                weightsAndBias: { number: "04", label: "Parameters" },
                activationFunctions: { number: "05", label: "Activations" },
                backpropagation: { number: "06", label: "Backprop" },
                parameterUpdates: { number: "07", label: "Updates" },
                bigramConnection: { number: "08", label: "Connection" },
                limitations: { number: "09", label: "Reflection" },
                playground: {
                    inputs: {
                        title: "Inputs",
                        desc: "Feature values fed into the perceptron. Each input is multiplied by its corresponding weight before being summed.",
                        x1: "First input feature value (x₁). Multiplied by weight w₁ before entering the sum node.",
                        x2: "Second input feature value (x₂). Multiplied by weight w₂ before entering the sum node.",
                    },
                    weights: {
                        title: "Parameters",
                        desc: "Learnable parameters that scale each input. The bias shifts the activation threshold independently of the inputs.",
                        w1: "Weight for input x₁. Controls how strongly x₁ influences the output. Updated by gradient descent during training.",
                        w2: "Weight for input x₂. Controls how strongly x₂ influences the output. Updated by gradient descent during training.",
                        bias: "Bias term (b). Shifts the weighted sum, allowing the neuron to activate even when all inputs are zero.",
                    },
                    activation: {
                        title: "Activation Function",
                        desc: "Non-linear transformation applied after the weighted sum. Without it, stacking layers would collapse into a single linear function.",
                        linear: "No transformation — output equals the weighted sum z. Useful as a baseline but cannot model non-linear patterns.",
                        relu: "Rectified Linear Unit. Outputs max(0, z). Sparse, efficient, and widely used in deep networks.",
                        sigmoid: "Squashes output to (0, 1). Useful for binary probability outputs, but can cause vanishing gradients.",
                        tanh: "Squashes output to (−1, 1). Zero-centered, often preferred over sigmoid for hidden layers.",
                    },
                    training: {
                        title: "Training",
                        desc: "Adjust the target and learning rate, then step through gradient descent to minimize the loss.",
                        target: "The desired output value (y). The model tries to minimize the squared difference between its prediction and this target.",
                        learningRate: "Learning rate (η). Controls the step size during gradient descent. Too high causes instability; too low slows convergence.",
                        step: "Run one gradient descent step: compute gradients and update w₁, w₂, and b by −η × gradient.",
                        auto: "Run 10 gradient descent steps in sequence to observe how parameters and loss evolve over multiple iterations.",
                        reset: "Reset all parameters and training history to their initial values.",
                        random: "Randomize weights and bias to explore a different region of the loss landscape.",
                        steps: "Total number of gradient descent steps taken so far in this training session.",
                        stepIndex: "Step number in the training history log.",
                        noData: "No training data yet",
                        noDataHint: "Click \"Train 1 Step\" or \"Auto-Train ×10\" to begin",
                        insightsTitle: "Training Insights",
                        runInference: "Run inference to view training data",
                        stats: {
                            finalLoss: { label: "Final Loss", desc: "The error level at the end of training. Lower is better." },
                            steps: { label: "Steps", desc: "How many times the model updated its parameters during training." },
                            batchSize: { label: "Batch Size", desc: "Number of examples processed per gradient update step." },
                            learningRate: { label: "Learning Rate", desc: "Step size for gradient descent. Too high causes instability; too low slows convergence." },
                            parameters: { label: "Parameters", desc: "Total number of learnable weights in the model." },
                        },
                    },
                    visualization: {
                        sum: "Weighted sum node (Σ). Computes z = w₁x₁ + w₂x₂ + b before the activation function is applied.",
                        output: "Final prediction ŷ = activation(z). This is the value the network outputs after applying the non-linearity.",
                        loss: "Mean squared error loss: L = (ŷ − target)². Measures how far the prediction is from the desired target.",
                        activationNode: "Activation function node. Applies the selected non-linearity to the weighted sum z.",
                        activationCurve: "The activation function curve. The dot shows the current input z and its corresponding output f(z).",
                        equation: "Full forward-pass equation: multiply each input by its weight, add the bias, then apply the activation function.",
                        lossCurve: "Loss over training steps. A descending curve indicates the model is learning — parameters are converging toward the target.",
                        lossCurveLabel: "Loss over training steps",
                        lossTooltipTitle: "What is Loss?",
                        lossTooltipErrorLabel: "Prediction Error",
                        lossTooltipError: "Loss measures how \"surprised\" the model is. A high loss means it's guessing wrong frequently.",
                        lossTooltipBenchmarkLabel: "The Benchmark",
                        lossTooltipBenchmark: "Pure random guessing gives a loss of ~4.56 (−ln(1/96)). Anything lower means the model has actually learned something.",
                        lossTooltipCaption: "The descending curve shows the model slowly discovering patterns in your text.",
                    },
                    tabs: {
                        perceptron: "Visualize the single-neuron forward pass: inputs are scaled by weights, summed with a bias, then passed through an activation.",
                        activation: "Explore how the chosen activation function transforms the weighted sum z into the final prediction ŷ.",
                        gradients: "Inspect the chain-rule gradient flow and see exactly how each parameter will be updated in the next training step.",
                        training: "Track loss and parameter evolution across training steps to observe gradient descent in action.",
                    },
                    gradients: {
                        forwardPass: "Forward pass: compute z, apply activation, and calculate the loss from the current prediction and target.",
                        forwardPassLabel: "Forward Pass",
                        chainRule: "Backpropagation via the chain rule: decompose ∂L/∂w into a product of local gradients through each node.",
                        chainRuleLabel: "Gradients (Chain Rule)",
                        weightUpdate: "Proposed parameter update: new value = old value − η × gradient. Applied when you click Train 1 Step.",
                        weightUpdateLabel: "Proposed Weight Update",
                        linearSum: "Linear pre-activation: z = w₁x₁ + w₂x₂ + b. The raw weighted sum before the activation function.",
                        linearSumLabel: "Linear sum",
                        prediction: "Prediction ŷ = activation(z). The output of the neuron after applying the non-linear activation function.",
                        predictionLabel: "Prediction",
                        loss: "Loss L = (ŷ − target)². Squared error between the prediction and the desired target value.",
                        lossLabel: "Loss",
                    },
                    buttons: {
                        trainStep: "Train 1 Step",
                        autoTrain: "Auto-Train ×10",
                        reset: "Reset",
                        random: "Random",
                    },
                    tabLabels: {
                        perceptron: "Perceptron",
                        activation: "Activation",
                        gradients: "Gradients",
                        training: "Training",
                    },
                    diagram: {
                        caption: "Adjust inputs, weights, and bias to see how the perceptron transforms them into an output.",
                        inputX1: "Input x₁",
                        inputX2: "Input x₂",
                        weightW1: "Weight w₁",
                        weightW2: "Weight w₂",
                        biasB: "Bias b",
                    },
                },
            },
        },
    },
    bigramNarrative: {
        hero: {
            eyebrow: "Understanding Language Models",
            titlePrefix: "The Bigram",
            titleSuffix: "Model",
            description: "A first-principles exploration of the simplest statistical language model — and why it still matters."
        },
        problem: {
            title: "The Problem of Prediction",
            lead: "Language is fundamentally sequential. Every word you read right now is informed by the words that came before it.",
            p1: "This property — that each token in a sequence carries ",
            p1Highlight: "expectations about what follows",
            p2: " — is what makes language both expressive and predictable. It's also what makes it so hard to model computationally.",
            p3: "The central challenge of language modeling is deceptively simple to state:",
            quote: "Given what we have already seen, what should come next?",
            p4: "This question has driven decades of research in ",
            h1: "computational linguistics",
            h2: "information theory",
            h3: "deep learning",
            p5: ". To build a model that can answer it, we need a way to capture the statistical structure of language. Let's start with the simplest possible approach.",
            label: "Foundation"
        },
        coreIdea: {
            label: "Core Idea",
            title: "The Simplest Statistical Idea",
            lead: "What if, instead of trying to understand meaning, we simply observed patterns?",
            p1: "Specifically: ",
            h1: "how often does one character follow another?",
            p2: " This is the core insight behind the Bigram model. It ignores grammar, semantics, and long-range dependencies entirely. It asks only one question: given the current token, what is the probability distribution over the next token?",
            caption: "The Bigram assumption: the next token depends only on the current one.",
            p3: "We model P(x_{t+1} | x_t) — the chance of seeing a particular next token given only the token we just observed. Nothing more, nothing less. This radical simplification is what makes the model both tractable and limited.",
            calloutTitle: "Key Insight",
            calloutP1: "The \"bi\" in Bigram means ",
            calloutH1: "two",
            calloutP2: ". The model considers pairs of tokens — the current one and the next one. It has zero memory of anything before the current token."
        },
        mechanics: {
            label: "Mechanics",
            title: "Building a Transition Table",
            lead: "To learn these probabilities, the model scans through a training corpus and counts every pair of consecutive tokens.",
            p1: "For each token A, it records how often each possible token B appears immediately after it. These counts form a ",
            h1: "matrix",
            p2: " — a two-dimensional table where rows represent the current token and columns represent the next token. Each cell holds the number of times that specific transition was observed in the training data.",
            p3: "The visualization below is a live rendering of this transition matrix. Brighter cells indicate more frequent pairings — patterns the model has learned from real text.",
            calloutTitle: "Reading the Matrix",
            calloutP1: "Each row represents a \"given\" character. Each column represents a \"next\" character. The brightness of a cell encodes how likely that transition is. Notice how some rows are nearly uniform (the model is unsure) while others have sharp peaks (strong preferences)."
        },
        normalization: {
            label: "Normalization",
            title: "From Counts to Probabilities",
            lead: "Raw counts alone don't tell us much. To make predictions, we need to convert them into probabilities.",
            p1: "We do this by ",
            h1: "normalizing each row",
            p2: " of the count matrix — dividing every count by the total number of transitions from that row's token. After normalization, each row sums to 1.0, forming a valid probability distribution.",
            p3: "The model can now make concrete statements: \"After the letter h, there is a 32% chance the next character is e, a 15% chance it's a, and so on.\"",
            p4: "Try it yourself below. Type any text to see what the model predicts will come next — based ",
            h2: "solely on the very last character",
            p5: " of your input."
        },
        sampling: {
            label: "Sampling",
            title: "Generating New Text",
            lead: "Once we have a probability distribution, we can do something remarkable: generate entirely new text.",
            p1: "The process is called ",
            h1: "autoregressive sampling",
            p2: ". Start with a seed character, sample the next one from its probability distribution, then use that new character as the seed for the next step. Repeat indefinitely.",
            calloutTitle: "Temperature",
            calloutP1: "The ",
            calloutH1: "temperature",
            calloutP2: " parameter controls how \"creative\" the generation is. At ",
            calloutH2: "low temperatures",
            calloutP3: ", the model almost always picks the most likely next token. At ",
            calloutH3: "high temperatures",
            calloutP4: ", it samples more uniformly — producing surprising and often nonsensical output.",
            p3: "Generate some text below and observe how a model with ",
            h2: "only one character of memory",
            p4: " produces output that is statistically plausible at the character level, yet meaningless at any higher level."
        },
        reflection: {
            label: "Reflection",
            title: "Power and Limitations",
            lead: "The Bigram model is powerful precisely because of its simplicity.",
            p1: "It requires very few parameters — just a V × V matrix, where V is the vocabulary size. It trains instantly. And it provides a clear ",
            h1: "probabilistic baseline",
            p2: " for language generation that every more sophisticated model must beat.",
            calloutTitle: "The Fundamental Limitation",
            calloutP1: "The model has ",
            calloutH1: "no memory beyond a single token",
            calloutP2: ". It cannot learn that \"th\" is often followed by \"e\", because by the time it sees \"h\", it has already forgotten the \"t\". It captures local co-occurrence but nothing about words, phrases, or meaning.",
            p3: "This limitation is exactly what motivates the progression to more sophisticated architectures: ",
            h2: "N-grams",
            p4: " extend the context window, ",
            h3: "MLPs",
            p5: " learn dense representations, and ",
            h4: "Transformers",
            p6: " attend to the entire sequence at once.",
            quote: "Each model in this lab builds on the same core question: given context, what comes next?"
        },
        tokens: {
            title: "Representing text",
            lead: "We split text into tokens.",
            charTitle: "Characters:",
            charDesc: "small vocab, easy to see.",
            wordTitle: "Words:",
            wordDesc: "richer, huge vocab.",
            note: "We use characters here.",
            charLevelTitle: "Character-level tokens",
            charLevelBody: "Small, fixed vocabulary of ~96 printable ASCII symbols. Every possible input is representable. Simple to implement, easy to visualize — ideal for understanding the fundamentals of language modeling.",
            wordLevelTitle: "Word-level tokens",
            wordLevelBody: "Richer semantic units that carry more meaning per token. But vocabulary can reach 50,000–500,000 entries, making the transition matrix enormous. Rare words cause sparsity; words unseen during training cause complete failures at inference time.",
            charLimitations: "Character-level models have a small, manageable vocabulary — but they must learn everything from scratch. There are no pre-built notions of words, morphology, or meaning. The model must discover that 't', 'h', 'e' together form a common word purely from co-occurrence statistics.",
            wordLimitations: "Word-level models are more expressive but face a fundamental scalability problem. English has over 170,000 words in common use. A bigram model at the word level would need a 170,000 × 170,000 transition matrix — nearly 29 billion cells — most of which would be empty (never observed in training). This sparsity problem is one of the core motivations for neural language models.",
            whyCharHere: "For this lab, we use character-level tokens. The vocabulary stays small enough to visualize the entire transition matrix at once, making the model's learned knowledge directly inspectable. Every design decision you see here scales directly to word-level and subword-level models — only the vocabulary size changes."
        },
        counting: {
            title: "The Bigram idea",
            lead: "Count pairs: current -> next. More counts = more likely.",
            builderTitle: "Step-by-step builder",
            builderDesc: "Walk through text; each pair adds +1 to a cell.",
            p1: "The core operation is almost embarrassingly simple: scan through the training text one character at a time, and for every consecutive pair (current character → next character), increment a counter. That's it. After scanning millions of characters, these counts encode the statistical structure of the language — which characters tend to follow which others, and how strongly.",
            p2: "The step-by-step builder below makes this concrete. Watch how each character pair in the input text adds exactly one count to the corresponding cell in the matrix. By the end, the matrix is a complete record of every transition observed in the training data.",
            calloutTitle: "Why counting works",
            calloutText: "The Law of Large Numbers guarantees that as training data grows, the observed frequencies converge to the true underlying probabilities of the language. With enough text, the bigram counts become a reliable statistical portrait of character-level patterns."
        },
        matrix: {
            title: "The transition table",
            lead: "Rows = current token, columns = next.",
            desc: "Build below, then see the full matrix."
        },
        probabilities: {
            title: "Counts to probabilities",
            lead: "Normalize each row to 100%.",
            desc: "Model reads last token's row and samples the next.",
            overlayTitle: "Counts -> Probabilities -> Sampling",
            overlayDesc: "Pick token, normalize row, sample next.",
            step1: "1) Row values",
            step2: "2) Normalize",
            step3: "3) Sample next token",
            currentToken: "Current token",
            typeChar: "Type a character",
            normalizeSimple: "Simple normalize",
            softmax: "Softmax",
            sampleNext: "Sample next token",
            mostLikely: "Most likely:",
            remaining: "Remaining:",
            stochastic: "Sampling is random."
        },
        limitations: {
            title: "Limitations",
            lead: "Bigram has no memory—only the last token.",
            desc: "No long context. Hence N-grams and neural nets."
        },
        footer: {
            text: "Continue exploring the other models in the lab to see how each one addresses the limitations of its predecessor.",
            brand: "LM-Lab · Educational Mode"
        }
    },
    bigramBuilder: {
        description: "We build the bigram matrix by scanning the text character by character. For each pair of consecutive characters (current → next), we increment the cell [current, next]. This table captures how often each character is followed by another.",
        placeholder: "Type text here...",
        hint: "Enter some text to see how the bigram matrix is constructed.",
        buttons: {
            build: "Build Bigram Matrix",
            next: "Next Step",
            autoPlay: "Auto Play",
            pause: "Pause",
            instant: "Instant Complete",
            reset: "Reset Steps"
        },
        vocab: "Educational vocabulary",
        normalized: "Normalized text:",
        empty: "(empty after filtering)",
        skipped: "Showing the first {max} unique characters for clarity ({count} unique character(s) omitted).",
        step1: "Step",
        step2: "updates cell [",
        step3: "].",
        pressBuild: "Press Build Bigram Matrix and start stepping through character pairs.",
        table: {
            curnxt: "cur \\ nxt"
        }
    },
    ngramNarrative: {
        hero: {
            eyebrow: "Understanding Language Models",
            titlePrefix: "The N-Gram",
            titleSuffix: "Model",
            description: "What happens when we give a language model more than one character of memory? A journey through context windows, combinatorial explosions, and the limits of counting.",
        },
        moreContext: {
            label: "More Context",
            title: "Beyond a Single Character",
            lead: "The Bigram model's fatal flaw is its amnesia — it only remembers the last character. What if we let the model look back further?",
            p1: "An N-gram model conditions its predictions on the",
            p1Highlight: "previous N characters",
            p1End: " instead of just one. A trigram (N=2) looks at the last two characters; a 4-gram looks at three; and so on.",
            p2: "This simple extension has a profound impact on prediction quality. With more context, the model can distinguish between sequences that a bigram would treat identically. After \"th\", it knows \"e\" is far more likely than after just \"h\" alone.",
            p3: "But this improvement comes at a steep cost — one that defines the central tension of statistical language modeling.",
            calloutTitle: "The N-gram Assumption",
            calloutText: "An N-gram model assumes the next token depends only on the previous N tokens. This is the Markov assumption of order N. It trades global context for tractability.",
        },
        contextWindow: {
            label: "Context Window",
            title: "Seeing More of the Past",
            lead: "The context window determines how many previous characters the model can \"see\" when making a prediction.",
            caption: "Notice how larger context windows capture more meaningful patterns — but the number of possible contexts explodes.",
            hint: "Watch how the context grows character by character as N increases.",
            p1: "Each step up in N gives the model more information to work with. But it also multiplies the number of possible states the model must track — a theme we'll explore in detail.",
        },
        howItWorks: {
            label: "Mechanics",
            title: "Building an N-Gram Table",
            lead: "Just like the bigram model, N-grams learn by counting. But now we count tuples of N+1 characters instead of pairs.",
            p1: "For each position in the training text, the model extracts the",
            p1Highlight: " N-character context",
            p1End: " and records which character follows it. These counts form a massive lookup table: given a specific context, what is the probability distribution over the next character?",
            p2: "When N=1, this table is the familiar 2D bigram matrix. For N=2, it becomes a 3D tensor. For N=3, a 4D tensor. The dimensionality grows linearly with N, but the number of cells grows exponentially.",
        },
        statistical: {
            label: "Statistical Nature",
            title: "A Purely Statistical Model",
            lead: "N-gram models have no understanding of language. They are sophisticated counting machines.",
            p1: "Every prediction is a",
            p1Highlight: "table lookup",
            p1End: " — the model finds the matching context in its table and returns the stored probability distribution. There are no learned parameters, no gradients, no optimization.",
            p2: "This makes N-grams extremely fast at inference and trivially interpretable: you can always ask \"why did the model predict X?\" and trace the answer back to exact training examples.",
            calloutTitle: "No Generalization",
            calloutText: "If the model has never seen a particular context in training, it has zero information about what comes next. Unlike neural networks, N-grams cannot generalize from similar contexts — each context is treated as completely independent.",
        },
        complexity: {
            label: "Complexity",
            title: "The Combinatorial Explosion",
            lead: "Here lies the fundamental limitation of N-gram models: the number of possible contexts grows as |V|^N.",
            p1: "With a vocabulary of ~96 characters, a bigram has 96 possible contexts. A trigram has 9,216. A 4-gram has",
            p1Highlight: " 884,736",
            p1End: ". A 5-gram has over 84 million. Most of these contexts will never appear in any realistic training corpus.",
            p2: "This exponential growth means that as N increases, the model's table becomes overwhelmingly sparse. Most cells contain zero counts, making predictions unreliable or impossible for unseen contexts.",
            comparisonLabel: "N-Gram Comparison · Live backend metrics",
            comparisonHint: "Compare perplexity, context utilization, and state space across different values of N.",
            metricsLegend: {
                perplexity: "Perplexity — lower means more confident predictions. Measures how \"surprised\" the model is on average.",
                utilization: "Context utilization — what fraction of all possible N-character contexts were actually observed in the training data.",
                contextSpace: "Context space — the total number of theoretically possible contexts (|V|^N). Grows exponentially.",
            },
        },
        vocabulary: {
            label: "Vocabulary",
            title: "Characters vs. Words",
            lead: "We use character-level tokens in this lab, but real-world N-grams often operate on words — making the explosion even worse.",
            p1: "With a word vocabulary of 50,000 tokens, even a",
            p1Highlight: " bigram matrix needs 2.5 billion cells",
            p1End: ". A trigram table would require 125 trillion entries. This is why word-level N-grams beyond N=3 are essentially impractical without aggressive smoothing and pruning.",
            p2: "Character-level models keep the vocabulary small (~96), making it feasible to visualize and explore the full table. But the tradeoff is that individual characters carry almost no semantic meaning.",
        },
        noUnderstanding: {
            label: "Limitations",
            title: "No True Understanding",
            lead: "N-gram models capture local co-occurrence patterns but have no notion of meaning, grammar, or long-range coherence.",
            p1: "The model treats \"the cat sat on the\" and \"the dog sat on the\" as completely unrelated contexts (for N < full sentence length). It cannot recognize that both involve an animal sitting on something.",
            p2: "This inability to",
            p2Highlight: "generalize across similar contexts",
            p2End: " is what ultimately limits N-gram models. No matter how much data you collect, there will always be valid contexts the model has never seen.",
            p3: "This fundamental limitation is exactly what motivates the transition to neural approaches — models that learn dense, continuous representations capable of recognizing similarity between contexts.",
        },
        conclusion: {
            label: "Reflection",
            title: "The Bridge to Neural Models",
            lead: "N-gram models push statistical language modeling to its logical extreme — and reveal why a fundamentally different approach is needed.",
            p1: "We have seen that increasing context improves predictions but triggers an exponential explosion in the state space. This is not a bug — it is an inherent property of discrete, count-based models.",
            p2: "The core problem is representation: N-grams represent each context as an isolated point in a vast discrete space. There is no notion of similarity between contexts, no way to share statistical strength between related patterns.",
            p3: "Neural language models solve this by mapping discrete tokens into continuous vector spaces where similar contexts live near each other. This allows them to generalize from seen examples to unseen but similar contexts.",
            p4: "The progression from Bigram → N-gram → Neural Network is not just historical — it reflects a deepening understanding of what it means to model language computationally.",
            quote: "The curse of dimensionality is not a failure of N-grams — it is the reason neural representations were invented.",
        },
        cta: {
            title: "Continue Exploring",
            labButton: "Open Free Lab",
            labDesc: "Switch to Free Lab mode for full access to the N-gram inference console, stepwise predictor, and text generator with adjustable context size.",
            neuralButton: "Next: Neural Networks",
            neuralDesc: "Discover how neural networks overcome the limitations of N-grams by learning dense representations and continuous parameter spaces.",
        },
        footer: {
            text: "The N-gram model demonstrates both the power and the ceiling of statistical language modeling. Next, we explore how neural networks break through this ceiling.",
            brand: "LM-Lab · Educational Mode",
        },
    },
    neuralNetworkNarrative: {
        hero: {
            eyebrow: "Understanding Language Models",
            titlePrefix: "Neural Networks &",
            titleSuffix: "Deep Learning",
            description: "In the previous chapters, we explored bigram and N-gram models — language systems built on counting and memorizing patterns. They work, but they hit a wall. Now we ask a different question: what if a machine could learn the patterns on its own, the way a student learns to read? This is the story of neural networks — mathematical structures inspired by the brain that learn by adjusting millions of tiny dials until they get things right.",
            titlePrimary: "Neural Networks",
            titleSecondary: "Deep Learning",
        },
        historical: {
            title: "The Dream of Thinking Machines",
            lead: "Before there were GPTs and deep learning, there was a simple, audacious idea: what if we could build a mathematical model of a brain cell?",
            p1: "The year is 1943. Warren McCulloch, a neuroscientist, and Walter Pitts, a self-taught logician barely out of his teens, publish a paper that will change computing forever. They propose that a single brain cell — a neuron — can be modeled as a simple logical gate: it receives signals from other neurons, and if those signals are strong enough, it \"fires\" and sends its own signal forward. McCulloch and Pitts showed that networks of these simple units could, in theory, compute anything. This was the birth of the idea that",
            p1Highlight: "intelligence could emerge from simple mathematical operations",
            p1End: " wired together in the right way. The brain doesn't run on magic — it runs on signals, thresholds, and connections. If we could replicate those in a machine, perhaps we could replicate thought itself.",
            p2: "In 1958, Frank Rosenblatt built the Mark I Perceptron — the first machine that could genuinely learn from experience. It was a physical device at Cornell, wired with photocells and potentiometers, designed to recognize simple shapes. The New York Times ran a story claiming the Navy had built a machine that could \"walk, talk, see, write, reproduce itself and be conscious of its existence.\" The hype was enormous. Rosenblatt himself was more careful, but the excitement was real: for the first time, a machine was adjusting its own parameters to improve at a task, without being explicitly programmed for every case.",
            p3: "Then came the crash. In 1969, Marvin Minsky and Seymour Papert published \"Perceptrons,\" a rigorous mathematical analysis proving that single-layer perceptrons had",
            p3Highlight: "fundamental limitations they could never overcome",
            p3End: ". The most famous example was XOR — a simple logical function that no single perceptron could learn, no matter how long you trained it. The book's impact was devastating. Funding dried up. Researchers moved to other fields. The period from roughly 1969 to 1986 became known as the \"AI Winter\" — a long, cold era when neural network research was widely considered a dead end.",
            quote: "\"The question is not whether intelligent machines can have any feelings, but whether machines can be intelligent without any feelings.\" — Marvin Minsky, 1986",
            p4: "But the idea refused to die. Throughout the 1970s and early 1980s, a handful of researchers kept working in relative obscurity. In 1986, David Rumelhart, Geoffrey Hinton, and Ronald Williams published their landmark paper on backpropagation — the algorithm that finally showed how multi-layer networks could learn. The AI Winter began to thaw. It would take another 25 years, the rise of massive datasets, and the GPU revolution before deep learning would conquer the world. But the seed was planted in 1943, and it never stopped growing.",
        },
        countingToLearning: {
            title: "From Counting to Learning",
            lead: "Remember our bigram model? It learned by counting how often one character followed another. The N-gram model extended this by counting longer sequences. Both are fundamentally the same idea: observe, tally, look up. Neural networks represent a completely different philosophy.",
            p1: "Think about what a bigram model actually stores. For every character pair in the training text, it increments a counter in a giant table. When it needs to predict the next character, it looks up the row for the current character and reads off the stored probabilities. This is pure memorization — the model has",
            p1Highlight: "no understanding of why certain patterns exist",
            p1End: ", only that they were observed. If the model has never seen a particular combination in its training data, it has literally zero information about it. It cannot reason by analogy, cannot notice that \"th\" behaves similarly to \"sh\" in many contexts, cannot transfer knowledge from one pattern to another.",
            p2: "Neural networks take the opposite approach. Instead of memorizing exact counts, they learn a set of",
            p2Highlight: "continuous numerical parameters — weights and biases",
            p2End: " — that are gradually adjusted through training. These parameters don't correspond to any specific training example. Instead, they encode compressed, generalized knowledge about the patterns in the data. A neural network that has learned about \"th\" will automatically have some useful knowledge about \"sh\" too, because the parameters that capture one pattern overlap with those that capture the other.",
            insightTitle: "The Key Insight",
            insightText: "A bigram model stores one number per character pair (the count). A neural network stores a set of shared parameters that collectively encode knowledge about ALL character pairs simultaneously. This is why neural networks can generalize: they don't memorize individual facts — they learn the underlying rules that generate those facts.",
            p3: "This shift from discrete counting to continuous optimization is perhaps the most important conceptual leap in all of machine learning. It's the difference between a student who memorizes every answer in a textbook and one who understands the underlying principles well enough to solve new problems they've never seen before. Let's see exactly how this works, starting with the simplest possible neural network: a single artificial neuron.",
        },
        perceptron: {
            title: "The Perceptron",
            lead: "Every neural network, no matter how massive, is built from copies of one simple building block: the artificial neuron, or perceptron. Understanding this single unit is the key to understanding everything that follows.",
            p1: "Imagine you're deciding whether to go outside. You have several inputs: is it sunny? Is it warm? Do you have free time? Each of these factors matters, but not equally — maybe weather matters more to you than free time. A perceptron works exactly like this decision process. It receives numerical inputs, and each input is",
            p1Highlight: "multiplied by a weight that represents how important that input is",
            p1End: ". A large positive weight means \"this input strongly pushes toward yes.\" A large negative weight means \"this input strongly pushes toward no.\" A weight near zero means \"this input barely matters.\" The perceptron adds up all these weighted inputs, adds a bias term (think of it as your baseline mood — are you generally inclined to go outside or stay in?), and produces a single number.",
            p2: "But a raw sum isn't enough. We need a decision — yes or no, fire or don't fire, just like a real neuron. So the sum is passed through an activation function that squashes it into a useful range. In the original perceptron, this was a simple step function: if the sum is above zero, output 1 (\"fire\"); otherwise, output 0 (\"don't fire\"). Modern neural networks use smoother functions, which we'll explore shortly.",
            formulaCaption: "The perceptron formula: multiply each input by its weight, sum everything up, add a bias, and pass the result through an activation function f. This is the atomic operation of all neural networks.",
            p3: "Here's what makes this powerful: with the right weights and bias, a single perceptron can",
            p3Highlight: "draw a straight line through data and classify everything on one side as category A and everything on the other as category B",
            p3End: ". It can learn to distinguish between characters, classify spam from legitimate email, or detect simple patterns — all by finding the right numbers for its weights and bias through training.",
            p4: "But there's a catch, and it's an important one. A single perceptron can only learn patterns that are linearly separable — problems where you can draw a straight line (or flat plane, in higher dimensions) between the categories. Many real-world problems aren't this simple, as we'll see when we discuss the infamous XOR problem. For now, let's look more closely at the two types of numbers that make a perceptron work: weights and biases.",
        },
        weightsAndBias: {
            title: "Weights and Biases",
            lead: "If the perceptron is the engine, then weights and biases are the dials and knobs that control how it behaves. These numbers — collectively called \"parameters\" — are everything the network knows. When we say a neural network has \"learned\" something, we mean it has found good values for these parameters.",
            p1: "Think of weights as volume knobs on a mixing board. Each input to the neuron has its own knob. Turning the knob up (large positive weight) makes that input's voice loud and influential. Turning it down (near zero) effectively mutes it. Turning it negative makes it a contrarian — when that input says \"yes,\" the neuron hears \"no.\" The weight on each connection encodes the",
            p1Highlight: "relationship between that specific input and the neuron's output",
            p1End: ". A neuron that has learned to detect the letter \"e\" after \"th\" will have a large positive weight on the connection from the \"h\" input and a large positive weight from the \"t\" input, while inputs for irrelevant characters will have weights near zero.",
            p2: "The bias is subtler but equally important. It acts as the neuron's",
            p2Highlight: "baseline tendency — its willingness to activate even before seeing any input",
            p2End: ". A positive bias means the neuron is eager to fire; it needs less evidence from the inputs to activate. A negative bias means the neuron is reluctant; it needs strong input signals to overcome its default \"off\" state. Together, the weights determine which direction the decision boundary faces, and the bias determines where it sits. Without bias, every decision boundary would have to pass through the origin — a severe constraint that would cripple learning.",
            p3: "Here's the beautiful part: at the start of training, all these numbers are random. The network knows nothing. But through the training process, each weight and bias is nudged — thousands or millions of times — in the direction that reduces errors. Over time, the random noise crystallizes into structured knowledge. A trained network's parameters are a compressed encoding of everything it learned from the training data, stored as nothing more than a long list of decimal numbers.",
            calloutTitle: "Why \"Parameters\" Matter",
            calloutText: "GPT-3 has 175 billion parameters. GPT-4 is rumored to have over a trillion. Each parameter is just a single decimal number — a weight or a bias. But collectively, these numbers encode an astonishing amount of knowledge about language, reasoning, and the world. The art of deep learning is finding the right values for all of them.",
        },
        activations: {
            title: "Activation Functions",
            lead: "We've established that a perceptron computes a weighted sum and adds a bias. But here's a problem: weighted sums are linear operations. If you stack linear operations on top of each other, you just get another linear operation. Without something to break the linearity, a 100-layer network would be mathematically equivalent to a single layer. Activation functions are the crucial ingredient that gives neural networks their power.",
            p1: "Here's the core issue: if every neuron just outputs its weighted sum directly, then no matter how many layers you stack, the entire network collapses into a single linear transformation. Imagine trying to draw a curved line using only a ruler — you can't. Linear functions can only model straight-line relationships. But the real world is full of curves, thresholds, and non-linear patterns. The letter \"e\" is the most common character in English, but only in certain contexts — after \"th\" it's very likely, but after \"ee\" much less so. These context-dependent, non-linear relationships are exactly what activation functions allow the network to capture.",
            p2: "An activation function sits at the output of every neuron and transforms the weighted sum into something non-linear. Think of it as a filter that reshapes the signal. The neuron computes its sum, and then the activation function decides how to express that result. Different activation functions have different personalities, and the choice of which one to use has a profound impact on how well the network learns.",
            reluTitle: "ReLU (Rectified Linear Unit)",
            reluDesc: "The workhorse of modern deep learning. The rule is dead simple: if the input is positive, pass it through unchanged; if it's negative, output zero. That's it. This creates a sharp \"elbow\" at zero — neurons either fire (positive) or stay silent (zero). ReLU is fast to compute, doesn't suffer from certain training pathologies, and works remarkably well in practice. Most neural networks you encounter today use ReLU or one of its variants.",
            sigmoidTitle: "Sigmoid (Logistic Function)",
            sigmoidDesc: "The classic S-curve. It smoothly squashes any input into the range (0, 1), making it perfect for outputs that represent probabilities. A very negative input maps to nearly 0; a very positive input maps to nearly 1; and zero maps to exactly 0.5. The sigmoid was the dominant activation function for decades, but it has a weakness: for very large or very small inputs, the curve is almost flat, which makes gradients vanishingly small and training agonizingly slow — a problem called \"vanishing gradients.\"",
            tanhTitle: "Tanh (Hyperbolic Tangent)",
            tanhDesc: "Like sigmoid but centered around zero, mapping inputs to the range (-1, 1). This zero-centering helps with training stability because the outputs have both positive and negative values, which means the gradients during backpropagation are less biased in one direction. Tanh shares sigmoid's vanishing gradient problem at the extremes but generally trains faster for hidden layers.",
            p3: "The choice of activation function isn't just a technical detail — it",
            p3Highlight: "fundamentally shapes how information flows through the network and how efficiently it can learn",
            p3End: ". ReLU's simplicity made deep networks (10+ layers) practical for the first time, because it avoids the vanishing gradient problem that plagued sigmoid and tanh. This single innovation — choosing a better activation function — was one of the key breakthroughs that enabled the deep learning revolution.",
        },
        backpropagation: {
            title: "Backpropagation",
            lead: "We now have a network with weights, biases, and activation functions. But how does it actually learn? How do random initial parameters transform into structured knowledge? The answer is backpropagation — arguably the most important algorithm in all of artificial intelligence.",
            p1: "Learning starts with failure. We feed the network an input (say, the character \"h\"), it produces a prediction (a probability distribution over the next character), and we compare that prediction to what actually came next in the training text. The difference between prediction and reality is the error, and we measure it with a",
            p1Highlight: "loss function — a single number that tells us exactly how wrong we are",
            p1End: ". When the loss is high, the network is confused. When it's low, the network is getting things right. The entire goal of training is to make this number as small as possible.",
            lossCaption: "The loss function L(y, ŷ) compares the network's prediction ŷ to the true answer y. A higher loss means a worse prediction. Training is the process of minimizing this value across all training examples.",
            p2: "Here's the key question: given that we know our current error, how should we change each weight to reduce it? This is where calculus enters the picture. For each weight in the network, we calculate",
            p2Highlight: "how much the loss would change if we nudged that weight slightly",
            p2End: " — this is the gradient, the mathematical slope of the error with respect to that particular weight. A positive gradient means \"increasing this weight will increase the error\" (so we should decrease it). A negative gradient means \"increasing this weight will decrease the error\" (so we should increase it). The magnitude tells us how sensitive the error is to that weight.",
            p3: "But here's the brilliant trick that makes this efficient. In a multi-layer network, computing the gradient for weights in early layers seems like it would require tracing every possible path through the network — an exponentially expensive computation. Backpropagation solves this by working backwards: it computes the",
            p3Highlight: "gradient at the output layer first, then efficiently propagates it back through each layer using the chain rule of calculus",
            p3End: ". Each layer receives the gradient from the layer above it, multiplies by its own local gradient, and passes the result backward. This elegant recursion means that computing gradients for ALL weights takes roughly the same time as a single forward pass through the network.",
            p4: "Once we have the gradient for every weight, we apply the update rule: each weight is nudged in the opposite direction of its gradient, by a small step controlled by the \"learning rate\" (η). A learning rate too large will overshoot the optimal values and oscillate wildly. One too small will take forever to converge. Finding the right learning rate is one of the practical arts of training neural networks.",
            updateCaption: "The gradient descent update rule: each weight w is updated by subtracting the learning rate η times the gradient of the loss with respect to that weight. This simple rule, applied millions of times, is how neural networks learn.",
            p5: "This entire cycle — forward pass, compute loss, backward pass, update weights — repeats for every batch of training examples, potentially millions of times. Each cycle nudges the weights a tiny amount toward better predictions. It's a slow, iterative process, like a sculptor chipping away at stone one flake at a time. But the cumulative effect is remarkable: random noise transforms into a system that captures deep patterns in language.",
        },
        parameterUpdates: {
            title: "Learning as Parameter Updates",
            lead: "We've described the mechanics of a single update step. But real training involves doing this thousands or millions of times. What does the whole process look like? How does a network go from knowing nothing to capturing the statistical structure of a language?",
            p1: "Picture this: you initialize a neural network with random weights. You feed it the character \"t\" and ask it to predict what comes next. It outputs a nearly uniform distribution — every character is equally likely. The letter \"h\" gets the same probability as \"z\" or \"7\". The predictions are",
            p1Highlight: "indistinguishable from rolling a die — pure, structured noise",
            p1End: ". This is the starting point of every neural network ever trained. The loss is enormous because the network is maximally confused.",
            p2: "Now training begins. For each example in the training data, we compute the loss, run backpropagation, and update every weight by a tiny amount. After a few hundred updates, something magical starts to happen: the loss drops. The network begins to favor common characters over rare ones. After a few thousand updates, it starts to learn character-specific patterns — that vowels tend to follow consonants, that spaces follow certain letters. After tens of thousands of updates, it captures subtler patterns: \"th\" is almost always followed by \"e\", \"a\", \"i\", or \"o\". The network is literally sculpting its parameters to match the statistical structure of the language, one tiny nudge at a time.",
            insightTitle: "The Error Landscape",
            insightText: "Imagine the network's loss as a height on a vast, mountainous landscape. Each possible configuration of weights corresponds to a point in this landscape. Training is like dropping a ball onto this terrain and letting it roll downhill. The ball follows the steepest slope (the gradient) at each step. It might get stuck in valleys that aren't the absolute lowest point (local minima), or slow down on flat plateaus. But remarkably, in practice, gradient descent finds good-enough solutions for even enormously complex problems. The landscape of neural networks, despite its astronomical dimensionality, turns out to be surprisingly navigable.",
            p3: "The training process typically takes hours, days, or even weeks for large models. But when it converges, the result is extraordinary: a set of numerical parameters that encode, in compressed form, the patterns of an entire language. The network hasn't memorized the training text — it has extracted its regularities and stored them in a form that generalizes to text it has never seen.",
        },
        bigramConnection: {
            title: "Connection to Bigram Models",
            lead: "Here's a remarkable fact: you can build a neural network that does exactly what a bigram model does — and then watch it learn to do something better. This connection between counting and learning is one of the most illuminating ideas in all of language modeling.",
            p1: "Remember the bigram transition matrix? It's a table where each row represents a current character and each column represents the next character, with each cell holding a probability. Now consider a single-layer neural network with no hidden layers: it takes a one-hot encoded character as input (a vector of zeros with a single 1 indicating the current character), multiplies it by a weight matrix, and applies softmax to produce a probability distribution over the next character. Here's the punchline: this weight matrix IS the transition matrix. A single-layer linear network, trained to predict the next character, will converge to weights that are essentially identical to the log-probabilities in a bigram table. Counting and learning arrive at the same answer.",
            p2: "But the neural network can go further. Add a hidden layer with non-linear activations, and suddenly the network can learn",
            p2Highlight: "dense, compressed representations that capture similarities between characters",
            p2End: ". In a bigram model, 'a' and 'e' are completely unrelated entries in a table — two different row indices with no connection. In a neural network, the hidden layer might learn to represent both vowels with similar activation patterns, because they appear in similar contexts. This means knowledge about 'a' automatically transfers to 'e' — the network generalizes.",
            insightTitle: "From Tables to Representations",
            insightText: "A bigram model stores 96×96 = 9,216 independent probabilities. A neural network with a 32-dimensional hidden layer stores 96×32 + 32×96 = 6,144 parameters — fewer numbers, but organized in a way that captures deep structural patterns rather than isolated counts. Each character becomes a point in a 32-dimensional space where distance reflects linguistic similarity. This is the birth of the idea that leads, eventually, to word embeddings, transformer attention, and modern LLMs.",
            p3: "This is why neural language models are the natural successor to N-grams. They don't just memorize patterns — they learn a compressed, structured representation of language that can generalize far beyond the training data. The bigram model is the base case that proves the concept; the neural network is the generalization that unleashes its full potential.",
        },
        limitations: {
            title: "Power and Limitations",
            lead: "We've seen that neural networks can learn to do everything a bigram model does and more. But how much more? What are the boundaries of these systems? Understanding both the power and the limitations of early neural networks is essential to understanding why the field kept pushing toward deeper, more sophisticated architectures.",
            p1: "The most famous limitation of early neural networks is the XOR problem. XOR is a simple logical function: given two binary inputs, it outputs 1 if exactly one input is 1, and 0 otherwise. Draw these four points on a 2D plane, and you'll see the problem immediately — there is",
            p1Highlight: "no single straight line that can separate the 1s from the 0s",
            p1End: ". And since a single perceptron can only learn linear boundaries, it will fail at XOR no matter how long you train it. This was the mathematical proof that ended the first wave of neural network research. Minsky and Papert showed this in 1969, and the field went cold for nearly two decades.",
            p2: "The solution, it turned out, was almost embarrassingly simple: add a hidden layer. A network with one hidden layer of even just two neurons can solve XOR perfectly. The first layer learns to split the input space into non-linear regions, and the second layer combines those regions into the final answer. More generally, a network with one sufficiently large hidden layer can approximate ANY continuous function to arbitrary precision — this is the Universal Approximation Theorem, one of the most important results in neural network theory. The problem was never that neural networks couldn't do it. The problem was that nobody knew how to train multi-layer networks until backpropagation came along.",
            p3: "Even with backpropagation, early neural networks faced real practical challenges. Training was slow and unstable. Networks with many layers suffered from vanishing gradients — the error signal would shrink to nearly zero as it propagated backward through many layers, making it impossible for early layers to learn. They required enormous amounts of data relative to their size, and the computational resources of the 1990s were orders of magnitude too small for the ambitious architectures researchers dreamed of. These practical limitations kept neural networks as a niche tool for decades, useful but not dominant.",
            quote: "\"What I cannot create, I do not understand.\" — Richard Feynman. This philosophy drives neural network research: we understand language models by building them from scratch, from the simplest bigram to the most complex transformer.",
            p4: "The story of neural networks is a story of overcoming limitations one by one. The perceptron couldn't solve XOR — so we added hidden layers. Deep networks suffered from vanishing gradients — so we invented ReLU and batch normalization. Training was slow — so we parallelized it on GPUs. Context was limited — so we invented attention mechanisms and transformers. Each limitation in this chapter is not a dead end but a signpost pointing toward the next breakthrough. The basic building blocks you've learned here — weighted sums, activation functions, backpropagation, gradient descent — are the same building blocks inside GPT-4, DALL-E, and every other modern AI system. The scale is different, but the principles are identical.",
        },
        footer: {
            text: "You've now traced the full arc from counting (bigrams) through extended context (N-grams) to learned representations (neural networks). These are the foundational concepts that underpin all of modern AI. The next frontier — attention mechanisms and transformer architectures — builds directly on everything you've learned here.",
            brand: "LM-LAB · Educational Mode"
        }
    }
};
