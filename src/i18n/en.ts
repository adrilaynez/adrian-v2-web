export const en = {
    common: {
        language: "Language",
        loading: "Loading...",
        error: "Error",
        comingSoon: "Coming Soon",
        backToProjects: "Back to Projects",
        toggleLanguage: "Toggle Language",
        code: "Code",
        liveDemo: "Live Demo",
        viewCaseStudy: "View Case Study",
    },
    nav: {
        home: "Home",
        projects: "Projects",
        lab: "Lab",
        notes: "Notes",
    },
    lab: {
        bigram: "Bigram",
        ngram: "N-Gram",
        mlp: "MLP",
        transformer: "Transformer",
        neuralNetworks: "Neural Nets",
        playground: {
            inputs: {
                x1Label: "x₁ (input 1)",
                x2Label: "x₂ (input 2)",
                scaleHint: "(on scale 0–1)",
                xLabel: "Input x",
                wLabel: "Weight w",
                bLabel: "Bias b",
                targetLabel: "Target",
            },
        },
        shell: {
            allModels: "Back to Lab",
        },
        active: "Lab Active",
        waking: "Waking Up",
        serverWarning: {
            title: "BACKEND COLD-START DETECTED",
            subtitle: "CONTAINMENT PROTOCOL ACTIVE",
            message: "The server is waking up from hibernation. Free-tier Render instances spin down after inactivity — yes, I'm hosting this on a free server because I'm a broke student.",
            donate: "If the 30s wait is too painful, feel free to sponsor my coffee fund so I can afford a real server. Or just wait, it's free entertainment.",
            status: "ATTEMPTING CONNECTION",
            dismiss: "I'LL SURVIVE",
            connected: "SIGNAL ACQUIRED",
        },
        mode: {
            educational: "Educational",
            educationalDescription: "Guided learning experience with story-driven explanations and progressive reveals.",
            freeLab: "Free Lab",
            freeLabDescription: "Full access to all tools and visualizations for manual experimentation and analysis.",
            selectViewingMode: "Select Viewing Mode",
            availableModels: "Available Models",
        },
        status: {
            ready: "Ready",
            coming: "Coming",
        },
        models: {
            bigram: {
                name: "Bigram Explorer",
                subtitle: "Chapter 1 · Counting Pairs",
                description: "Start here. The simplest language model: count how often one character follows another, store it in a table, and sample the next character. You'll see exactly how a model can generate text from pure statistics — and why one character of memory isn't enough.",
            },
            ngram: {
                name: "N-Gram Lab",
                subtitle: "Chapter 2 · More Memory, More Problems",
                description: "What if the model remembers two characters? Three? Five? Predictions get sharper — but the table explodes exponentially and the model still can't generalize. Discover the fundamental limits of counting-based language models.",
            },
            neuralNetworks: {
                name: "Neural Networks",
                subtitle: "Chapter 3 · From Counting to Learning",
                description: "Counting hit a wall. Now we learn instead. Build intuition for perceptrons, activation functions, backpropagation, and gradient descent — the building blocks that let machines discover patterns on their own.",
            },
            mlp: {
                name: "MLP Language Model",
                subtitle: "Chapter 4 · Neural Language Modeling",
                description: "Apply everything you've learned: replace the N-gram lookup table with a neural network that uses dense embeddings and learned weights. See how an MLP generalizes beyond exact matches and produces better text with fewer parameters.",
            },
            transformer: {
                name: "Transformer Architecture",
                subtitle: "Chapter 5 · Attention Is All You Need",
                description: "The architecture behind GPT and modern AI. Self-attention lets the model dynamically focus on any part of the sequence, eliminating fixed context windows entirely. Coming soon.",
            },
        },
        dashboard: {
            chip: "Model Interpretability Lab",
            suite: "Suite",
            description1: "Explore the inner workings of language models through interactive visualizations.",
            description2: "Follow a guided path or experiment freely in the sandbox.",
            launchUnit: "LAUNCH UNIT",
            secureLock: "SECURE LOCK",
            footerCopyright: "© 2026 LM-LAB INSTRUMENTS",
            footerSystem: "INTERPRETABILITY_SYSTEM",
            secureConnection: "Secure Connection",
            hardwareMock: "Hardware: v4-8 TPU MOCK",
        },
        placeholders: {
            mlp: {
                title: "MLP Explorer",
                description: "Multi-Layer Perceptron language model explorer. Currently under development - check back soon.",
            },
            transformer: {
                title: "Transformer Explorer",
                description: "Attention-based transformer model explorer. Currently under development - check back soon.",
            },
        },
        landing: {
            hero: {
                badge: "Research Unit",
                subtitle: "Interactive Interpretability Lab",
                description: "Demystifying Language Models through first-principles engineering and visual proof.",
                subDescription: "This unit focuses on mechanistic interpretability: the reverse-engineering of neural weights into understandable human concepts.",
                start: "Initialize Base Model",
                recommended: "Recommended for beginners",
            },
            highlights: {
                visualizations: "Interactive Viz",
                inference: "Live Inference",
                guided: "Guided Path",
                backend: "PyTorch Backend",
            },
            learningPath: {
                title: "Learning Path",
                status: {
                    soon: "Developing",
                    ready: "Unit Active",
                },
            },
            modes: {
                title: "Laboratory Protocols",
                entryTitle: "Choose Your Experience",
                entrySubtitle: "Select how you want to explore the lab. You can change this at any time.",
                defaultNote: "Defaulting to Educational Mode",
                educational: {
                    title: "Educational Mode",
                    subtitle: "Guided Discovery",
                    description: "Step-by-step narrative explaining the 'why' behind the math. Best for conceptual learning.",
                    tag: "Recommended",
                    features: ["Narrative walkthroughs", "Progressive reveals", "Conceptual explanations"],
                },
                freeLab: {
                    title: "Free Lab Mode",
                    subtitle: "Sandbox Environment",
                    description: "Full access to all visualization tools and generation parameters. For advanced experimentation.",
                    tag: "Advanced",
                    features: ["All tools unlocked", "Raw parameter control", "No guided flow"],
                },
                cta: "Start with Bigram",
                ctaSubtext: "The simplest model — the best starting point",
                changeMode: "Change mode",
                selectedMode: "Selected",
            },
            availableModels: {
                title: "Biological Units Available",
                enter: "Enter Lab",
                locked: "Protocol Restricted",
            },
            footer: {
                text: "Scientific Instrument v2.2 // Build 2026",
            },
        },
    },
    training: {
        title: "Training Insights",
        noData: "Run inference to view training data",
        tooltip: {
            lossTitle: "What is Loss?",
            lossErrorPrefix: "Prediction Error:",
            lossError: "Loss measures how 'surprised' the model is. A high loss means it's guessing wrong frequently.",
            lossBenchmarkPrefix: "The Benchmark:",
            lossBenchmark: "Pure random guessing would give a loss of ~4.56 (-ln(1/96)). Anything lower means the model has actually learned something!",
            lossCurve: "The descending curve shows the model slowly discovering patterns in your text.",
        },
        stats: {
            finalLoss: "Final Loss",
            steps: "Steps",
            batchSize: "Batch Size",
            learningRate: "Learning Rate",
            parameters: "Parameters",
            tooltips: {
                finalLoss: "The error level. At the end of training, it should be as low as possible.",
                steps: "How many times the model practiced to improve its predictions.",
                batchSize: "The amount of information pieces the model processes at once.",
                learningRate: "The learning speed. Neither too fast to avoid missing, nor too slow to avoid taking too long.",
                parameters: "The size of the neural network or 'brain' of the model.",
            },
        },
    },
    ngram: {
        training: {
            title: "Training Insights",
            stats: {
                totalTokens: "Total Tokens",
                uniqueContexts: "Unique Contexts",
                utilization: "Context Utilization",
                sparsity: "Sparsity",
                transitionDensity: "Transition Density",
                subs: {
                    possiblePrefix: "of",
                    possibleSuffix: "possible",
                    fractionObserved: "Fraction of contexts observed",
                    unseen: "Unseen context fraction",
                },
            },
        },
        widgets: {
            typoBreaker: {
                title: "Break the Model",
                subtitle: "Type a misspelled word or novel phrase — watch the model fail",
                placeholder: "Type a misspelled word or new phrase…",
                reset: "Reset",
                test: "Test",
                tryLabel: "Try:",
                contextLookup: "Context lookup",
                modelConfidence: "Model confidence",
                randomMarker: "↑ random (1/{vocab} ≈ 1%)",
                verdictKnownPrefix: "The model found a familiar context",
                verdictKnownSuffix: "and can make a reasonable prediction. But change even one character and the entire context becomes unknown.",
                verdictUnknownStrong: "No matching context found.",
                verdictUnknownBody: "The model has never seen this exact character sequence in training. Confidence collapses to random chance (1/{vocab} per character). A human would easily understand the intent — the N-gram model cannot.",
                examples: {
                    swapTwoLetters: "Swap two letters",
                    commonMisspelling: "Common misspelling",
                    novelWord: "Novel word",
                    missingVowel: "Missing vowel",
                },
            },
            generalizationFailure: {
                seenInTraining: "Seen in training",
                neverSeenInTraining: "Never seen in training",
                nextWord: "next word →",
                confidence: "confidence",
                neverSeenNoPrediction: "Never seen. No prediction possible.",
                explanation: "Swapping \"cat\" for \"dog\" creates a brand-new row in the table. The model cannot transfer what it knows about cats — each context is completely isolated.",
            },
            sparsityHeatmap: {
                title: "Table Density Heatmap",
                subtitle: "How much of the probability table actually has data?",
                entriesSuffix: "entries",
                fill: "Fill",
                density: "Density:",
                legend: {
                    high: "High",
                    medium: "Medium",
                    low: "Low",
                    empty: "Empty",
                },
                insights: {
                    n1: "The bigram table is mostly filled — with only 96 possible contexts, even a modest corpus covers most character pairs. But this model only sees one character of history.",
                    n2: "The trigram table is already noticeably sparser. With 9,216 possible contexts, many 2-character combinations never appear in training. The model starts guessing randomly for unseen contexts.",
                    n3: "The 4-gram table is almost entirely empty. With 884,736 possible contexts, the vast majority have zero training examples. The model is essentially blind for most inputs.",
                    n4: "The 5-gram table is a desert of zeros. Over 8 billion possible contexts, and your training data covers a vanishingly small fraction. This is not a solvable problem — it's a mathematical certainty.",
                },
            },
            confidenceImprovement: {
                title: "How Context Sharpens Predictions",
                subtitle: "Click any row to see the full candidate distribution",
                after: "After",
                summary: "32% → 85% → 91% — more context = sharper predictions",
                hints: {
                    n1: "After just \"h\", many vowels and consonants are plausible. The model spreads probability thinly.",
                    n2: "\"th\" is a powerful signal — in English, \"the\" is the most common word. Confidence jumps dramatically.",
                    n3: "\"the\" almost always ends with a space. The model is now 91% sure — very little ambiguity remains.",
                },
            },
            contextDrilldown: {
                lookupTitle: "Context Lookup",
                lookupSubtitle: "Type a {n}-character context to see its next-character distribution",
                lookupPlaceholder: "Enter {n} characters…",
                lookupButton: "Lookup",
                progressSuffix: "characters",
                noDataFree: "No data found for this context. The model may not have seen \"{context}\" in training. This is the sparsity problem in action.",
                drilldownTitle: "Context Drilldown",
                drilldownSubtitle: "Pick {n} character{suffix} to explore the model's predictions",
                breadcrumbStart: "Start",
                breadcrumbDistribution: "distribution",
                pickFirst: "Pick the first character ({remaining} remaining)",
                pickNext: "After \"{context}\" — pick next ({remaining} remaining)",
                back: "Back",
                fetching: "Fetching distribution for \"{context}\"…",
                fetchError: "Failed to fetch distribution",
                noDataDrilldown: "No data for context \"{context}\". This context was never observed in training — the sparsity problem in action.",
                chartTitle: "P(next | \"{context}\")",
                chartTop: "Top {count} predictions",
            },
            infiniteTable: {
                title: "The Data Coverage Problem",
                subtitle: "How much of each N-gram table can you fill with real data?",
                trainingDataSize: "Training data size",
                tokensLabel: "{count} tokens",
                entriesLabel: "{count} entries",
                insight: {
                    v0: "Even with {tokens} tokens of training data, the 5-gram table ({entries} entries) is virtually empty. The model would have no prediction for almost any context it encounters.",
                    v1: "With {tokens} tokens, the 5-gram table is less than 1% filled. Most contexts the model encounters at test time will have zero training examples.",
                    v2: "With {tokens} tokens, low-N tables fill up — but the 5-gram table is still only {pct} covered. Sparsity is a data problem, not just a storage problem.",
                },
            },
            countingComparison: {
                bigramTitle: "Bigram (N=1)",
                bigramContext: "1 char context",
                bigramNote: "Wide spread — many characters plausibly follow \"h\".",
                trigramTitle: "Trigram (N=2)",
                trigramContext: "2 char context",
                trigramNote: "Sharper — \"th\" almost always leads to \"e\".",
                tooltips: {
                    h_e: "After 'h', 'e' is the most common next character — but many other letters also follow 'h'.",
                    h_a: "After 'h', 'a' appears in words like 'have', 'hand', 'hard'.",
                    h_i: "After 'h', 'i' appears in words like 'his', 'him', 'hit'.",
                    th_e: "After 'th', 'e' is overwhelmingly likely — 'the' is the most common English word.",
                    th_a: "After 'th', 'a' appears in 'that', 'than', 'thank'.",
                    th_i: "After 'th', 'i' appears in 'this', 'thing', 'think'.",
                },
            },
        },
    },
    landing: {
        hero: {
            status: "System Online :: v2.2",
            role: "Research & Engineering",
            title: "ADRIAN LAYNEZ ORTIZ",
            tagline1: "Mathematics & Computer Science.",
            tagline2: "Mechanistic Interpretability · High-Performance Engineering.",
            cta: {
                lab: "View Lab Work",
                notes: "Read Notes",
            },
        },
        metrics: {
            research: "Years of Research",
            repos: "Open Source Repos",
            projects: "Active Projects",
            curiosity: "Curiosity",
        },
        about: {
            badge: "About",
            building: "Currently Building",
            projectTitle: "Deep Learning Engine — CUDA / C++",
            projectDesc: "Custom kernels for matrix operations and backpropagation",
            bio: {
                titlePrefix: "Bridging Abstract Mathematics",
                titleSuffix: "& Machine Intelligence",
                p1: "I am pursuing a double degree in <strong class='text-foreground'>Mathematics and Computer Science</strong> at the Universidad Complutense de Madrid. My research focuses on understanding neural networks at their deepest level — from gradient dynamics to kernel-level optimization.",
                p2: "I specialize in <strong class='text-foreground'>Mechanistic Interpretability</strong> — the science of reverse-engineering how neural networks represent and process information internally. Rather than treating models as black boxes, I decompose their circuits to understand <em class='text-foreground/80'>why they work</em>.",
                mission: "My mission: make AI systems transparent through rigorous mathematical analysis and low-level engineering.",
            },
        },
        skills: {
            title: "Technical Proficiencies",
            linearAlgebra: "Linear Algebra",
            topology: "Topology",
            convexOpt: "Convex Optimization",
        },
        work: {
            badge: "Selected Work",
            titlePrefix: "Engineering from",
            titleSuffix: "First Principles",
            description: "Every project begins with a question. From reimplementing seminal papers to writing bare-metal GPU kernels, each one is an exercise in deep understanding.",
            viewAll: "View All Projects",
            items: {
                nanoTransformer: {
                    title: "Nano-Transformer",
                    desc: "Ground-up reproduction of 'Attention Is All You Need' in PyTorch — Multi-Head Attention, Positional Encodings, and LayerNorm implemented without pre-built Transformer modules.",
                },
                cudaKernels: {
                    title: "CUDA Matrix Kernels",
                    desc: "Handwritten CUDA kernels exploring SGEMM optimization — from naive implementations to tiled shared-memory strategies, benchmarked against cuBLAS.",
                },
                autograd: {
                    title: "Autograd Engine",
                    desc: "Lightweight reverse-mode automatic differentiation library. Dynamically constructs computation graphs and propagates gradients via the chain rule.",
                },
                mathDl: {
                    title: "The Mathematics of Deep Learning",
                    desc: "Interactive articles exploring the rigorous theory behind modern AI — SGD convergence analysis, the linear algebra of LoRA, and differential geometry on neural manifolds.",
                },
                distributed: {
                    title: "Distributed Inference",
                    desc: "Architectural explorations in data-parallel training, model sharding, and optimized inference pipelines for large-scale neural networks.",
                },
            },
        },
        contact: {
            badge: "Open to Opportunities",
            titlePrefix: "Let's Build",
            titleMiddle: "Something",
            titleSuffix: "Together",
            description: "Whether it's a research collaboration, an internship opportunity, or just a conversation about the mathematics of intelligence — I'd love to hear from you.",
            email: "Get in Touch",
            github: "GitHub Profile",
            githubShort: "GitHub",
            linkedin: "LinkedIn",
        },
    },
    projects: {
        hero: {
            badge: "Research & Development",
            titlePrefix: "Constructing the",
            titleSuffix: "Digital Frontier.",
            description: "A curated collection of my work in distributed systems, AI infrastructure, and high-performance computing.",
        },
        flagship: {
            badge: "Flagship Project",
            featured: "Featured",
            liveDemo: "Live Demo Available",
            title: "LM-Lab",
            description: "An interactive platform for exploring language model architectures from first principles. Visualize transition matrices, probe inference dynamics, and generate text — all powered by a live FastAPI backend with PyTorch.",
            highlights: {
                inference: {
                    title: "Live Inference",
                    desc: "Real-time next-character prediction with probability distributions",
                },
                matrix: {
                    title: "Transition Matrix",
                    desc: "Interactive canvas heatmap of the learned bigram probabilities",
                },
                generation: {
                    title: "Text Generation",
                    desc: "Generate text with configurable temperature and step-by-step tracing",
                },
            },
            cta: {
                explorer: "Open Lab",
                architecture: "View Architecture",
                demo: "Run Interactive Demo",
            },
        },
        experiments: {
            title: "Selected Experiments",
            items: {
                distriKv: {
                    title: "Distri-KV",
                    desc: "A distributed key-value store implemented in Go, featuring Raft consensus and sharding.",
                },
                neuroVis: {
                    title: "NeuroVis",
                    desc: "Interactive visualization tool for neural network activations in real-time.",
                },
                autoAgent: {
                    title: "Auto-Agent",
                    desc: "A lightweight autonomous agent framework focused on coding tasks.",
                },
            },
        },
    },
    notes: {
        hero: {
            est: "EST. 2024",
            archive: "RESEARCH ARCHIVE",
            titlePrefix: "The Engineering",
            titleSuffix: "Logbook",
            description: "Explorations in <strong class='text-primary'>distributed intelligence</strong>, high-dimensional topology, and the mechanics of modern software.",
        },
        featured: {
            badge: "LATEST RESEARCH",
            readTime: "{minutes} min read",
            figure: "Figure 1.0: Latent Space Visualization",
        },
        grid: {
            title: "Previous Entries",
        },
        backToNotes: "Back to Notes",
        noteNotFound: "Note Not Found",
    },
    footer: {
        builtBy: "Built by",
        sourceAvailable: "The source code is available on",
    },
    datasetExplorer: {
        title: "Corpus Evidence",
        subtitle: "Why did the model learn '{context}' -> '{next}'?",
        scanning: "Scanning training corpus...",
        occurrencesFound: "Occurrences Found",
        source: "Source",
        contextSnippets: "Context Snippets",
        noExamples: "No examples found for this transition.",
        fetchError: "Failed to fetch dataset examples",
        explorerTitle: "Corpus Explorer",
        searching: "Searching Dataset...",
        querySequence: "Query Sequence",
        found: "Found {count} occurrences",
        exampleContexts: "Example Contexts",
        noExamplesValidation: "No examples found in the validation snippet.",
    },
    models: {
        bigram: {
            title: "Bigram Language Model",
            description: "The fundamental building block of sequence modeling. A probabilistic model that predicts the next character based solely on the immediate predecessor.",
            params: "Parameters",
            vocab: "Vocabulary",
            trainingData: "Training Data",
            loss: "Final Loss",
            unknown: "Unknown",
            tooltips: {
                params: "They are like the brain's connections. This model is simple, so it doesn't need many.",
                vocab: "It's the set of letters and symbols the model knows, like its own alphabet.",
                trainingData: "The amount of text the model read to learn how to write.",
                loss: "It's the 'error' score. The lower it is, the better the model knows which letter comes next.",
            },
            sections: {
                visualization: {
                    title: "Visualization: Transition Matrix",
                    description: "This is where the model's 'knowledge' lives. For a Bigram model, this grid represents which letters typically follow others.",
                },
                inference: {
                    title: "Inference and Generation",
                    description: "Interact with the model in real-time. Watch how it 'guesses' the next character based on learned probabilities.",
                    placeholder: "Type text to analyze...",
                },
                architecture: {
                    title: "Model Architecture",
                    description: "A technical look at the 'neurons' and layers that process information.",
                },
                training: {
                    title: "Training Insights",
                    description: "Observing the learning process. These metrics show how the model optimized its parameters by reducing prediction error (loss) over 5000 iterations.",
                },
            },
            hero: {
                scientificInstrument: "Scientific Instrument v1.0",
                explanationButton: "Need an intuitive explanation?",
                explanationSub: "Understand the core idea before diving into the math and visualizations.",
            },
            matrix: {
                title: "Transition Matrix",
                activeSlice: "Active Slice Transition",
                tryIt: {
                    label: "Try it:",
                    text: "Click any colored cell in the matrix to see",
                    highlight: "real training examples",
                },
                searchPlaceholder: "Highlight character…",
                runInference: "Run inference to generate the transition matrix",
                nextChar: "Next char",
                probability: "P (%)",
                distribution: "Distribution",
                tooltip: {
                    title: "How to read this chart?",
                    desc: "Rows represent the current character and columns represent the next character. Brighter cells indicate higher transition probability.",
                    rows: "Rows (Y):",
                    rowsDesc: "The letter the model just wrote.",
                    cols: "Columns (X):",
                    colsDesc: "The letter the model is trying to guess.",
                    brightness: "Brightness:",
                    brightnessDesc: "The brighter a square is, the more likely that pair of letters appears in the text.",
                    example: "Example: If the row is 'q' and the 'u' column shines brightly, it means the model knows that after 'q' almost always comes 'u'.",
                },
                slice: "Slice:",
                datasetMeta: {
                    learnedFrom: "Learned from",
                    summarizes: "summarizes",
                    rawChars: "raw characters",
                    inTrain: "in training split",
                    vocab: "across",
                    symbols: "unique symbols",
                    corpus: "Corpus Name:",
                    rawText: "Total Raw Text:",
                    trainingSplit: "Training Data:",
                    vocabulary: "Vocabulary Size:",
                    charTokens: "characters",
                },
                probFlow: {
                    badge: "Probability Flow Visualizer",
                    alreadyNormalized: "⚠ Matrix appears pre-normalized",
                    description: "Explore how raw counts become probabilities and how the model samples the next token. This interactive diagram shows the complete inference pipeline: from selecting a context character, to normalizing its row into a probability distribution, to stochastically sampling the next token.",
                    step1: "Step 1: Select Context",
                    step2: "Step 2: Normalize",
                    step3: "Step 3: Sample Next Token",
                    currentToken: "Current Token",
                    typeToChange: "Type to change context",
                    normalize: "Normalize",
                    softmax: "Softmax",
                    temperature: "Temperature",
                    educational: {
                        normTitle: "Simple Normalization",
                        normDesc: "Divide each count by the row sum. This converts raw frequencies into probabilities that sum to 1.0.",
                        softmaxTitle: "Softmax (Temperature-Scaled)",
                        softmaxDesc: "Exponentiates values and normalizes. Temperature controls sharpness: low temp → peaked distribution, high temp → uniform distribution.",
                        tempTitle: "Temperature",
                        tempDesc: "Controls the sharpness of the distribution. Low temperature (< 1) concentrates probability on the most likely tokens. High temperature (> 1) spreads it more evenly, producing more varied — and often surprising — output.",
                    },
                    tempLabel: "Temperature",
                    tempTooltip: "Controls randomness. Lower = more deterministic, Higher = more creative/random",
                    sampleButton: "Sample Next Token",
                    sample: "Sample Next Token",
                    sampling: "Sampling...",
                    result: "Sampled Result",
                    sampled: "Sampled",
                    topCandidate: "Top candidate",
                    mostLikely: "Most Likely",
                    probability: "Probability",
                    roll: "Random Roll",
                    explanation: "The model threw a weighted dice (roll = {roll}) and selected '{token}' with probability {prob}%",
                    stochasticNote: "Sampling is stochastic — each click may produce a different result even for the same context character.",
                },
                labModeGuide: "This is the full transition matrix trained on Paul Graham essays. Each row is a character; each column is the character that follows. Brighter cells = more frequent transitions. Click any cell to see real training examples from the corpus.",
                limitationGuide: "Notice the fundamental constraint: the model only looks at the last character. It cannot learn that 'th' is almost always followed by 'e', because by the time it sees 'h', the 't' is already forgotten. This single-token memory is exactly what N-gram and neural models overcome.",
                storySteps: {
                    problem: {
                        title: "The Problem",
                        body: "Language is sequential — every character depends on what came before. The challenge is to capture this structure computationally. How do we build a model that can predict what comes next in a stream of text?",
                    },
                    representation: {
                        title: "Representing Text",
                        body: "Before we can model language, we need to decide how to represent it. The choice of representation determines the vocabulary size, the model's capacity, and its limitations.",
                    },
                    solution: {
                        title: "The Bigram Solution",
                        body: "The simplest approach: count how often each character follows every other character in a large training corpus. These counts, once normalized into probabilities, form a complete statistical model of character-level language.",
                    },
                    matrix: {
                        title: "The Transition Matrix",
                        body: "Every count is stored in a V × V matrix (V = vocabulary size). Each row represents a current character; each column represents the next. The brightness of a cell encodes the transition probability learned from real text.",
                    },
                    probabilities: {
                        title: "From Counts to Probabilities",
                        body: "Raw counts are normalized row-by-row so each row sums to 1.0, forming a valid probability distribution. The model can then make concrete predictions: \"After 'h', there is a 34% chance the next character is 'e'.\"",
                    },
                    limitation: {
                        title: "The Fundamental Limitation",
                        body: "The bigram model has zero memory beyond the immediately preceding character. It cannot learn that 'th' is almost always followed by 'e', because by the time it sees 'h', the 't' is already forgotten. This single-token horizon is what motivates N-gram and neural models.",
                    },
                },
                representation: {
                    charTitle: "Character-level tokens",
                    charBody: "Small, fixed vocabulary (~96 printable ASCII characters). Every possible input is representable. Simple to implement and visualize — ideal for understanding the fundamentals.",
                    wordTitle: "Word-level tokens",
                    wordBody: "Richer semantic units, but vocabulary can reach 50,000–500,000 entries. Rare words cause sparsity; unseen words at inference time cause failures. Much harder to scale.",
                },
                builderLabel: "Step-by-step bigram builder",
            },
            inference: {
                title: "Inference Console",
                probDist: "1. Probability Distribution",
                probDistDesc: "Type a phrase to see the top-k most likely next characters.",
                tooltip: {
                    title: "What is Inference?",
                    process: "The Process:",
                    processDesc: "The model takes your text, looks at the",
                    processHighlight: "last character",
                    processEnd: ", and looks up the probabilities for what comes next in its brain (the Matrix).",
                    topK: "Top-K:",
                    topKDesc: "We only show the top winners. If K=5, you see the 5 most likely candidates.",
                    note: "Note: This model is \"deterministic\" in its probabilities but \"stochastic\" (random) when it actually picks a character to generate text.",
                },
                lastChar: "Last char:",
                form: {
                    input: "Input Text",
                    placeholder: "Type text to analyze...",
                    topK: "Top-K Predictions",
                    analyze: "Analyze",
                    analyzing: "Analyzing...",
                },
            },
            stepwise: {
                title: "Stepwise Prediction",
                mainTitle: "2. Stepwise Prediction",
                description: "Watch the model predict a sequence character-by-character.",
                form: {
                    input: "Input Text",
                    placeholder: "Starting text...",
                    steps: "Prediction Steps",
                    predict: "Predict Steps",
                    predicting: "Predicting...",
                },
                table: {
                    step: "Step",
                    char: "Char",
                    prob: "Probability",
                },
                result: "Result:",
            },
            generation: {
                title: "Generation Playground",
                mainTitle: "3. Text Generation",
                description: "Let the model hallucinate text by sampling from the distribution.",
                tooltip: {
                    title: "How is text generated?",
                    sampling: "Sampling:",
                    samplingDesc: "The model doesn't just pick the #1 answer. It \"rolls a dice\" weighted by probabilities. This is why it can generate different text every time.",
                    temp: "Temperature:",
                    tempDesc: "Higher values make the dice roll \"wilder\" (more random). Lower values make it \"safer\" and more repetitive.",
                    note: "Try temperature 2.0 to see complete gibberish, or 0.1 to see it get stuck in loops!",
                },
                form: {
                    startChar: "Start Character",
                    numTokens: "Number of Tokens",
                    temp: "Temperature",
                    generate: "Generate",
                    generating: "Generating...",
                },
                copyToClipboard: "Copy generated text",
            },
            architecture: {
                title: "Technical Specification",
                subtitle: "Detailed breakdown of the model's internal mechanism, capabilities, and constraints.",
                mechanism: "Inference Mechanism",
                capabilities: "Capabilities",
                constraints: "Constraints",
                modelCard: {
                    title: "Model Card",
                    type: "Architecture Type",
                    complexity: "Complexity Rating",
                    useCases: "Primary Use Cases",
                    description: "Description",
                },
                tooltips: {
                    matrixW: {
                        title: "What is Matrix W?",
                        desc: "It's essentially a lookup table of 9216 numbers (96x96 characters in the vocab). Each number represents the \"unnormalized score\" of how likely one character follows another.",
                    },
                    softmax: {
                        title: "What is Softmax?",
                        desc: "Softmax takes raw scores (logits) and squashes them into a probability distribution. All numbers become positive and add up to 1 (100%).",
                    },
                    loss: {
                        title: "What is Loss (Cross-Entropy)?",
                        desc: "Loss measures the distance between the model's prediction and the truth. If the truth is 'n' and the model gave 'n' a 0.1% chance, the loss will be very high. Training is the process of tuning weights to minimize this distance.",
                    },
                },
                stepsList: {
                    matrixW: "Look up the row of the weight matrix W corresponding to the current character's index. This row contains the raw unnormalized scores (logits) for every possible next character.",
                    softmax: "Apply softmax to the logit row to produce a valid probability distribution over the vocabulary. Every value becomes positive and the row sums to exactly 1.0.",
                    loss: "During training, compute cross-entropy loss between the predicted distribution and the true next character. Backpropagate gradients to update W and minimize future prediction error.",
                },
                analysis: {
                    strengths: [
                        "Exact closed-form solution — no gradient descent required. Counts are sufficient statistics.",
                        "Instant training on any corpus size. O(N) in the number of training tokens.",
                        "Fully interpretable: every cell in W is a directly readable probability.",
                    ],
                    limitations: [
                        "Zero context beyond the immediately preceding token — cannot model multi-character patterns.",
                        "No generalization: each character pair is treated independently with no notion of similarity.",
                        "Vocabulary scales as O(V²) — impractical for word-level models with large vocabularies.",
                    ],
                },
                steps: {
                    predicts: "Predicts next character via:",
                    optimizes: "Optimizes parameters using:",
                },
            },
            guide: {
                badge: "Guide for Non-Technical Explorers",
                title: "How does this \"Brain\" work?",
                subtitle: "Explaining the Bigram model so even my mom can understand it (with lots of love).",
                switchHint: "Switch to Educational Mode to see the conceptual guide",
                cards: {
                    memory: {
                        title: "Goldfish Memory",
                        desc: "A **Bigram** model has the shortest memory in the world: it only remembers the **last letter** it wrote. To decide which letter comes next, it can only look at the previous one. It has no context of entire words or phrases.",
                    },
                    darts: {
                        title: "Darts Throw",
                        desc: "The model doesn't \"read\". It just has a giant table that says: \"If the last letter was 'a', there's a 10% probability that the next is 'n'\". Throwing the dart (sampling) is what generates text in a random but coherent way.",
                    },
                    heatmap: {
                        title: "The Heatmap",
                        desc: "The colored grid (Matrix) is the **heart** of the model. The bright squares are the most frequent \"routes\" the model found in the books it read during its training.",
                    },
                },
            },
            historicalContext: {
                description: "The bigram model is the simplest instance of a Markov chain applied to language. First studied by Claude Shannon in his 1948 paper 'A Mathematical Theory of Communication', character-level bigrams demonstrated that even zero-context statistical models capture meaningful structure in natural language.",
                limitations: [
                    "Zero memory beyond the immediate predecessor — cannot learn multi-character patterns like 'th' → 'e'.",
                    "No generalization — each character pair is treated independently with no notion of similarity.",
                ],
                evolution: "The limitations of bigram models directly motivated N-gram extensions (longer context) and eventually neural approaches (learned representations). Every modern language model can trace its lineage back to this simple transition matrix.",
            },
            educationalOverlay: {
                visualGuideTitle: "Visualization Guide",
                visualGuideDescription: "Each cell in this matrix represents P(next | current) - the probability that one character follows another. Brighter cells indicate more frequent character pairings found in the training corpus.",
                probabilityAnalysisTitle: "Probability Analysis",
                probabilityAnalysisDescription: "Type any text to see which characters the model predicts will come next, ranked by learned probability. The model only looks at the very last character - it has no memory of earlier context.",
                generationLabTitle: "Generation Lab",
                generationLabDescription: "Text generation works by repeatedly sampling from the probability distribution. The temperature parameter controls how random each sample is - lower values produce more predictable output, higher values produce creative (or nonsensical) sequences.",
            },
        },
        ngram: {
            title: "N-Gram Language Model",
            description: "A character-level statistical language model with variable context size. Visualize how increasing the context window sharpens predictions at the cost of exponential sparsity.",
            sections: {
                context: {
                    title: "Context Size",
                    description: "Adjust the context size (N) to condition predictions on more history.",
                },
                slice: {
                    title: "Active Slice",
                    descriptionN1: "For N=1 (Bigram), we visualize the simple Markov transition matrix P(next | current).",
                    descriptionNPlus: "For N>1, we visualize the conditional slice P(next | context). Click cells to trace examples.",
                },
                inference: {
                    title: "Inference & Generation",
                    description: "Interact with the model in real-time. Observe how it selects the next token based on the learned probabilities.",
                    placeholder: "Type text to analyze...",
                    distribution: {
                        title: "Probability Distribution",
                        desc: "Type a phrase to see the top-k most likely next characters.",
                    },
                    stepwise: {
                        title: "Stepwise Prediction",
                        desc: "Watch the model predict a sequence character-by-character.",
                    },
                    generation: {
                        title: "Text Generation",
                        desc: "Let the model hallucinate text by sampling from the distribution.",
                    },
                },
            },
            hero: {
                stats: {
                    uniqueContexts: { label: "Unique Contexts", desc: "Observed n-grams" },
                    vocab: { label: "Vocabulary", desc: "Unique characters" },
                    contextSpace: { label: "Context Space", desc: "|V|^{n}" },
                    tokens: { label: "Training Tokens", desc: "Total tokens seen" },
                },
            },
            viz: {
                hint: {
                    label: "Try it:",
                    text: "Click any colored cell in the matrix to see <strong class='text-white font-semibold'>real training examples</strong>.",
                },
            },
            controls: {
                contextSize: "Context Size (N)",
                contextDesc: "Number of previous characters to condition on",
                unigram: "Unigram",
                bigram: "Bigram",
                trigram: "Trigram",
                fourgram: "4-gram",
                fivegram: "5-gram",
                explosion: "Explosion (5+)",
            },
            lab: {
                technicalExplanation: {
                    title: "Technical explanation",
                    description: "Detailed breakdown of the model's internal mechanism, capabilities, and constraints.",
                    mechanism: "Inference mechanism",
                    capabilitiesTitle: "Capabilities",
                    constraintsTitle: "Constraints",
                    steps: {
                        lookup: "Look up the probability row for the last {n}-character context.",
                        normalize: "Apply smoothing (add-α, α={alpha}) and normalize into a probability distribution.",
                        predict: "Sample or take argmax to predict the next character.",
                    },
                    capabilities: [
                        "Exact closed-form training — counting is sufficient.",
                        "Sharper local predictions as context length increases.",
                        "Fully interpretable: every table entry is a readable statistic.",
                    ],
                    constraints: [
                        "No generalization: unseen contexts have no entry.",
                        "Exponential state space: |V|^N contexts grows too fast to fill.",
                        "Limited horizon: it forgets everything before the last N characters.",
                    ],
                    modelCardTitle: "Model card",
                    complexity: "Complexity rating",
                    complexityValue: "Low",
                    useCases: "Primary use cases",
                    useCasesList: [
                        "Educational exploration",
                        "Baseline for comparison",
                        "Fast local text generation",
                    ],
                    trainingStats: "Training stats",
                    modelCard: "MODEL CARD · N={n}",
                    modelType: "Model type",
                    modelTypeValue: "{nPlusOne}-gram (context length = {n})",
                    parameterCount: "Parameter count",
                    parameterCountValue: "|V|^{n} × |V| = {count}",
                    trainingMethod: "Training method",
                    trainingMethodValue: "Maximum likelihood (counting)",
                    smoothing: "Smoothing",
                    smoothingValue: "Add-α Laplace smoothing (α={alpha})",
                    corpusInfo: "Corpus",
                    trainingTokens: "Training tokens",
                    trainingTokensValue: "{count}",
                    uniqueContexts: "Unique contexts",
                    uniqueContextsValue: "{seen} / {possible}",
                    perplexity: "Perplexity",
                    finalLoss: "Final loss",
                    inferenceComplexity: "Inference complexity",
                    inferenceComplexityValue: "O(|V|) per step (table lookup)",
                    mathematicalFormulation: "Mathematical formulation",
                    formulaDesc: "Counts are normalized into conditional probabilities for each context.",
                },
                guidedExperiments: "Guided Experiments",
                guidedExperimentsChallenges: "5 challenges",
                advancedMetrics: "Advanced Metrics",
                advancedMetricsExperts: "for experts",
                advancedMetricsDesc: "Loss, perplexity & performance",
                advancedMetricsHint: "These metrics (loss, perplexity, NLL) will make more sense after the Neural Networks chapter. For now, lower perplexity = better predictions.",
                badge: "Free Lab Mode · Full instrument access",
                experiments: {
                    instructions: "Instructions",
                    expectedObservation: "Expected observation",
                    goToPanel: "Go to panel",
                    1: {
                        title: "The Context Effect",
                        instruction: "Set N=1, generate 50 characters, and save the output. Then set N=3 and generate again from the same seed phrase.",
                        observation: "N=3 output reads more naturally — you'll see common prefixes like 'th', 'the', 'in' appear more reliably than with N=1.",
                    },
                    2: {
                        title: "Finding the Wall",
                        instruction: "Step through N=1 → N=2 → N=3 → N=4 and watch the Sparsity panel after each change. Record the perplexity and context utilization at each N.",
                        observation: "Perplexity drops with each step, but context utilization also plummets. At N=4, most rows in the table are empty — the model runs out of evidence.",
                    },
                    3: {
                        title: "The Impossible Context",
                        instruction: "Set N=4. In the Inference Console, type a phrase the model has never seen — try 'zqxj' or any unusual 4-character combination.",
                        observation: "The model returns no confident prediction. It has no entry for this exact 4-character context and cannot reason by analogy.",
                    },
                    4: {
                        title: "Bigram vs 4-gram Showdown",
                        instruction: "Generate 80 characters at N=1 and save it. Then switch to N=4 and generate 80 characters from the same seed. Read both outputs aloud.",
                        observation: "N=1 sounds random. N=4 produces recognizable fragments but breaks down mid-sequence when it hits unseen contexts and has to guess randomly.",
                    },
                    5: {
                        title: "Diminishing Returns",
                        instruction: "Record the perplexity from the Performance Summary at N=1, 2, 3, and 4. Calculate the drop from each step to the next.",
                        observation: "The improvement from N=1→2 is large. N=2→3 is smaller. N=3→4 is smaller still. More memory helps less and less as sparsity grows.",
                    },
                },
                contextLevels: {
                    1: "No context — each character is predicted independently from the corpus frequency. Fastest but least accurate.",
                    2: "Conditions on 1 previous character. Simple Markov chain; low sparsity, moderate precision.",
                    3: "Conditions on 2 previous characters. Better predictions but context space grows to |V|².",
                    4: "Conditions on 3 characters. High precision on seen sequences; significant sparsity on unseen ones.",
                    5: "Maximum context. Very sharp predictions where data exists, but most contexts are unseen — combinatorial explosion imminent.",
                },
                flow: {
                    afterContext: "The matrix below shows the probability distribution learned from training data for the current N level.",
                    afterMatrix: "Use the inference console to query the model with your own text and observe how context size affects predictions.",
                    afterComparison: "The training quality chart below reflects how well the model fits the corpus at the selected N level.",
                },
                performanceSummary: {
                    title: "Performance Summary",
                    description: "Runtime and training metrics for the current model",
                    inferenceTime: "Inference Time",
                    device: "Device",
                    corpusSize: "Corpus Size",
                    trainingDuration: "Training Duration",
                    totalTokens: "Total Tokens",
                    perplexity: "Perplexity",
                    finalLoss: "Final NLL",
                    ms: "ms",
                    tokens: "tokens",
                },
                comparison: {
                    title: "Model Comparison",
                    description: "Metrics across N=1..5",
                    ppl: "PPL",
                    util: "Util",
                    space: "Space",
                    tooltipPpl: "Perplexity — lower means more confident predictions",
                    tooltipUtil: "Fraction of possible contexts seen during training",
                    tooltipSpace: "Total possible context combinations (|V|^N)",
                },
                sparsity: {
                    title: "Data Sparsity",
                    description: "How much of the context space is actually observed",
                    observedContexts: "Observed Contexts",
                    possibleSuffix: "possible",
                    avgTransitions: "Avg. Transitions / Context",
                    nextTokens: "next-tokens per observed context",
                    utilLabel: "Context utilization",
                    utilHint: "Fraction of possible contexts seen in training data",
                    sparsityLabel: "Table sparsity",
                    sparsityHint: "Fraction of (context, next-token) pairs never observed",
                },
                warning5: {
                    title: "Combinatorial threshold exceeded",
                    hint: "Reduce N to 1–4 for live inference, stepwise prediction, and generation. Lower N also reduces sparsity.",
                },
                sections: {
                    transitions: "Transition Probabilities",
                    transitionsDescN1: "Full unigram/bigram matrix P(next | current)",
                    transitionsDescNPlus: "Slice P(next | context)",
                    conditionedOn: "Conditioned on:",
                    sparsity: "Data Sparsity",
                    trainingQuality: "Training Quality",
                    trainingQualityDesc: "Loss curve for the N={n} model during training",
                    nextToken: "Next-Token Prediction",
                    nextTokenDesc: "Type text and see the probability distribution over next characters",
                    stepwise: "Stepwise Prediction",
                    stepwiseDesc: "Trace the context window sliding character by character",
                    generation: "Text Generation",
                    generationDesc: "Generate text auto-regressively using the current N-gram model",
                },
                hero: {
                    title: "N-Gram Language Model",
                    description: "A character-level statistical language model with variable context size. Visualize how increasing the context window sharpens predictions at the cost of exponential sparsity.",
                    uniqueContexts: "Unique Contexts",
                    vocabulary: "Vocabulary",
                    contextSpace: "Context Space",
                    trainingTokens: "Training Tokens",
                    uniqueChars: "Unique characters",
                    totalTokensSeen: "Total tokens seen",
                },
                lossChart: {
                    title: "Training loss (NLL)",
                    final: "Final:",
                    ppl: "PPL:",
                    start: "Start",
                    progress: "Training progress",
                    end: "End",
                    perplexity: "Perplexity",
                    perplexityHint: "Lower = more confident predictions",
                    finalNll: "Final NLL",
                    finalNllHint: "Negative log-likelihood on train data",
                },
                footer: "LM-Lab · Scientific Instrument v1.0",
            },
            training: {
                title: "Training Insights",
                stats: {
                    totalTokens: "Total Tokens",
                    uniqueContexts: "Unique Contexts",
                    utilization: "Utilization",
                    sparsity: "Sparsity",
                    transitionDensity: "Transition Matrix Density",
                    subs: {
                        possiblePrefix: "of",
                        possibleSuffix: "possible",
                        fractionObserved: "fraction of possible contexts observed",
                        unseen: "of contexts never seen",
                    },
                },
            },
            historical: {
                title: "Historical Significance & Context",
                learnMore: "Learn More",
                description: "Description",
                limitations: "Key Limitations",
                evolution: "Evolution to Modern AI",
            },
            explosion: {
                title: "Context Too Large — Combinatorial Explosion",
                description: "As valid N increases, the number of possible contexts grows exponentially (|V|^N). For this vocabulary size, calculating the full transition matrix becomes computationally impractical and requires an enormous dataset to avoid sparsity.",
                complexity: "|V|^N = Space Complexity",
                limit: "Classical Limit Reached",
            },
            diagnostics: {
                vocabSize: "Vocabulary",
                contextSize: "Context Size (N)",
                contextSpace: "Context Space (|V|^N)",
                sparsity: "Sparsity",
                sub: {
                    observed: "{count} observed",
                    possible: "Possible Contexts",
                    utilized: "{percent}% utilized",
                },
            },
            educationalOverlay: {
                contextControlTitle: "Context Size Control",
                contextControlDescription: "Increasing N lets the model condition on more history - but the number of possible contexts grows as |V|^N. This exponential blowup is the central tension of n-gram models: more context means sharper predictions but also more data sparsity.",
                sliceVisualizationTitle: "Matrix Slice View",
                sliceVisualizationDescription: "For N > 1 the full transition tensor is too large to display. Instead, we fix the current context and show the resulting probability row - a slice through the high-dimensional table.",
                probabilityDistributionTitle: "Probability Distribution",
                probabilityDistributionDescription: "The model looks at the last N characters of your input, finds the matching context in its lookup table, and returns the probability distribution over all possible next characters.",
                generationPredictionTitle: "Generation & Prediction",
                generationPredictionDescription: "In educational mode we focus on understanding how a single next token is chosen. Switch to Free Lab to unlock the full stepwise tracer and text generation playground.",
                simplifiedSimulation: "Full stepwise prediction and generation available in Free Lab mode.",
            },
        },
        mlp: {
            title: "MLP + Embeddings",
            description: "Explore 108 trained MLP configurations. Watch embeddings emerge from noise, compare training dynamics across architectures, and generate text from learned character-level representations.",
            hero: {
                badge: "Research Lab",
            },
            freeLab: {
                title: "MLP Configuration Lab",
                description: "Select any trained configuration from the Model Zoo, inspect training curves, explore the embedding space, and compare models side-by-side.",
            },
            page: {
                switchToEducational: "Switch to Educational Mode for the full guided narrative",
            },
            narrative: {
                hero: {
                    eyebrow: "Chapter 4 · Neural Language Modeling",
                    titlePrefix: "Beyond Tables:",
                    titleHighlight: "MLP + Embeddings",
                    description: "You've built perceptrons, learned backpropagation, and seen why depth matters. Now the question changes: how do we apply that neural network machinery to actual language?",
                },
                sections: {
                    s00: { number: "00", label: "Building Blocks" },
                    s01: { number: "01", label: "Language Input" },
                    s02: { number: "02", label: "Scalability Wall" },
                    s03: { number: "03", label: "The Breakthrough" },
                    s04: { number: "04", label: "Empirical Exploration" },
                    s05: { number: "05", label: "Structural Limits" },
                    s06: { number: "06", label: "Training Stability" },
                    s07: { number: "07", label: "Looking Forward" },
                    s08: { number: "08", label: "Looking Forward" },
                },
                s00: {
                    heading: "From Building Blocks to Language",
                    lead: "You've already done the hard part. The Neural Networks chapter gave you perceptrons, backpropagation, and why depth unlocks complex decision boundaries. This chapter asks a new question: how do we feed actual language into that network?",
                    p1: "An MLP flows inputs through hidden layers, each applying a non-linear transformation, then produces a probability distribution via a softmax output. The architecture you learned carries over — only the input domain changes.",
                    figLabel1: "MLP Architecture · Schematic",
                    figHint1: "A feedforward network: input tokens flow through hidden layers to a softmax output distribution.",
                    p2: "In the XOR lesson, depth enabled curved decision boundaries that a flat network could never draw. For language, the same principle scales: stacked layers compose simple character patterns into higher-order structure — subwords, words, meanings.",
                    formulaCaption: "The same MLP formula from the Neural Networks chapter. The novelty here is what goes into x — the challenge of this chapter.",
                    calloutTitle: "The open question",
                    calloutText: "The network is ready. The challenge is representation: how do we convert a discrete symbol — the character 'a', the word 'cat' — into a number vector the network can process? That question defines the rest of this chapter.",
                    figLabel2: "Interactive · Non-Linearity & Decision Boundaries",
                    figHint2: "You saw XOR in the Neural Networks chapter. Toggle between models here to see how depth handles progressively more complex patterns.",
                },
                s01: {
                    heading: "Feeding Language to a Neural Network",
                    lead: "The simplest approach: take the previous N tokens, convert each to a number vector, concatenate them, and feed the result into the MLP to predict the next token.",
                    p1: "To feed characters into a neural network, we first need to turn them into numbers. The most straightforward method is a",
                    p1H1: "one-hot vector",
                    p1Mid: ": a list of zeros the length of the vocabulary, with a single 1 in the slot for that character. With a context window of size N, we concatenate N such vectors to form an input of dimension",
                    p1H2: "N × V",
                    p1End: ", then pass it through hidden layers to produce a probability distribution over the next token.",
                    formulaCaption: "The input to the MLP is a concatenation of N one-hot vectors, one per context token.",
                    calloutTitle: "What does loss mean here?",
                    calloutP1: "Loss measures surprise. After each prediction, the model compares its probability for every possible next character against what actually came next. High confidence on the right answer means low loss; near-zero confidence means high loss.",
                    calloutP2: "Training minimizes this average surprise over millions of characters. You know this mechanic from the Neural Networks chapter — here it operates on language tokens.",
                    figLabel1: "Interactive · Loss Intuition",
                    figHint1: "Drag the slider to set how confident the model is in the correct token. See how cross-entropy loss explodes as confidence approaches zero.",
                    p2: "This marks a step forward from N-gram tables. Instead of memorizing exact co-occurrence counts, the model",
                    p2H1: "learns a function",
                    p2End: "that maps context patterns to predictions — one that can generalize to novel combinations.",
                    p3: "The MLP can discover that certain character sequences behave similarly, even if it never saw the exact N-gram before. Hidden layers learn internal features that compress and abstract the raw input patterns.",
                    calloutTitle2: "Key improvement over N-grams",
                    calloutText2: "N-gram models assign zero probability to any context never observed in training. An MLP assigns non-zero probability to unseen contexts because it learns a smooth function — not a lookup table.",
                    figLabel2: "Interactive · MLP Forward Pass",
                    figHint2: "Type a short seed and step through each stage of the forward pass — from raw tokens to final probability distribution.",
                },
                s02: {
                    heading: "The Problem With One-Hot Inputs",
                    lead: "One-hot encoding seems natural, but it creates severe scalability problems that become catastrophic as vocabularies grow.",
                    p1H1: "Input dimensionality explosion.",
                    p1: "With a character-level vocabulary of ~96 tokens and a context of 8 characters, the input vector has 768 dimensions — manageable. But with a word-level vocabulary of 50,000 tokens and a context of 5 words, the input jumps to 250,000 dimensions. The first weight matrix alone would have tens of billions of parameters.",
                    p2H1: "Massive first-layer weight matrices.",
                    p2: "The matrix W₁ connecting the input to the first hidden layer has shape (N·V) × H, where H is the hidden size. For large vocabularies, this single matrix dominates the entire parameter budget, making training slow and memory-prohibitive.",
                    p3H1: "No notion of similarity.",
                    p3: "In one-hot space, every token is equidistant from every other token. The vectors for \"cat\" and \"kitten\" are just as far apart as \"cat\" and \"quantum\". The model must learn every relationship from scratch, with no structural prior that semantically related tokens should behave similarly.",
                    formulaCaption: "All one-hot vectors are equidistant — the model gets zero information about semantic similarity from the encoding itself.",
                    calloutTitle: "The scalability wall",
                    calloutText: "These three problems — dimensional explosion, huge weight matrices, and orthogonal representations — together form a scalability wall. The naive one-hot MLP simply cannot scale to real-world vocabularies. We need a fundamentally better way to represent tokens.",
                    figLabel1: "Interactive · N-gram Table vs MLP Parameters",
                    figHint1: "Drag the slider to increase context size N. Watch the n-gram table explode while MLP parameters grow modestly — and notice how most n-gram cells stay empty.",
                },
                s03: {
                    heading: "The Game Changer: Word Embeddings",
                    lead: "Instead of representing each token as a sparse one-hot vector, we learn a dense, low-dimensional vector for every token in the vocabulary. These are called embeddings.",
                    p1: "An embedding is a lookup table — a matrix E of shape V × D, where V is the vocabulary size and D is the embedding dimension (typically 10–300). To get the representation of token t, we select the t-th row of E. This is equivalent to multiplying E by the one-hot vector, but much more efficient.",
                    formulaCaption: "An embedding lookup: selecting row t from the embedding matrix E gives a dense D-dimensional vector.",
                    p2: "The key insight is that",
                    p2H1: "each dimension of the embedding captures a latent semantic property",
                    p2End: ". These dimensions are not hand-designed — they emerge automatically from training. One dimension might encode \"animate vs. inanimate,\" another might capture \"verb tense,\" and others encode more abstract patterns humans cannot easily name.",
                    p3: "Because embeddings are dense and continuous, similar tokens naturally cluster in embedding space. The model can leverage this structure to",
                    p3H1: "generalize across semantically related tokens",
                    p3End: ". If the model has learned something about \"cat,\" it can partially transfer that knowledge to \"kitten\" because their embedding vectors are close.",
                    p4: "This dramatically reduces the effective input dimensionality. Instead of N × V (potentially hundreds of thousands), the MLP now receives N × D (perhaps a few hundred) — orders of magnitude smaller, with richer information.",
                    pullQuote: "Embeddings transform tokens from isolated symbols into points in a continuous semantic space, where proximity encodes meaning. This single idea unlocked a new era of language modeling.",
                    figLabel1: "Illustrative · Embedding Space (Simplified)",
                    figHint1: "This is a pedagogical illustration — not real model data. Click tokens to explore how similar characters cluster together.",
                },
                s04: {
                    heading: "Exploring MLP + Embedding Configurations",
                    lead: "With embeddings in place, the MLP language model has several key hyperparameters that control its capacity, efficiency, and behavior. Understanding their impact requires systematic experimentation.",
                    p1: "The core architectural choices include the",
                    p1H1: "embedding dimension",
                    p1Mid1: "(how many latent features per token), the",
                    p1H2: "hidden layer size",
                    p1Mid2: "(how many neurons in each hidden layer), the",
                    p1H3: "number of hidden layers",
                    p1End: "(depth of the network), and the context window size (how many previous tokens the model sees).",
                    p2: "To understand how these choices affect model behavior, we trained many MLP language models with different hyperparameter configurations on the same dataset. This systematic sweep reveals key trade-offs: larger embeddings capture richer semantics but risk overfitting on small data, wider hidden layers increase capacity but slow training, and deeper networks can learn more abstract features but are harder to optimize.",
                    hyperparamCards: {
                        embDim: { title: "Embedding Dimension", desc: "Controls the richness of token representations. Larger values capture more semantic nuance but require more data to train effectively." },
                        hiddenSize: { title: "Hidden Layer Size", desc: "Determines the model's computational width. Wider layers can detect more patterns per layer but increase memory and compute." },
                        numLayers: { title: "Number of Layers", desc: "Controls representational depth. Deeper models can compose features hierarchically but face training stability challenges." },
                        contextWindow: { title: "Context Window", desc: "How many previous tokens the model considers. Larger windows give more information but increase input dimensionality linearly." },
                    },
                    figLabel1: "Interactive · Softmax Temperature",
                    figHint1: "Adjust temperature to see how it sharpens or flattens the probability distribution over next tokens. Low temperature = deterministic; high = creative.",
                    calloutTitle: "Why systematic exploration matters",
                    calloutText: "There is no single \"best\" configuration — the optimal hyperparameters depend on dataset size, vocabulary, and computational budget. The only way to develop intuition is to explore the space empirically and observe how each choice affects loss, perplexity, and generation quality.",
                    figLabel2: "Interactive · Hyperparameter Explorer",
                    figHint2: "Adjust sliders to explore how embedding dimension, hidden size, and learning rate affect validation loss, compute cost, training dynamics, and learned embeddings.",
                    p3: "The interactive explorer lets you compare models across these dimensions, visualizing validation loss, perplexity, training stability, compute cost, generation quality, and the learned embedding space. Anomaly badges flag concerning patterns like overfitting or unstable gradients.",
                },
                s05: {
                    heading: "New Limitations of MLP + Embeddings",
                    lead: "Embeddings solve the representation problem, but the MLP architecture itself introduces structural limitations that no amount of tuning can overcome.",
                    p1H1: "Fixed-size context window.",
                    p1: "An MLP must receive a fixed number of input tokens. It cannot dynamically attend to longer or shorter contexts — every prediction uses exactly N previous tokens, no more, no less. Information outside this window is completely invisible to the model. This is not a training failure — it is a hard architectural constraint.",
                    p2: "The consequence is stark for language: pronouns, references, and topic continuity all depend on context that may be many tokens back. Drag the slider below to see how a window of 3 tokens blinds the model to who \"she\" is — even though the answer is right there in the sentence.",
                    figLabel1: "Interactive · Context Window Blindness",
                    figHint1: "Drag the slider to grow the context window. Watch when 'Mary' (the referent) comes into view — and notice how small the window must be to hide it entirely.",
                    p3H1: "Long-range dependencies are out of reach.",
                    p3: "The problem compounds over longer texts. In real language, a pronoun may refer to a noun introduced dozens of tokens earlier. No practically-sized fixed window can reliably bridge these gaps — and even when it can, the signal is buried in N−1 other tokens competing for the network's attention.",
                    figLabel2: "Demo · Long-Range Dependency Failure",
                    figHint2: "A 19-word sentence where the pronoun 'she' refers to 'scientist' 15 tokens back. Compare how the MLP's prediction changes as the window grows, versus a model with full context.",
                    p4H1: "Position-dependent token meaning.",
                    p4: "Because the MLP concatenates embeddings end to end, the same token at position 1 and position 3 occupies different slices of the input vector — and therefore activates different columns of W₁. The model learns entirely separate weights for \"the at position 1\" versus \"the at position 3.\" There is no shared, position-invariant representation of what a token means.",
                    figLabel3: "Interactive · Position Sensitivity",
                    figHint3: "Toggle 'the' between position 1 and position 3. The highlighted columns in W₁ show which parameters each instance activates — completely different sets.",
                    p5H1: "Parameter explosion and signal dilution.",
                    p5: "Even with embeddings, the first weight matrix W₁ has shape (N · D) × H. Doubling the context window doubles the size of this layer. For long contexts this becomes the dominant cost. At the same time, as N grows each token's embedding shrinks to a smaller fraction of the total input — from 100% at N=1 to just 6% at N=16 — diluting every signal without adding any mechanism to focus on the most informative tokens.",
                    figLabel4: "Interactive · Concatenation Bottleneck",
                    figHint4: "Switch between Parameter Growth and Signal Dilution views. Drag the context size slider and watch W₁ expand — and each token's share of the input shrink.",
                    calloutTitle: "The same root cause",
                    calloutText: "All four limitations share a common origin: the MLP treats its entire context as a single flat vector. It has no mechanism to reason about the structure, ordering, or relative importance of individual tokens. Overcoming this requires architectures that process sequences step by step, carrying forward a memory of what came before.",
                },
                s06: {
                    heading: "Deep Training Challenges for Large MLPs",
                    lead: "Making deep MLPs actually train well was one of the hardest practical problems in the history of neural networks. Without careful techniques, deep networks simply fail to learn.",
                    panels: {
                        initialization: {
                            title: "Weight Initialization",
                            preview: "Too large and activations explode; too small and gradients vanish.",
                        },
                        gradients: {
                            title: "Vanishing & Exploding Gradients",
                            preview: "Gradients multiply through layers — if each factor is < 1 or > 1, they vanish or explode.",
                        },
                        batchnorm: {
                            title: "Batch Normalization",
                            preview: "Normalize activations to keep internal distributions stable during training.",
                        },
                    },
                    p1H1: "Weight initialization.",
                    p1: "If weights are initialized too large, activations explode through the layers. Too small, and gradients vanish before reaching the early layers. Proper initialization schemes (like Xavier or Kaiming) set the initial scale based on layer dimensions to maintain stable signal propagation.",
                    formulaCaption1: "Kaiming initialization: weights are drawn from a Gaussian scaled by the fan-in, keeping variance stable through ReLU layers.",
                    figLabel1: "Interactive · Initialization Sensitivity",
                    figHint1: "Compare loss curves under different initialization scales. Well-scaled initialization is critical for convergence.",
                    p2H1: "Vanishing and exploding gradients.",
                    p2: "During backpropagation, gradients are multiplied through each layer. In a deep network, if these multipliers are consistently less than 1, gradients shrink exponentially (vanishing). If greater than 1, they grow exponentially (exploding). Either way, the network fails to learn effectively.",
                    formulaCaption2: "The chain rule through L layers: gradients are products of per-layer Jacobians. If each factor is slightly < 1 or > 1, the product vanishes or explodes.",
                    figLabel2: "Interactive · Gradient Flow Across Layers",
                    figHint2: "Toggle between vanishing, stable, and exploding gradient regimes to see how gradient magnitude changes per layer.",
                    p3: "For many years, training networks deeper than 2–3 layers was extremely unreliable. The combination of activation function choice (ReLU replaced Sigmoid/Tanh for hidden layers), proper initialization, and normalization techniques was essential for stable training.",
                    p4H1: "Batch Normalization",
                    p4: "was a key breakthrough. By normalizing activations within each layer to have zero mean and unit variance (across a mini-batch), it keeps internal distributions stable as the network trains. This dramatically reduces sensitivity to initialization and learning rate, enabling reliable training of much deeper networks.",
                    formulaCaption3: "Batch Normalization: normalize activations h using batch statistics (μ_B, σ²_B), then rescale with learned parameters γ and β.",
                    calloutTitle: "Why BatchNorm changed everything",
                    calloutText: "Before BatchNorm, training deep networks required meticulous hyperparameter tuning. After it, practitioners could reliably train 10, 20, or even 100+ layer networks. It acts as a stabilizer that smooths the loss landscape, allowing gradient descent to converge faster and more reliably.",
                    figLabel3: "Interactive · Batch Normalization Effect",
                    figHint3: "Toggle BatchNorm on and off to see how it stabilizes activation distributions across layers.",
                },
                s07: {
                    heading: "The Path Ahead",
                    lead: "The MLP showed that neural networks can learn language. Its fixed window revealed something equally important: learning language well requires understanding sequences — not just snapshots.",
                    p1: "The structural limits you just explored share a single root: the MLP sees its context as one flat vector. No step-by-step processing. No memory of what came before. No way to track how meaning accumulates across tokens.",
                    p2: "That constraint leads naturally to a new set of questions:",
                    rnnQ1: "Fixed window — what if the network processed one token at a time instead?",
                    rnnQ2: "No memory — what if each step passed a hidden state forward to the next?",
                    rnnQ3: "Independent predictions — what if every output informed the next input?",
                    pullQuote: "The MLP looks at a window. What we need is a model that takes a journey — one token at a time, building a running memory of everything it has seen.",
                    p3: "That is the Recurrent Neural Network. Instead of reading a fixed-size snapshot, an RNN reads one token, updates a hidden state, reads the next, updates again — carrying the thread of the sequence forward at each step.",
                    p4: "Despite these limits, the MLP + Embeddings framework established concepts every modern language model still uses: learned token representations, non-linear feature hierarchies, and end-to-end gradient-based training. The embedding layers and feedforward blocks inside Transformers are direct descendants of the ideas in this chapter.",
                },
                s08: {
                    heading: "Final Limitations and the Path Ahead",
                    lead: "Even with embeddings, deep architectures, and modern training techniques, the MLP fundamentally operates on fixed-size windows — and this ceiling defines its era in the history of language modeling.",
                    p1: "The MLP processes each context window independently. It has no memory of what came before the window, no mechanism to dynamically attend to distant tokens, and no way to handle variable-length sequences without padding or truncation. Every prediction is made from the same fixed-size snapshot.",
                    p2: "This means MLPs cannot model long-range dependencies — the kind of structure that makes natural language coherent across sentences and paragraphs. A pronoun referring to a noun 50 tokens ago is simply out of reach for an MLP with a context of 8.",
                    p3: "These structural limitations motivated a series of architectural innovations that define the modern trajectory of language modeling:",
                    p3H1: "Recurrent Neural Networks (RNNs)",
                    p3Mid1: "introduced sequential memory, processing one token at a time while maintaining a hidden state.",
                    p3H2: "Convolutional architectures (like WaveNet)",
                    p3Mid2: "applied dilated convolutions to capture hierarchical patterns over sequences. And ultimately,",
                    p3H3: "Transformers",
                    p3End: "introduced self-attention — a mechanism that allows every token to directly attend to every other token, regardless of distance.",
                    pullQuote: "The MLP was the first architecture to show that neural networks could learn language — but its fixed window revealed that learning language requires architectures that understand sequences, not just snapshots.",
                    p4: "Despite these limitations, the MLP + Embeddings framework established concepts that remain foundational in every modern language model: learned token representations, non-linear feature hierarchies, and end-to-end gradient-based training. Every Transformer still uses embedding layers and feedforward MLP blocks — the ideas introduced here never went away; they evolved.",
                },
                cta: {
                    heading: "Continue Exploring",
                    freeLabTitle: "Open Free Lab",
                    freeLabDesc: "Experiment with MLP + Embedding models interactively. Train, visualize embeddings, and generate text with different hyperparameters.",
                    transformerTitle: "Next: Recurrent Neural Networks",
                    transformerDesc: "Discover how RNNs overcome the fixed-window limitation by processing sequences one token at a time, carrying a hidden state that acts as memory.",
                },
                footer: {
                    text: "From counting tables to learned representations — the MLP + Embeddings model marked the moment language modeling became truly neural.",
                    brand: "LM-Lab · MLP + Embeddings Narrative",
                },
                oneHot: {
                    title: "One-Hot Encoding",
                    sparse: "Sparse, high-dimensional. Every token is equally distant from every other. No notion of similarity.",
                    learnedTitle: "Learned Embeddings",
                    dense: "Dense, low-dimensional. Similar words (\"cat\" and \"mat\") get similar vectors — the model can generalize.",
                },
                mlpDiagram: {
                    input: "Input",
                    inputDesc: "Context tokens (one-hot or embeddings)",
                    hidden1: "Hidden 1",
                    hidden1Desc: "Learned features",
                    hidden2: "Hidden 2",
                    hidden2Desc: "Higher-order patterns",
                    output: "Output",
                    outputDesc: "Next-token probabilities",
                },
                thinkFirst: {
                    xor: {
                        question: "A linear model can only draw straight lines. How many hidden neurons do you think it takes to separate 4 XOR-like clusters?",
                        reveal: "Just 2 neurons in one hidden layer can solve XOR — try it below!",
                    },
                    embedding: {
                        question: "If you could represent each character as just 3 numbers instead of 96, what properties would you want those numbers to capture?",
                        reveal: "Embeddings learn exactly this — dense vectors where similar characters get similar numbers, automatically.",
                    },
                    hyperparams: {
                        question: "Which do you think matters more for prediction quality: a larger embedding or a wider hidden layer?",
                        reveal: "It depends on the data! Use the explorer below to find out empirically.",
                    },
                    contextWindow: {
                        question: "The model sees 3 tokens of context. 'Mary walked into the garden, and she...' — Can the model figure out who 'she' is?",
                        reveal: "No — 'Mary' is 8 tokens back, far outside a 3-token window.",
                    },
                },
                guidedExperiments: {
                    title: "Guided Experiments — Try These",
                    bestConfig: {
                        title: "Find the Best Config",
                        tryThis: "Adjust sliders to minimize validation loss. Which hyperparameter has the biggest impact?",
                        observe: "Look at the loss curve shape — does it plateau early or keep improving?",
                    },
                    overfitting: {
                        title: "Spot Overfitting",
                        tryThis: "Find a config where train loss is much lower than val loss.",
                        observe: "The anomaly badge lights up and the gap metric shows the divergence.",
                    },
                    embeddings: {
                        title: "Watch Embeddings Learn",
                        tryThis: "Select a config and scrub the training snapshot slider in the Embedding Space tab.",
                        observe: "Tokens move from random noise to structured clusters as training progresses.",
                    },
                    generation: {
                        title: "Generate and Compare",
                        tryThis: "Generate text from the best and worst configs (lowest vs highest loss).",
                        observe: "Notice how prediction quality correlates directly with validation loss.",
                    },
                },
            },
            explorer: {
                loading: "Loading configurations…",
                errorPrefix: "Failed to load MLP grid:",
                noConfigs: "No MLP configurations available from backend.",
                onboarding: {
                    title: "Quick Tour",
                    scatter: {
                        text: "Each dot is a trained model. Click any dot to select it and see its full metrics below. Models closer to the bottom-left corner have lower validation loss (better performance).",
                    },
                    sliders: {
                        text: "Use these sliders to explore different hyperparameter combinations. The explorer will automatically select the closest trained model to your chosen settings.",
                    },
                    metrics: {
                        text: "Watch these metric cards update as you select different models. Look for the anomaly badges — they flag models with training issues like overfitting or gradient instability.",
                    },
                    next: "Next",
                    gotIt: "Got it!",
                },
                sections: {
                    s01Title: "Model Zoo Overview",
                    s01Subtitle: "fully-trained configurations — click any dot to select it and sync the sliders below.",
                    s02Title: "Selected Configuration",
                    s02Subtitle: "Metric cards, anomaly flags, and a plain-English summary of this model's training quality.",
                    s03Title: "Embedding Space",
                    s03Subtitle: "Vocabulary tokens projected to 2D via PCA. Scrub through training snapshots to watch structure emerge from noise.",
                    s04Title: "Text Generation",
                    s04Subtitle: "Generate character sequences from the selected model. Adjust temperature and length to explore the output distribution.",
                    s05Title: "Advanced Training Diagnostics",
                    s05Subtitle: "Gradient flow, neuron health, and overfitting patterns across the full training run.",
                },
                zoo: {
                    expandableTitle: "Model Zoo · {count} Configurations",
                    description: "Start here. Each dot is one fully-trained model. Click any dot to select it and sync the sliders below. Use filters to find the strongest configs, the worst performers, or outliers worth investigating.",
                },
                sliders: {
                    embeddingDim: "Embedding Dim",
                    hiddenSize: "Hidden Size",
                    learningRate: "Learning Rate",
                },
                config: {
                    active: "Active:",
                    score: "score",
                },
                metrics: {
                    valLoss: "Val Loss",
                    trainLoss: "Train Loss",
                    loss: "Loss",
                    trainSmoothed: "Train (smoothed)",
                    perplexity: "Perplexity",
                    random: "random:",
                    trainValGap: "Train–Val Gap",
                    params: "Params",
                    compute: "Compute",
                    tooltips: {
                        valLoss: "Validation loss: how well the model predicts held-out data it never saw during training. Lower = better. This is the primary metric — unlike training loss, it cannot be gamed by memorization.",
                        trainLossOnly: "Training loss only — validation loss unavailable for this config. Training loss can be misleadingly optimistic because the model has seen these examples.",
                        trainSmoothed: "Mean of the last ~10% of logged training loss values. Smoothing removes per-batch noise, giving a stable estimate of where training loss settled.",
                        perplexity: "Perplexity ≈ exp(loss). Intuition: if perplexity = 20, the model is as uncertain as randomly choosing among 20 tokens. Lower = better. A random model has perplexity equal to the vocabulary size.",
                        randomPerplexity: "Perplexity a uniform random model would achieve. Any useful model should be well below this.",
                        trainValGap: "Train–Val Gap = val_loss − smoothed_train_loss. Positive = overfitting (model fits training data better than new data). Negative = healthy or underfitting. Values > 0.3 are a concern. Uses smoothed train loss for stability.",
                        paramsCount: "Total number of learnable weights and biases in this model configuration.",
                        compute: "Compute = parameters × training steps ({steps}k). This is a deterministic proxy for compute cost — machine-independent, unlike wall-clock time. Bar shows cost relative to the most expensive config in this grid.",
                        computeDetail: "{params} parameters × {steps}k steps. Larger models cost more to train but don't always generalise better.",
                        score: "Composite quality score — higher is better. Computed as how much this config improved over the random baseline, relative to the best config in the grid.",
                    },
                },
                anomalies: {
                    aboveRandom: "≥ Random",
                    overfitting: "Overfitting",
                    valLossUp: "Val Loss ↑",
                    noConvergence: "No Convergence",
                    unstableGrad: "Unstable ∇",
                    pplMismatch: "PPL ≠ exp(L)",
                    tooltips: {
                        aboveRandom: "Final loss is at or above the random baseline — the model may not have learned meaningful patterns from the data.",
                        overfitting: "The train–val gap exceeds 0.3 — the model memorizes training data better than it generalizes. Consider: smaller model, more data, or regularization.",
                        valLossUp: "Validation loss was still increasing at end of training — a sign of overfitting onset. The model may have trained too long.",
                        noConvergence: "Loss did not meaningfully decrease over training — the learning rate may be too high or too low for this architecture.",
                        unstableGrad: "Gradient norms varied by >1000× during training — indicates optimization instability, often caused by too-high learning rate.",
                        pplMismatch: "Reported perplexity does not match exp(loss). This may indicate a data pipeline issue or stale cached metrics.",
                    },
                },
                summaries: {
                    aboveRandom: "This model barely outperforms random guessing. It likely failed to learn meaningful patterns — check the learning rate and architecture.",
                    nonDecreasing: "Loss did not decrease during training. The model failed to converge — the learning rate may be too high or too low for this architecture.",
                    overfitting: "This model overfits — it memorizes training data better than it generalizes. The gap between training and validation loss is large.",
                    lossIncreasing: "Validation loss was still rising at the end of training — a sign of late-stage overfitting. The model trained too long for its capacity.",
                    unstableGradients: "Gradient norms varied wildly during training — the optimization was unstable. This often means the learning rate is too high.",
                    stillImproving: "Training was still improving at the final step. Given more compute, this model might converge further.",
                    balanced: "This model trains stably and generalizes well. A strong, balanced configuration.",
                    converged: "Training completed. The model converged with a moderate generalization gap.",
                },
                computeLabels: {
                    minimal: "Minimal",
                    low: "Low",
                    moderate: "Moderate",
                    high: "High",
                    veryHigh: "Very High",
                },
                timeline: {
                    title: "Training Timeline",
                    noData: "No timeline data available.",
                    pts: "pts",
                    every: "every",
                    steps: "steps",
                    total: "total",
                    nonUniform: "non-uniform",
                    trend: "Trend:",
                    variance: "Variance:",
                    converged: "Converged ~",
                    tooltips: {
                        chart: "Loss curves over training steps. Green = validation loss (on held-out data, primary). Purple = training loss. Dashed red = random baseline — a model guessing uniformly. Convergence means both curves flatten.",
                        pts: "Number of metric checkpoints logged during training. More points = smoother, more informative curves.",
                        interval: "A metric snapshot was saved every {interval} gradient update steps. Finer logging reveals more training dynamics but costs slightly more disk space.",
                        totalSteps: "Total gradient updates performed. All configs in this grid train for exactly {steps}k steps for fair comparison.",
                        trend: "Direction of validation loss in the second half of training. Decreasing = still learning. Flat = converged. Increasing = overfitting onset.",
                        variance: "Statistical variance of validation loss over the full training run. Near-zero = stable training. High variance = noisy or unstable optimization.",
                        convergenceStep: "Training step where validation loss first fell below 50% of its initial value. Earlier = faster learning.",
                    },
                    chart: {
                        randomBaseline: "random baseline",
                        train: "train",
                        valPrimary: "val (primary)",
                        trainingSteps: "Training Steps",
                    },
                },
                embeddingSpace: {
                    title: "PCA 2D · Training Drift",
                    tooltip: "Each point is one vocabulary token, projected from the learned embedding space to 2D via PCA. Use the snapshot slider to watch embeddings evolve from random noise to structured clusters during training.",
                },
                generation: {
                    title: "Generated Sample",
                    seedPlaceholder: "Seed text…",
                    generateButton: "Generate",
                    temp: "Temp",
                    tokens: "Tokens",
                    tempTooltip: "Temperature controls randomness. Low (0.1) = deterministic, always picks the most likely next character. High (2.0) = creative but chaotic, samples from a flatter distribution.",
                    tokensTooltip: "Maximum number of characters to generate. The model generates one character at a time; more tokens = longer output but slower.",
                    estPpl: "est. PPL ≈",
                    chars: "chars",
                    pressGenerate: "Press Generate to produce text from the selected model.",
                    pplTooltip: "Perplexity = exp(loss). This is the model's estimated perplexity on the training distribution — lower means more confident and fluent predictions.",
                },
                diagnostics: {
                    intro: "These diagnostics reveal internal training dynamics: gradient flow, neuron usage, and overfitting patterns. They help advanced users understand how the model learns beyond simple loss curves.",
                    gradNormLabel: "Gradient Norm over Steps",
                    deadNeuronLabel: "Dead Neuron Ratio over Steps",
                    gradNormSection: "ⓘ Gradient Norms per Layer",
                    activationSection: "ⓘ Activation Health over Training",
                    genGapSection: "ⓘ Generalization Gap Heatmap · All Configs",
                    tooltips: {
                        gradNorm: "Gradient norm is the overall magnitude of weight updates at each training step. Stable, moderate values indicate healthy optimization. Sudden spikes suggest instability; values collapsing to zero indicate vanishing gradients.",
                        deadNeuron: "Fraction of neurons that never activate during training. A 'dead' neuron always outputs zero, contributing nothing to learning. High values (> 20%) indicate wasted model capacity — often caused by large learning rates or poor initialization.",
                        gradNormLayer: "Shows gradient magnitudes per parameter group (C=embedding, W1/b1=hidden layer, W2/b2=output layer) across training snapshots. Balanced magnitudes across layers suggest stable, well-conditioned learning. Red cells indicate large gradients that may destabilize training.",
                        activationHealth: "Displays saturation and dead neuron statistics over training time. Saturation (left bar) means neurons are stuck near the tanh activation limits (±1) and contribute near-zero gradient. Dead fraction (right, pink) are neurons that never activate. Both indicate underutilized model capacity.",
                        genGap: "Difference between training loss and validation loss (val − train) across all configurations, averaged over learning rates. Green = model generalizes well to unseen data. Red = overfitting — the model memorizes training data but fails on new examples. Use this to identify which architecture choices cause overfitting.",
                    },
                },
                dataSource: "Real data from {count} trained configs · {steps}k steps each · logged every {interval} steps.",
                primaryValLoss: "Primary: validation loss.",
                primaryTrainLoss: "Primary: training loss (val unavailable).",
            },
            compareMode: {
                needMore: "Need at least 2 configurations to compare.",
                needAtLeastTwoConfigs: "Need at least 2 configurations to compare.",
                description: "Select two configurations to compare side-by-side. The seed text from the main generator is synced to both.",
                selectTwoConfigsToCompare: "Select two configurations to compare side-by-side. The seed text from the main generator is synced to both.",
                configLabel: "Config {label}",
                config: "Config",
                configA: "Config A",
                configB: "Config B",
                title: "Side-by-Side Comparison",
                seed: "Seed:",
                editSeedHint: "(edit in main generator above)",
                editInMainGeneratorAbove: "(edit in main generator above)",
                selectAConfig: "Select a config…",
                selectConfigA: "Select Config A",
                selectConfigB: "Select Config B above",
                diffTitle: "B vs A — differences",
                diffSummary: "B vs A — differences",
                noTimelineData: "No timeline data.",
                steps: "Steps",
                train: "train",
                val: "val",
                trainingLoss: "Training Loss",
                embeddingSpace: "Embedding Space",
                generatedText: "Generated Text",
                temperature: "T",
                generate: "Generate",
                generating: "Generating…",
                seedTextAboveWillBeUsed: "Seed text above will be used.",
                metrics: {
                    valLoss: "Val Loss",
                    perplexity: "Perplexity",
                    gap: "Gap",
                    genGap: "Gen Gap",
                    score: "Score",
                    params: "Params",
                },
                panel: {
                    trainingLoss: "Training Loss",
                    embeddingSpace: "Embedding Space",
                    generatedText: "Generated Text",
                    seedUsed: "Seed text above will be used.",
                    noTimeline: "No timeline data.",
                    tempLabel: "T=",
                    generateButton: "Generate",
                },
            },
            scatterPlot: {
                description: "Each dot is one trained model. X = parameter count (cost), Y = final validation loss (lower is better). Color = embedding dimension. The dashed line is the",
                paretoFrontier: "Pareto frontier",
                paretoDesc: "— best loss for each compute level.",
                highlighted: "highlighted",
                filters: {
                    all: "All",
                    allTip: "Show all configurations",
                    best: "Best ★",
                    bestTip: "Top 25% by composite score",
                    worst: "Worst",
                    worstTip: "Bottom 25% by score — highest loss, lowest quality",
                    anomalies: "Anomalies",
                    anomaliesTip: "Configs with generalization gap > 0.3 or score < 0.2",
                },
                legend: {
                    paretoLine: "Pareto frontier",
                },
                footer: "{count} configurations · Click any dot to select · Lower-right = more compute, less payoff",
                axisX: "Parameters",
                axisY: "Val Loss",
            },
            embeddingDrift: {
                snapshotLabel: "Training snapshot",
                trainingSnapshot: "Training snapshot",
                stepLabel: "Step",
                step: "Step",
                loading: "Loading embeddings…",
                phaseText: {
                    p0: "Random initialization — embeddings have no structure yet.",
                    p1: "Early training — clusters are beginning to form.",
                    p2: "Early-mid training — character categories becoming distinct.",
                    p3: "Mid training — embedding space shows clear structure. Similar tokens cluster together.",
                    p4: "Late training — structure consolidating, noise reducing.",
                    p5: "Final checkpoint — fully trained embeddings. This is what the model uses for prediction.",
                },
                phases: {
                    "0": "Random initialization — embeddings have no structure yet.",
                    "1": "Early training — clusters are beginning to form.",
                    "2": "Early-mid training — character categories becoming distinct.",
                    "3": "Mid training — embedding space shows clear structure. Similar tokens cluster together.",
                    "4": "Late training — structure consolidating, noise reducing.",
                    "5": "Final checkpoint — fully trained embeddings. This is what the model uses for prediction.",
                },
                snapshotUnavailable: "Snapshot unavailable — showing nearest available checkpoint.",
            },
            embeddingViz: {
                loading: "Loading embeddings…",
                waiting: "Waiting for embedding data…",
                dim1: "Dimension 1 (PCA)",
                dim2: "Dimension 2 (PCA)",
                tokens: "tokens",
                clickInfo: "Click any token to highlight its nearest neighbors. Similar tokens cluster together in the learned embedding space.",
                clickToHighlight: "Click any token to highlight its nearest neighbors. Similar tokens cluster together in the learned embedding space.",
                deselectInfo: "Click another token or click \"{token}\" again to deselect. Dashed lines connect to the 4 nearest neighbors in embedding space.",
                clickToDeselect: "Click another token or click \"{token}\" again to deselect. Dashed lines connect to the 4 nearest neighbors in embedding space.",
                categories: {
                    vowel: "Vowels",
                    vowels: "Vowels",
                    consonant: "Consonants",
                    consonants: "Consonants",
                    digit: "Digits",
                    digits: "Digits",
                    punctuation: "Punctuation",
                    whitespace: "Space / Special",
                    spaceSpecial: "Space / Special",
                },
            },
            nearestNeighbors: {
                title: "Nearest Neighbors (Cosine Similarity)",
                loading: "Loading neighbor data…",
                neighborsOf: "Neighbors of",
                noNeighborData: "No neighbor data for this token.",
                noData: "No neighbor data for this token.",
                selectPrompt: "Select a token above to see its nearest neighbors by cosine similarity in embedding space.",
            },
            snapshotDiagnostics: {
                noSnapshotData: "No snapshot data available.",
                noGradData: "No gradient norm data in snapshots.",
                noSatData: "No activation saturation data in snapshots.",
                gradLegend: "Green = small gradients · Yellow/Red = large gradients · Consistent magnitudes across layers indicate healthy training.",
                saturatedLeft: "Saturated activations (left)",
                deadRight: "Dead neurons (right)",
                satLegend: "High saturation means many neurons are pinned at tanh extremes (±1). Dead neurons never activate. Both waste capacity.",
                satNote: "High saturation means many neurons are pinned at tanh extremes (±1). Dead neurons never activate. Both waste capacity.",
                stepHeader: "Step",
                step: "Step",
            },
            genGapHeatmap: {
                header: "hidden ↓ / emb →",
                axisLabel: "hidden ↓ / emb →",
                gapLabel: "Gap:",
                legend: {
                    healthy: "< 0 (healthy)",
                    low: "0–0.1",
                    mid: "0.1–0.2",
                    medium: "0.1–0.2",
                    high: "0.2–0.3",
                    overfit: "> 0.3 (overfit)",
                },
                note: "Each cell averages the train–val gap across all learning rates for that (emb_dim, hidden_size) pair. Red = overfitting. Green = healthy generalization.",
                description: "Each cell averages the train–val gap across all learning rates for that (emb_dim, hidden_size) pair. Red = overfitting. Green = healthy generalization.",
                configs: "configs (across LRs)",
                avgGap: "avg gap=",
                bestLoss: "best loss=",
            },
        },
        neuralNetworks: {
            title: "Neural Networks & Deep Learning",
            description: "A first-principles exploration of artificial neural networks — from the perceptron to backpropagation. Understand how learned parameters replace counting and why dense representations generalize where N-grams fail.",
            hero: {
                badge: "Neural Computation",
            },
            freeLab: {
                title: "Neural Network Playground",
                description: "Experiment freely with perceptrons, activation functions, weight updates, and training dynamics.",
            },
            guidedExperiments: {
                title: "Guided Experiments",
                subtitle: "Five quick exercises to build intuition",
                handVsTraining: {
                    title: "Build Your First Neuron — By Hand, Then By Training",
                    doThis: "Set x₁=1, x₂=0.5. Manually adjust w₁, w₂, b until output ≈ 0.8. Then reset and train with target=0.8.",
                    observeThis: "Training finds similar values automatically. Gradient descent replaces manual guessing.",
                },
                activationComparison: {
                    title: "Compare Activation Functions",
                    doThis: "Switch between Linear, ReLU, Sigmoid, and Tanh. Set x₁=−2, x₂=2, w₁=1, w₂=1, b=0.",
                    observeThis: "Linear passes negatives unchanged. ReLU zeros them. Sigmoid and Tanh squash to bounded ranges.",
                },
                learningRateExtremes: {
                    title: "Break Training with Extreme Learning Rates",
                    doThis: "Switch the model to Linear. Set target=0.8. Train with η=0.05 (slow), then η=1.0 (normal), then η=2.0 (aggressive).",
                    observeThis: "Low η converges slowly. Higher η can overshoot and oscillate. The sweet spot depends on the model and data.",
                },
                convergenceBehavior: {
                    title: "Watch Loss Converge Over Many Steps",
                    doThis: "Set target=0.9, η=1.0. Click Auto-Train ×10 repeatedly and watch the loss chart.",
                    observeThis: "Loss drops fast at first, then plateaus. Early steps matter most. Diminishing returns set in quickly.",
                },
                randomInitialization: {
                    title: "See Why Random Starts Matter",
                    doThis: "Train to target=0.5 with default params. Reset. Change w₁ to 2.0, train again. Compare final loss.",
                    observeThis: "Different starting points lead to different solutions. Neural nets are non-convex; initialization matters.",
                },
            },
            sections: {
                artificialNeuron: { number: "02", label: "The Perceptron" },
                nonLinearity: { number: "03", label: "Scaling Up" },
                findingDirection: { number: "04", label: "Correction" },
                makingItLearn: { number: "05", label: "Training" },
                trainingAtScale: { number: "06", label: "Scale" },
                overfittingTrap: { number: "07", label: "Overfitting" },
                fromNumbers: { number: "08", label: "Language" },
                playground: {
                    inputs: {
                        title: "Inputs",
                        desc: "Feature values fed into the perceptron. Each input is multiplied by its corresponding weight before being summed.",
                        x1: "First input feature value (x₁). Multiplied by weight w₁ before entering the sum node.",
                        x2: "Second input feature value (x₂). Multiplied by weight w₂ before entering the sum node.",
                    },
                    weights: {
                        title: "Parameters",
                        desc: "Learnable parameters that scale each input. The bias shifts the activation threshold independently of the inputs.",
                        w1: "Weight for input x₁. Controls how strongly x₁ influences the output. Updated by gradient descent during training.",
                        w2: "Weight for input x₂. Controls how strongly x₂ influences the output. Updated by gradient descent during training.",
                        bias: "Bias term (b). Shifts the weighted sum, allowing the neuron to activate even when all inputs are zero.",
                    },
                    activation: {
                        title: "Activation Function",
                        explorerTitle: "Interactive · Activation Functions",
                        ariaLabel: "{name} activation function graph",
                        inputLabel: "Weighted sum z (input to activation)",
                        caption: "Toggle between activation functions and drag the z slider to see how each transforms the weighted sum.",
                        desc: "Non-linear transformation applied after the weighted sum. Without it, stacking layers would collapse into a single linear function.",
                        linear: "No transformation — output equals the weighted sum z. Useful as a baseline but cannot model non-linear patterns.",
                        relu: "Rectified Linear Unit. Outputs max(0, z). Sparse, efficient, and widely used in deep networks.",
                        sigmoid: "Squashes output to (0, 1). Useful for binary probability outputs, but can cause vanishing gradients.",
                        tanh: "Squashes output to (−1, 1). Zero-centered, often preferred over sigmoid for hidden layers.",
                        labels: {
                            relu: "ReLU",
                            sigmoid: "Sigmoid",
                            tanh: "Tanh",
                        }
                    },
                    training: {
                        title: "Training",
                        desc: "Adjust the target and learning rate, then step through gradient descent to minimize the loss.",
                        target: "The desired output value (y). The model tries to minimize the squared difference between its prediction and this target.",
                        learningRate: "Learning rate (η). Controls the step size during gradient descent. Too high causes instability; too low slows convergence.",
                        step: "Run one gradient descent step: compute gradients and update w₁, w₂, and b by −η × gradient.",
                        auto: "Run 10 gradient descent steps in sequence to observe how parameters and loss evolve over multiple iterations.",
                        reset: "Reset all parameters and training history to their initial values.",
                        random: "Randomize weights and bias to explore a different region of the loss landscape.",
                        steps: "Total number of gradient descent steps taken so far in this training session.",
                        stepIndex: "Step number in the training history log.",
                        noData: "No training data yet",
                        noDataHint: "Click \"Train 1 Step\" or \"Auto-Train ×10\" to begin",
                        insightsTitle: "Training Insights",
                        runInference: "Run inference to view training data",
                        stats: {
                            finalLoss: { label: "Final Loss", desc: "The error level at the end of training. Lower is better." },
                            steps: { label: "Steps", desc: "How many times the model updated its parameters during training." },
                            batchSize: { label: "Batch Size", desc: "Number of examples processed per gradient update step." },
                            learningRate: { label: "Learning Rate", desc: "Step size for gradient descent. Too high causes instability; too low slows convergence." },
                            parameters: { label: "Parameters", desc: "Total number of learnable weights in the model." },
                        },
                    },
                    visualization: {
                        sum: "Weighted sum node (Σ). Computes z = w₁x₁ + w₂x₂ + b before the activation function is applied.",
                        output: "Final prediction ŷ = activation(z). This is the value the network outputs after applying the non-linearity.",
                        loss: "Mean squared error loss: L = (ŷ − target)². Measures how far the prediction is from the desired target.",
                        activationNode: "Activation function node. Applies the selected non-linearity to the weighted sum z.",
                        activationCurve: "The activation function curve. The dot shows the current input z and its corresponding output f(z).",
                        equation: "Full forward-pass equation: multiply each input by its weight, add the bias, then apply the activation function.",
                        lossCurve: "Loss over training steps. A descending curve indicates the model is learning — parameters are converging toward the target.",
                        lossCurveLabel: "Loss over training steps",
                        lossTooltipTitle: "What is Loss?",
                        lossTooltipErrorLabel: "Prediction Error",
                        lossTooltipError: "Loss measures how \"surprised\" the model is. A high loss means it's guessing wrong frequently.",
                        lossTooltipBenchmarkLabel: "The Benchmark",
                        lossTooltipBenchmark: "Pure random guessing gives a loss of ~4.56 (−ln(1/96)). Anything lower means the model has actually learned something.",
                        lossTooltipCaption: "The descending curve shows the model slowly discovering patterns in your text.",
                    },
                    tabs: {
                        perceptron: "Visualize the single-neuron forward pass: inputs are scaled by weights, summed with a bias, then passed through an activation.",
                        activation: "Explore how the chosen activation function transforms the weighted sum z into the final prediction ŷ.",
                        gradients: "Inspect the chain-rule gradient flow and see exactly how each parameter will be updated in the next training step.",
                        training: "Track loss and parameter evolution across training steps to observe gradient descent in action.",
                    },
                    gradients: {
                        visualizerTitle: "Interactive · Backpropagation Step-by-Step",
                        forwardPass: "Forward pass: compute z, apply activation, and calculate the loss from the current prediction and target.",
                        forwardPassLabel: "Forward Pass",
                        chainRule: "Backpropagation via the chain rule: decompose ∂L/∂w into a product of local gradients through each node.",
                        chainRuleLabel: "Gradients (Chain Rule)",
                        weightUpdate: "Proposed parameter update: new value = old value − η × gradient. Applied when you click Train 1 Step.",
                        weightUpdateLabel: "Weight Updates",
                        linearSum: "Linear pre-activation: z = w₁x₁ + w₂x₂ + b. The raw weighted sum before the activation function.",
                        linearSumLabel: "Linear",
                        prediction: "Prediction ŷ = activation(z). The output of the neuron after applying the non-linear activation function.",
                        predictionLabel: "Activation",
                        loss: "Loss L = (ŷ − target)². Squared error between the prediction and the desired target value.",
                        lossLabel: "Loss",
                        reset: "Reset",
                        caption: "Step through forward pass, backpropagation, and weight updates to see how a neuron learns.",
                        buttonLabels: {
                            idle: "Run Forward Pass →",
                            forward: "Backpropagate →",
                            backward: "Update Weights →",
                            update: "New Forward Pass →",
                        }
                    },
                    buttons: {
                        trainStep: "Train 1 Step",
                        autoTrain: "Auto-Train ×10",
                        reset: "Reset",
                        random: "Random",
                    },
                    tabLabels: {
                        perceptron: "Perceptron",
                        activation: "Activation",
                        gradients: "Gradients",
                        training: "Training",
                    },
                    diagram: {
                        title: "Interactive · Perceptron",
                        ariaLabel: "Perceptron flow diagram",
                        caption: "Adjust inputs, weights, and bias to see how the perceptron transforms them into an output.",
                        inputX1: "Input x₁",
                        inputX2: "Input x₂",
                        weightW1: "Weight w₁",
                        weightW2: "Weight w₂",
                        biasB: "Bias b",
                        tooltipW1: "Weight w₁ = {val} — Controls how much input x₁ influences the sum",
                        tooltipW2: "Weight w₂ = {val} — Controls how much input x₂ influences the sum",
                        tooltipX1: "Input x₁ = {val} — The first input feature fed into the neuron",
                        tooltipX2: "Input x₂ = {val} — The second input feature fed into the neuron",
                        tooltipSum: "Weighted Sum z = {val} — Calculated as (w₁×x₁) + (w₂×x₂) + b",
                        tooltipBias: "Bias b = {val} — Shifts the decision threshold; positive = easier to activate, negative = harder",
                        tooltipActivation: "ReLU Activation = {val} — ReLU(z) = max(0, z). Outputs z if positive, otherwise 0",
                        tooltipOutput: "Final Output = {val} — The neuron's prediction after applying ReLU to the weighted sum",
                        breakdownTitle: "Contribution Breakdown",
                    },
                },
            },
        },
    },
    bigramNarrative: {
        hero: {
            eyebrow: "Understanding Language Models",
            titlePrefix: "The Bigram",
            titleSuffix: "Model",
            description: "A first-principles exploration of the simplest statistical language model — and why it still matters.",
            autoCompleteHint: "This model predicts one character at a time. Try typing."
        },
        problem: {
            title: "The Problem of Prediction",
            lead: "Language is fundamentally sequential. Every word you read right now is informed by the words that came before it.",
            p1: "This property — that each token in a sequence carries ",
            p1Highlight: "expectations about what follows",
            p2: " — is what makes language both expressive and predictable. It's also what makes it so hard to model computationally.",
            p3: "The central challenge of language modeling is deceptively simple to state:",
            quote: "Given what we have already seen, what should come next?",
            p4: "This question has driven decades of research in ",
            h1: "computational linguistics",
            h2: "information theory",
            h3: "deep learning",
            p5: ". To build a model that can answer it, we need a way to capture the statistical structure of language. Let's start with the simplest possible approach.",
            label: "Foundation"
        },
        coreIdea: {
            label: "Core Idea",
            title: "The Simplest Statistical Idea",
            lead: "What if, instead of trying to understand meaning, we simply observed patterns?",
            p1: "Specifically: ",
            h1: "how often does one character follow another?",
            p2: " This is the core insight behind the Bigram model. It ignores grammar, semantics, and long-range dependencies entirely. It asks only one question: given the current token, what is the probability distribution over the next token?",
            caption: "The Bigram assumption: the next token depends only on the current one.",
            p3: "This radical simplification is what makes the model both tractable and limited.",
            calloutTitle: "Key Insight",
            calloutP1: "The \"bi\" in Bigram means ",
            calloutH1: "two",
            calloutP2: ". The model considers pairs of tokens — the current one and the next one. It has zero memory of anything before the current token."
        },
        mechanics: {
            label: "Mechanics",
            title: "Building a Transition Table",
            lead: "To learn these probabilities, the model scans through a training corpus and counts every pair of consecutive tokens.",
            p1: "For each token A, it records how often each possible token B appears immediately after it. These counts form a ",
            h1: "matrix",
            p2: " — a two-dimensional table where rows represent the current token and columns represent the next token. Each cell holds the number of times that specific transition was observed in the training data.",
            p3: "The visualization below is a live rendering of this transition matrix. Brighter cells indicate more frequent pairings — patterns the model has learned from real text.",
            dataSourceTitle: "Where Does This Data Come From?",
            dataSourceP1: "This matrix was built by training on a real text corpus (in this case, an essay by Paul Graham). The model scanned through the entire text character by character, counting every pair it saw.",
            dataSourceP2: "For example, if the text contained 'the' 1,000 times, the model incremented the count for 't→h' by 1,000 and 'h→e' by 1,000. After scanning the entire corpus, it normalized each row to create probabilities.",
            dataSourceP3: "What you see below is the final result: a 96×96 table where each of the 96 printable ASCII characters has its own row, showing the probability distribution for what character comes next.",
            calloutTitle: "Reading the Matrix",
            calloutP1: "Each row represents a \"given\" character. Each column represents a \"next\" character. The brightness of a cell encodes how likely that transition is. Notice how some rows are nearly uniform (the model is unsure) while others have sharp peaks (strong preferences).",
            tinyMatrixLabel: "Simplified 5×5 matrix · ['t', 'h', 'e', 'a', '·']",
            tinyMatrixHint: "Hover any cell to see the exact probability. Rows = current character, Columns = next character.",
            tinyMatrixHover: "Hover a cell to see its probability",
            tinyMatrixColLabel: "next character →",
            tinyMatrixRowLabel: "current character →",
            tinyMatrixHigh: "frequent",
            tinyMatrixLow: "rare",
            tinyMatrixRare: "very rare / never",
        },
        normalization: {
            label: "Normalization",
            title: "From Counts to Probabilities",
            lead: "Raw counts alone don't tell us much. To make predictions, we need to convert them into probabilities.",
            p1: "We do this by ",
            h1: "normalizing each row",
            p2: " of the count matrix — dividing every count by the total number of transitions from that row's token. After normalization, each row sums to 1.0, forming a valid probability distribution.",
            p3: "The model can now make concrete statements: \"After the letter h, there is a 32% chance the next character is e, a 15% chance it's a, and so on.\"",
            plainEnglishTitle: "Plain English",
            plainEnglish: "In plain English: To find the chance of letter B after letter A, count how many times you saw A→B, then divide by all the times you saw A followed by anything.",
            p4: "Try it yourself below. Type any text to see what the model predicts will come next — based ",
            h2: "only on the last character",
            p5: " of your input."
        },
        normalizationViz: {
            context: "Example: After 't', what are the chances of each next character?",
            step1Title: "Step 1: Raw Counts",
            step2Title: "Step 2: Divide by Total",
            step2Desc: "Each count divided by the sum of all counts in the row",
            step3Title: "Step 3: Probability Distribution",
            animate: "Animate Conversion",
            reset: "Reset"
        },
        sampling: {
            label: "Sampling",
            title: "Generating New Text",
            lead: "Once we have a probability distribution, we can do something remarkable: generate entirely new text.",
            p1: "The process is called ",
            h1: "autoregressive sampling",
            p2: ". Start with a seed character, sample the next one from its probability distribution, then use that new character as the seed for the next step. Repeat indefinitely.",
            calloutTitle: "Temperature",
            calloutP1: "The ",
            calloutH1: "temperature",
            calloutP2: " parameter controls how \"creative\" the generation is. At ",
            calloutH2: "low temperatures",
            calloutP3: ", the model almost always picks the most likely next token. At ",
            calloutH3: "high temperatures",
            calloutP4: ", it samples more uniformly — producing surprising and often nonsensical output.",
            tempP1: "How you sample matters as much as what you learned. A single ",
            tempH1: "temperature",
            tempP2: " parameter scales the logits before the final softmax step. Below 1.0, the distribution sharpens — the model almost always picks its top-ranked character. Above 1.0, the distribution flattens — every character gets a fairer shot, at the cost of coherence.",
            tempBridge: "Now try the generation playground below. The same bigram model produces markedly different text at temperature 0.1 versus 2.5 — not because its knowledge changed, but because its sampling strategy did.",
            softmaxFigureLabel: "Softmax Temperature · Conceptual",
            softmaxFigureHint: "Drag the slider to see how temperature reshapes the same probability distribution.",
            playgroundLabel: "Generation Playground",
            playgroundHint: "Adjust temperature and observe how it affects the creativity of the generated text.",
            p3: "Generate some text below and observe how a model with ",
            h2: "only one character of memory",
            p4: " produces output that is statistically plausible at the character level, yet meaningless at any higher level."
        },
        reflection: {
            label: "Reflection",
            title: "Power and Limitations",
            lead: "The Bigram model is powerful precisely because of its simplicity.",
            p1: "It requires very few parameters — just a V × V matrix, where V is the vocabulary size. It trains instantly. And it provides a clear ",
            h1: "probabilistic baseline",
            p2: " for language generation that every more sophisticated model must beat.",
            calloutTitle: "The Fundamental Limitation",
            calloutP1: "The model has ",
            calloutH1: "no memory beyond a single token",
            calloutP2: ". It cannot learn that \"th\" is often followed by \"e\", because by the time it sees \"h\", it has already forgotten the \"t\". It captures local co-occurrence but nothing about words, phrases, or meaning.",
            p3: "This limitation is exactly what motivates the progression to more sophisticated architectures: ",
            h2: "N-grams",
            p4: " extend the context window, ",
            h3: "neural networks",
            p5: " learn from data instead of counting, ",
            h4: "MLPs",
            p6: " apply that learning to language, and transformers attend to the entire sequence at once.",
            quote: "Each model in this lab builds on the same core question: given context, what comes next?"
        },
        // DEPRECATED: tokens section moved to ngramNarrative.tokenization
        // Kept here for backwards compatibility only - no longer rendered in BigramNarrative UI
        tokens: {
            label: "Representation",
            title: "Representing text",
            lead: "We split text into tokens.",
            charTitle: "Characters:",
            charDesc: "small vocab, easy to see.",
            wordTitle: "Words:",
            wordDesc: "richer, huge vocab.",
            note: "We use characters here.",
            charLevelTitle: "Character-level tokens",
            charLevelBody: "Small, fixed vocabulary of ~96 printable ASCII symbols. Every possible input is representable. Simple to implement, easy to visualize — ideal for understanding the fundamentals of language modeling.",
            wordLevelTitle: "Word-level tokens",
            wordLevelBody: "Richer semantic units that carry more meaning per token. But vocabulary can reach 50,000–500,000 entries, making the transition matrix enormous. Rare words cause sparsity; words unseen during training cause complete failures at inference time.",
            charLimitations: "Character-level models have a small, manageable vocabulary — but they must learn everything from scratch. There are no pre-built notions of words, morphology, or meaning. The model must discover that 't', 'h', 'e' together form a common word purely from co-occurrence statistics.",
            wordLimitations: "Word-level models are more expressive but face a fundamental scalability problem. English has over 170,000 words in common use. A bigram model at the word level would need a 170,000 × 170,000 transition matrix — nearly 29 billion cells — most of which would be empty (never observed in training). This sparsity problem is one of the core motivations for neural language models.",
            whyCharHere: "For this lab, we use character-level tokens. The vocabulary stays small enough to visualize the entire transition matrix at once, making the model's learned knowledge directly inspectable. Every design decision you see here scales directly to word-level and subword-level models — only the vocabulary size changes."
        },
        counting: {
            title: "The Bigram idea",
            lead: "Count pairs: current -> next. More counts = more likely.",
            builderTitle: "Step-by-step builder",
            builderDesc: "Walk through text; each pair adds +1 to a cell.",
            p1: "The core operation is almost embarrassingly simple: scan through the training text one character at a time, and for every consecutive pair (current character → next character), increment a counter. That's it. After scanning millions of characters, these counts encode the statistical structure of the language — which characters tend to follow which others, and how strongly.",
            p2: "The step-by-step builder below makes this concrete. Watch how each character pair in the input text adds exactly one count to the corresponding cell in the matrix. By the end, the matrix is a complete record of every transition observed in the training data.",
            calloutTitle: "Why counting works",
            calloutText: "The Law of Large Numbers guarantees that as training data grows, the observed frequencies converge to the true underlying probabilities of the language. With enough text, the bigram counts become a reliable statistical portrait of character-level patterns."
        },
        matrix: {
            title: "The transition table",
            lead: "Rows = current token, columns = next.",
            desc: "Build below, then see the full matrix."
        },
        probabilities: {
            title: "Counts to probabilities",
            lead: "Normalize each row to 100%.",
            desc: "Model reads last token's row and samples the next.",
            inferenceIntro: "The tool below lets you walk through this inference pipeline step by step: pick any context character, choose how to normalize its row — plain division or softmax — then sample to see which character the model would predict next. Try a few characters and notice how the distribution changes shape depending on what the model saw most often in training.",
            overlayTitle: "Counts -> Probabilities -> Sampling",
            overlayDesc: "Pick token, normalize row, sample next.",
            step1: "1) Row values",
            step2: "2) Normalize",
            step3: "3) Sample next token",
            currentToken: "Current token",
            typeChar: "Type a character",
            normalizeSimple: "Simple normalize",
            softmax: "Softmax",
            sampleNext: "Sample next token",
            mostLikely: "Most likely:",
            remaining: "Remaining:",
            stochastic: "Sampling is random."
        },
        limitations: {
            title: "Limitations",
            lead: "Bigram has no memory—only the last token.",
            desc: "No long context. Hence N-grams and neural nets."
        },
        textToNumbers: {
            label: "How Computers See Text",
            title: "Turning Text Into Numbers",
            lead: "A computer can't read letters like you do. It needs numbers it can store and compare.",
            p1: "We give each character a number, like a seat number in a theater. Now the model can count which numbered seat tends to come next.",
            bridge: "Once text becomes numbers, we can build a big table of counts. Then we can turn that table into chances for the next letter.",
        },
        predictionExample: {
            label: "See It In Action",
            title: "One Prediction, Step by Step",
            lead: "Before we explain the math, let's watch the model make a single prediction. Pick any character below.",
            inputLabel: "input",
            lookupLabel: "model looks up",
            step1: "Pick a character",
            step2: "Row '{char}' in the table",
            step3: "Top predictions",
            hint: "Click any character above to see what the model predicts next.",
        },
        predictionChallenge: {
            label: "Try It",
            title: "Can You Think Like the Model?",
            lead: "Before we reveal how the bigram model works, try predicting what comes next yourself. Use your intuition about English.",
            prompt: "What character comes next?",
            score: "Score",
            correct: "Correct!",
            wrong: "The answer was '{answer}' —",
            next: "Next →",
            finish: "See results →",
            perfect: "Perfect score! You think just like the model.",
            good: "Nice work — your intuition matches the statistics.",
            tryAgain: "Language patterns can be tricky. Try again!",
            restart: "Try again",
        },
        cliffhanger: {
            label: "The Trap",
            title: "One Big Problem",
            lead: "This model only remembers one letter. That is a tiny memory.",
            p1: "If you see 't' and then 'h', you want the model to remember both. But a bigram model forgets the 't' as soon as it sees 'h'.",
            hookLine: "So what happens if we let the model remember more than one letter?",
        },
        cta: {
            title: "Continue Exploring",
            freeLabButton: "Open Free Lab",
            freeLabDesc: "Switch to Free Lab mode to change N, test your own phrases, and see where the model becomes silent.",
            nextTitle: "Next: What If We Remember More?",
            nextDesc: "Meet the N-gram model — a next-letter guessing machine that looks at the last few characters, not just one.",
        },
        footer: {
            text: "Next, we'll grow this idea into an N-gram model — an extended version that remembers more than one previous character.",
            brand: "LM-Lab · Educational Mode"
        }
    },
    bigramBuilder: {
        description: "We build the bigram matrix by scanning the text character by character. For each pair of consecutive characters (current → next), we increment the cell [current, next]. This table captures how often each character is followed by another.",
        placeholder: "Type text here...",
        hint: "Enter some text to see how the bigram matrix is constructed.",
        buttons: {
            build: "Build Bigram Matrix",
            next: "Next Step",
            autoPlay: "Auto Play",
            pause: "Pause",
            instant: "Instant Complete",
            reset: "Reset Steps"
        },
        vocab: "Educational vocabulary",
        normalized: "Normalized text:",
        empty: "(empty after filtering)",
        skipped: "Showing the first {max} unique characters for clarity ({count} unique character(s) omitted).",
        step1: "Step",
        step2: "updates cell [",
        step3: "].",
        currentStep: "Current Step",
        updatingCell: "Updating cell at row",
        updatingCellCol: "col",
        pressBuild: "Press Build Bigram Matrix and start stepping through character pairs.",
        table: {
            curnxt: "cur \\ nxt"
        }
    },
    bigramWidgets: {
        nnComparison: {
            title: "Interactive · Bigram vs. Neural Network",
            bigramTitle: "Bigram Probabilities (counting)",
            neuralTitle: "Neural Network Weights (learned)",
            stats: {
                steps: "Training steps:",
                distance: "Distance:",
                match: "✓ Neural weights closely match bigram probabilities"
            },
            buttons: {
                train: "Train 1 Step",
                auto: "Auto-Train ×20",
                reset: "Reset"
            },
            caption: "The neural network learns weights that converge to the same transition probabilities the bigram model computes by counting.",
            progression: "Snapshots:",
            live: "Live",
            emotionalMoment: "These random numbers, trained with nothing but gradient descent, learned exactly what counting gave us.",
        },
        textToNumbers: {
            placeholder: "Type something…",
            empty: "Start typing to see character codes…",
            tooltip: "code:"
        },
        pairHighlighter: {
            hint: "Hover a character to see its bigram pair"
        },
        memoryLimit: {
            context: "Context:",
            chars: "chars",
            locked: "locked",
            modelSees: "Model sees:",
            guessingNext: "guessing next…",
            lockedNote: "Context-{size} available in the N-gram chapter",
            ngramLink: "N-gram model →",
            topPredictions: "Top predictions",
            correctAnswer: "Correct answer \"{target}\" ranked #{rank}"
        },
        matrixOverlay: {
            dismiss: "Click to dismiss",
            after: "After",
            mostCommon: "the most common next character is",
            tryHovering: "— try hovering row",
            inMatrix: "in the matrix below.",
            clickToDismiss: "click to dismiss"
        },
        heroAutoComplete: {
            placeholder: "a",
            after: "After “{input}”, likely next",
            hint: "Type one character to see predictions"
        },
        softmax: {
            title: "Softmax Temperature · Conceptual",
            description: "Temperature reshapes the probability distribution without changing the ranking of tokens. Low temperature sharpens the distribution; high temperature flattens it.",
            label: "Temperature",
            deterministic: "Deterministic",
            neutral: "Neutral",
            chaotic: "Chaotic",
            mode: {
                deterministic: { label: "Deterministic", sub: "Always picks the top token. No creativity." },
                conservative: { label: "Conservative", sub: "Mostly picks top tokens with occasional variety." },
                neutral: { label: "Neutral", sub: "Standard sampling — balanced quality and diversity." },
                creative: { label: "Creative", sub: "Explores less likely options. More surprising output." },
                chaotic: { label: "Chaotic", sub: "Nearly uniform — picks almost any token at random." },
            },
            presets: {
                deterministic: "Deterministic",
                balanced: "Balanced",
                neutral: "Neutral",
                creative: "Creative",
            },
            stats: {
                topToken: "Top token",
                entropy: "Entropy",
                spread: "Spread",
                max: "of max",
            },
            note: "Temperature does not change the model's knowledge — only how randomly it samples from what it knows. The token rankings stay the same; only the sharpness of the distribution changes.",
        },
    },
    ngramNarrative: {
        hero: {
            eyebrow: "Understanding Language Models",
            titlePrefix: "What If We",
            titleSuffix: "Remember More?",
            description: "The bigram model could only see one character behind. What happens when we give it two? Three? Five? The answer is both thrilling and devastating.",
        },
        moreContext: {
            label: "More Context",
            title: "Beyond a Single Character",
            lead: "You saw that the bigram model can only look at one character behind. What if we let it look at two? Three? Five?",
            p1: "An N-gram model looks at the",
            p1Highlight: "previous N characters",
            p1End: " before it guesses the next one. Example: N=2 means it can see two characters of context.",
            p2: "More context makes guesses smarter. After \"th\", the model can strongly expect \"e\" — it has seen that pattern many times.",
            p3: "But more memory has a hidden cost. We are about to watch that cost grow faster than your intuition expects.",
            calloutTitle: "The N-gram Assumption",
            calloutText: "The key assumption: the next character depends only on the previous N characters. Everything before that is forgotten. It's like a sliding window — and the question is: how big should it be?",
        },
        contextWindow: {
            label: "Context Window",
            title: "Seeing More of the Past",
            lead: "The context window is how many previous characters the model can \"see\" before it makes a guess.",
            caption: "As the window gets larger, the model can use richer patterns. But the number of possible windows grows extremely fast.",
            hint: "Watch how the context grows as N increases.",
            p1: "Each step up in N gives the model more clues. It also creates many more situations the model might need to remember later.",
        },
        howItWorks: {
            label: "Mechanics",
            title: "Counting with Context",
            lead: "The core idea is unchanged from bigrams — we still count. But now, instead of asking 'what follows this one character?', we ask 'what follows this sequence of N characters?' The table gets deeper, but the logic stays simple.",
            p1: "For every position in the training text, the model extracts the",
            p1Highlight: " N-character context",
            p1End: " and records which character comes next. At prediction time it looks up the matching context row and reads off the stored probability distribution — pure table lookup, no math.",
            p2: "With N=1 (bigram) the table is a flat V×V grid. With N=2 it becomes a stack of grids — one per two-character prefix. Each additional character of context adds another dimension. The table doesn't just grow; it multiplies.",
            bridge: "The transition table above shows individual rows from this giant lookup table. But how do longer contexts actually change the counts? The widget below puts bigram and trigram counting side by side on the same training text so you can see the difference directly.",
        },
        improvement: {
            label: "Improvement",
            title: "The Prediction Gets Better",
            lead: "More context means less ambiguity. When the model can see two characters instead of one, it rules out far more candidates — and the remaining predictions become dramatically more confident.",
            example: "After 'h', dozens of characters are plausible. After 'th', the model strongly expects 'e'. After 'the', a space becomes almost certain. Each extra character of context narrows the field.",
        },
        whyNotMore: {
            title: "Why Not N=100?",
            lead: "If more context makes predictions better, why stop at 3 or 4? Why not look at the last 100 characters?",
            p1: "Because every extra character of context multiplies the table by the vocabulary size. A bigram table has 9,216 entries (96²). A trigram jumps to 884,736 (96³). A 4-gram reaches over 84 million (96⁴). Going to N=100 would require a table with more entries than atoms in the observable universe. The next section makes this explosion visceral.",
        },
        statistical: {
            label: "Statistical Nature",
            title: "A Purely Statistical Model",
            lead: "N-gram models have no understanding of language. They are sophisticated counting machines.",
            p1: "Every prediction is a",
            p1Highlight: "table lookup",
            p1End: " — the model finds the matching context in its table and returns the stored probability distribution. There are no learned parameters, no gradients, no optimization.",
            p2: "This makes N-grams extremely fast at inference and trivially interpretable: you can always ask \"why did the model predict X?\" and trace the answer back to exact training examples.",
            calloutTitle: "No Generalization",
            calloutText: "If the model has never seen a particular context in training, it has zero information about what comes next. Unlike neural networks, N-grams cannot generalize from similar contexts — each context is treated as completely independent.",
        },
        complexity: {
            label: "Complexity",
            title: "The Price of Memory",
            lead: "Here's where the math turns against us.",
            p1: "With 96 possible characters, every extra character of context multiplies the table by 96. N=1: 96 contexts. N=2: 9,216. N=3: 884,736. N=4: 85 million. N=5: over 8 billion.",
            p1Highlight: " 884,736",
            p1End: ". A 5-gram has over 84 million. Most of these contexts will never appear in any realistic training corpus.",
            p2: "Most of those contexts never appear in real text. That means most of the table is empty — this is called sparsity — and empty rows cannot guide predictions.",
            vocabCalloutTitle: "And it gets much worse with words",
            vocabCalloutText: "This lab uses characters (~96 possible). Real language models use words instead. With 50,000 words, even a bigram matrix needs 2.5 billion cells. A trigram table would need 125 trillion. The math turns catastrophic extremely fast.",
            comparisonLabel: "N-Gram Comparison · Live backend metrics",
            comparisonHint: "Compare perplexity, context utilization, and state space across different values of N.",
            metricsLegend: {
                perplexity: "Perplexity means \"how surprised the model is\" on average; lower means better guesses and more confidence.",
                utilization: "Context utilization means how much of the huge table was actually filled by the training text; low utilization means many contexts were never seen.",
                contextSpace: "Context space means how many different contexts could exist in theory; it grows extremely fast as N increases.",
            },
        },
        tokenization: {
            intro: "This complexity becomes catastrophic when we move from characters to words.",
            subsectionTitle: "Characters vs. Words: A Critical Trade-off",
            charTitle: "Character-Level Tokens",
            charDesc: "Small, fixed vocabulary (~96 ASCII characters). Every input is representable. Simple to implement and visualize — ideal for understanding fundamentals. But each token carries almost no semantic meaning.",
            charExample: "Vocab: ~96 | Example: ['t', 'h', 'e']",
            wordTitle: "Word-Level Tokens",
            wordDesc: "Semantically rich units that convey meaning per token. But vocabulary explodes to 50,000–500,000 entries, making the transition matrix enormous. Rare words cause sparsity; unseen words cause complete failure.",
            wordExample: "Vocab: ~50,000 | Example: ['the', 'cat', 'sat']",
            explosionIntro: "The combinatorial explosion at the word level makes even simple N-grams computationally infeasible:",
            tableLabel: "Word-level N-gram explosion · Combinatorial math",
            tableHint: "Assuming basic English vocabulary of 50,000 words. Most cells would be empty (sparse).",
            tableHeaders: {
                model: "Model",
                formula: "Formula",
                combinations: "Combinations",
                scientific: "Scientific",
            },
            noteLabel: "Note:",
            noteText: "These numbers assume a full 50,000-word English vocabulary. Real systems use aggressive pruning, smoothing, and backoff strategies to make this tractable — but the fundamental scaling problem remains.",
            languageP1: "Word-level models are also",
            languageH1: "rigidly language-dependent",
            languageP2: ". A model tokenized for English words breaks completely when given Spanish input, requiring an entirely new vocabulary and matrix. Character-level models, while less semantically rich per token, can handle multiple languages sharing the same alphabet.",
            multilingualCalloutTitle: "The Multilingual Problem",
            multilingualCalloutText: "An English word-level bigram trained on 'the cat sat' has no idea what to do with 'el gato se sentó'. Every word is out-of-vocabulary. Character models avoid this by operating at a lower, more universal level — though they sacrifice semantic density in the process.",
        },
        vocabulary: {
            label: "Vocabulary",
            title: "Characters vs. Words",
            lead: "We use character-level tokens in this lab, but real-world N-grams often operate on words — making the explosion even worse.",
            p1: "With a word vocabulary of 50,000 tokens, even a",
            p1Highlight: " bigram matrix needs 2.5 billion cells",
            p1End: ". A trigram table would require 125 trillion entries. This is why word-level N-grams beyond N=3 are essentially impractical without aggressive smoothing and pruning.",
            p2: "Character-level models keep the vocabulary small (~96), making it feasible to visualize and explore the full table. But the tradeoff is that individual characters carry almost no semantic meaning.",
        },
        noUnderstanding: {
            label: "Limitations",
            title: "No True Understanding",
            lead: "N-gram models capture local co-occurrence patterns but have no notion of meaning, grammar, or long-range coherence.",
            p1: "The model treats \"the cat sat on the\" and \"the dog sat on the\" as completely unrelated contexts (for N < full sentence length). It cannot recognize that both involve an animal sitting on something.",
            p2: "This inability to",
            p2Highlight: "generalize across similar contexts",
            p2End: " is what ultimately limits N-gram models. No matter how much data you collect, there will always be valid contexts the model has never seen.",
            p3: "This fundamental limitation is exactly what motivates the transition to neural approaches — models that learn dense, continuous representations capable of recognizing similarity between contexts.",
        },
        deeperProblem: {
            label: "Limitations",
            title: "The Deeper Problem",
            lead: "The explosion is a practical problem — you can't build a big-enough table. But there's a conceptual problem that's even worse: even with infinite data, counting still fails.",
            p1: "Imagine the text starts with 'the cat sat on the'. If the model has seen that exact context, it can predict what comes next from memory.",
            p2: "Now change one word: 'the dog sat on the'. A human sees it's almost the same situation. The N-gram model treats it like a completely new, unrelated context.",
            p3: "N-grams have no concept of 'similar.' The contexts 'the cat' and 'the dog' are as different to the model as 'the cat' and 'xyzq'. Each is a separate row in the table, with zero connection between them.",
            infiniteData: {
                title: "Even Infinite Data Can't Help",
                p1: "Suppose you had unlimited training text — every book ever written. Could you fill the table? No. Language is creative: people invent new sentences constantly. The number of possible 10-word sequences vastly exceeds the number of sentences ever uttered. No corpus, however large, can cover every valid context.",
            },
            failureExamples: {
                title: "When Counting Breaks Down",
                typoLabel: "Typos",
                typoText: "A user types 'teh cat' instead of 'the cat'. The model has never seen the context 'teh' and returns a uniform (random) distribution. One wrong keystroke erases all learned knowledge.",
                novelLabel: "Novel words",
                novelText: "A new word enters the language — 'selfie', 'blockchain', 'vibe-check'. The model has zero entries for any context containing these words. It cannot even guess that 'selfie' behaves like other nouns.",
            },
            calloutTitle: "No Generalization",
            calloutText: "If the model has never seen a particular sequence in training, it has nothing to say. It can't guess. It can't reason by analogy. It just shrugs. This is the fundamental limitation that motivates neural approaches.",
        },
        endOfCounting: {
            label: "Reflection",
            title: "The End of Counting",
            lead: "We've reached the end of what counting can do.",
            p1: "We started with bigrams, which remember one character. We pushed to N-grams, which remember more, and we watched predictions improve.",
            p2: "Then we hit two walls. The explosion wall: the table grows too fast to fill. More memory multiplies the table again and again.",
            p3: "The generalization wall: each context is an island. The model cannot share knowledge between similar contexts, so it fails on new phrases.",
            quote: "The era of counting is over. The era of learning begins.",
            hookLine: "In the next chapter, we stop counting. We start learning.",
        },
        conclusion: {
            label: "Reflection",
            title: "The Bridge to Neural Models",
            lead: "N-gram models push statistical language modeling to its logical extreme — and reveal why a fundamentally different approach is needed.",
            p1: "We have seen that increasing context improves predictions but triggers an exponential explosion in the state space. This is not a bug — it is an inherent property of discrete, count-based models.",
            p2: "The core problem is representation: N-grams represent each context as an isolated point in a vast discrete space. There is no notion of similarity between contexts, no way to share statistical strength between related patterns.",
            p3: "Neural language models solve this by mapping discrete tokens into continuous vector spaces where similar contexts live near each other. This allows them to generalize from seen examples to unseen but similar contexts.",
            p4: "The progression from Bigram → N-gram → Neural Network is not just historical — it reflects a deepening understanding of what it means to model language computationally.",
            quote: "The curse of dimensionality is not a failure of N-grams — it is the reason neural representations were invented.",
        },
        cta: {
            title: "Continue Exploring",
            labButton: "Open Free Lab",
            labDesc: "Switch to Free Lab mode to change N, test your own phrases, and see where the model becomes silent.",
            neuralButton: "Next: From Counting to Learning",
            neuralDesc: "We've pushed counting to its limit. Now we build something that learns.",
        },
        generationBattle: {
            title: "Generation Battle",
            subtitle: "Same seed, different memory",
            description: "Watch how the same starting text produces dramatically different output as the model's context window grows.",
            columnHeader: "N = {n}",
            qualityLabels: {
                1: "Random noise",
                2: "Letter patterns emerge",
                3: "Word fragments appear",
                4: "Recognizable phrases",
            },
            streaming: "Generating…",
            seedLabel: "Seed text",
            generateButton: "Generate All",
            regenerateButton: "Regenerate",
            tokensLabel: "{count} characters",
            emptyState: "Press Generate to start the battle",
            temperatureLabel: "temperature",
            copyToClipboard: "Copy to clipboard",
        },
        footer: {
            text: "The statistical era is complete. You've seen what counting can do — and where it breaks. Next: models that learn.",
            brand: "LM-Lab · Educational Mode",
        },
        predictingAfter: "Predicting the next character after:",
        readingChart: "Reading the chart",
        ui: {
            collapse: "collapse",
            expand: "expand",
        },
        figures: {
            contextWindow: {
                label: "Context window · Natural language example",
                predictingAfter: "Predicting the next character after:",
                sees: "sees:",
                next: "next?",
                contextSize: "Context size:",
                modelBestGuess: "Model's best guess",
                confident: "{pct}% confident",
                candidates: "Candidates:",
                n1hint: "With just 1 character, the model sees only \"a\" — too little to narrow down the options.",
                n2hint: "Two characters give \"za\" — still ambiguous, but starting to form patterns.",
                n3hint: "Three characters reveal \"zza\" — the model starts recognizing word-like fragments.",
                n4hint: "Four characters show \"izza\" — strong signal that this is probably \"pizza\".",
                n5hint: "Five characters capture \"pizza\" — the model knows exactly what comes next.",
            },
            transitionExamples: {
                label: "Transition examples · Training corpus evidence",
                hint: "Expand any row to see real passages from the training data where this transition was observed.",
            },
            countingComparison: {
                label: "Counting comparison · Bigram vs. Trigram",
                hint: "Same training text, different granularity. Notice how longer contexts produce more specific counts.",
            },
            confidenceImprovement: {
                label: "Confidence improvement · Context length effect",
                hint: "Each extra character of context sharpens the prediction.",
            },
            exponentialGrowth: {
                label: "Exponential growth · Table size by N",
                hint: "Each step multiplies the previous count by the vocabulary size.",
            },
            generalizationFailure: {
                label: "Generalization failure · Cat vs. Dog",
                hint: "Hover the right column to see what the model returns for an unseen context.",
            },
            statisticalEra: {
                label: "Statistical era · Learning path",
                hint: "The counting era is complete. Something fundamentally different comes next.",
            },
            generationBattle: {
                label: "Generation battle · Side-by-side comparison",
                hint: "Each column uses the same seed text but a different context size. Longer context produces more coherent output — until sparsity takes over.",
            },
            sparsityHeatmap: {
                label: "Sparsity heatmap · Table density by N",
                hint: "Switch between N values to see how quickly the probability table empties out.",
            },
            infiniteTable: {
                label: "Data coverage · The infinite data problem",
                hint: "Drag the slider to see how much of each N-gram table can be filled with real training data.",
            },
            typoBreaker: {
                label: "Break the model · Typo & novel word failure",
                hint: "Type a misspelled or novel word to see the N-gram model's confidence collapse.",
            },
        }
    },
    ngramPedagogy: {
        primer: {
            title: "What is a {name}?",
            isEdu: {
                p1: "Imagine you are trying to guess the next letter someone will type. A <0>{name}</0> model peeks at the last <1>{length}</1> letter{suffix} and asks: <2>\"Based on what I just saw, what usually comes next?\"</2>",
                n1: "With only 1 character of memory, the model is essentially guessing blindly from frequency alone.",
                n2: "Two characters of context is enough to learn simple patterns like 'th' → 'e', but not much more.",
                nSmall: "With {n} characters, the model starts capturing short word fragments — but the number of possible contexts is already {count}.",
                nLarge: "At N={n}, the model theoretically has rich local context — but storing every possible {n}-character combination requires billions of entries."
            },
            isFree: {
                p1: "A {name} conditions on the last <0>{length}</0> token{suffix}. Context space grows as |V|<sup>{n}</sup>."
            },
            liveWindow: "Live context window"
        },
        growth: {
            title: "Context growth",
            body: "Watch how the window of visible history expands as N increases. More context means sharper guesses — but also exponentially more possibilities."
        },
        transitions: {
            title: "Transition examples",
            isEduBody: "Instead of a giant table, let's trace a few transitions through the phrase <0>the qui</0>. Each row shows: \"given this context, the next character was...\" — plus real evidence from the training corpus.",
            isFreeBody: "Sample transitions from <0>the qui</0> with corpus evidence.",
            matches: "{count} match{suffix}",
            searching: "Searching training data...",
            noMatches: "No matches found in sampled corpus.",
            corpusEvidence: "Corpus evidence",
            noMatchesExpanded: {
                title: "No matches in sample",
                explanation: "The training corpus sample doesn't contain this exact transition. This is expected — not every possible N-gram appears in a finite corpus. This is the sparsity problem.",
                hint: "Try expanding a different row, or reduce N to see more matches.",
            },
        },
        explosion: {
            title: "Combinatorial Explosion",
            body1: "A {n}-gram model with V={vocabSize} characters would need to store probabilities for every possible {n}-character context. That's:",
            entries: "{count} entries",
            body2: "Over {billionCount} combinations. Most would never be observed in training data, making the table astronomically sparse and impractical.",
            limitReached: "Classical scaling limit reached"
        },
        comparison: {
            title: "Model comparison",
            isEduBody: "As N grows, perplexity drops (the model gets better at predicting locally) — but context utilization plummets because most possible contexts are never seen in training.",
            isFreeBody: "Backend-driven metrics per N. Lower perplexity = better local fit.",
            quality: "Quality (↑ = lower ppl)",
            utilization: "Utilization"
        },
        limitations: {
            title: "Key limitations",
            items: {
                context: {
                    title: "Limited context",
                    isEdu: "Even with N=5, the model forgets everything before those 5 characters. It can never learn that a paragraph is about cooking just because it saw the word 'recipe' ten sentences ago.",
                    isFree: "Even N=5 captures only 5 tokens of history. Long-range dependencies remain invisible."
                },
                scalability: {
                    title: "Exponential scalability",
                    isEdu: "Every extra character of context multiplies the table size by the vocabulary size (~96×). Going from N=3 to N=4 means ~96× more rows to store.",
                    isFree: "Context space grows as |V|^N. Storage and data requirements become intractable for N > 4."
                },
                vocabulary: {
                    title: "Vocabulary explosion",
                    isEdu: "If we used words instead of characters, the vocabulary jumps from ~96 to tens of thousands — making even a bigram table enormous.",
                    isFree: "Word-level N-grams face vocabulary sizes of 50k+, making tables impractical even for small N."
                }
            }
        },
        story: {
            title: "The story of N-grams",
            subtitle: "Why more context seemed like the answer — and why it wasn't enough",
            steps: {
                s1: {
                    title: "The bigram bottleneck",
                    body: "We started with the simplest idea: predict the next character using only the previous one. But a bigram model has the memory of a goldfish — it immediately forgets everything except the last letter."
                },
                s2: {
                    title: "A natural extension",
                    body: "The obvious fix? Look at more history. A trigram looks at 2 previous characters, a 4-gram at 3, and so on. Each step gives the model richer local context and noticeably better predictions."
                },
                s3: {
                    title: "The cost of memory",
                    body: "But there's a catch. Each extra character of context multiplies the number of possible states by the vocabulary size. A trigram with 96 characters already has 884,736 possible contexts. Most are never observed in training — the table becomes astronomically sparse."
                },
                s4: {
                    title: "The scaling wall",
                    body: "By N=5, we would need over 8 billion table entries. No dataset is large enough to fill that table meaningfully. This is the fundamental reason N-grams were eventually replaced by neural models that can generalize across similar contexts."
                }
            }
        }
    },
    challenge: {
        badge: "CHALLENGE",
        solvedBadge: "SOLVED",
        checkButton: "Check Answer",
        skip: "Skip",
        showHint: "Show hint",
        hideHint: "Hide hint",
    },
    neuralNetworkNarrative: {
        sections: {
            discovery: { number: "01", label: "The Spark" },
            fromNumbers: { number: "08", label: "Language" },
        },
        narratorTooltips: {
            learning: "Learning = adjusting weights so the answers get less wrong over time.",
            weights: "Weights decide how much each input matters.",
            activation: "An activation is the non-linear 'gate' that lets neural nets bend lines into curves.",
            nonLinearity: "Non-linearity = the reason deep nets can do more than draw straight lines.",
            relu: "ReLU = max(0, x). It keeps positive signals and kills negative ones.",
            parameters: "Parameters = the learnable numbers (weights + biases) the network adjusts.",
            gradient: "Gradient = which way is downhill (the direction that reduces loss fastest).",
            loss: "Loss = how wrong the model is.",
            epoch: "Epoch = one pass through all training data.",
            batch: "Batch = a small group of examples processed together.",
            step: "Step = one weight update.",
            contextWindow: "Context window = the chunk of previous characters the model can see.",
        },
        discovery: {
            heading: "Let's Teach a Machine to Learn",
            lead: "Counting hit a wall. ",
            leadHighlight: "N-grams can't share knowledge between similar patterns",
            leadEnd: " and their tables explode with every extra character of context. We need a completely different approach — one that doesn't memorize, but discovers.",
            bigramBridge: "Remember the bigram model? It counted how often one letter follows another. When you typed \"th\", it looked up how many times each letter appeared after \"th\" in the training text. But internally, letters are just numbers — 't' might be 19, 'h' might be 7. The bigram model never did any math with those numbers. It just used them as addresses in a giant table.",
            bigramQuestion: "What if, instead of a lookup table, we actually computed something with those numbers? What if we could take the numbers for 't' and 'h', do some math, and get a prediction for what comes next?",
            letterDemoLabel: "Interactive · From Letters to Numbers",
            letterDemoHint: "Pick a letter pair and see how letters become numbers that flow through a simple computation. This is the core idea of a neural network.",
            hookP1: "That's the core idea of a neural network: it takes numbers, follows a few simple steps, and produces an answer. The only hard part is ",
            hookP1Highlight: "choosing the right steps",
            hookP1End: " so the answers are usually correct.",
            hookP2: "Imagine predicting how long your commute to school takes. You know two things: the distance (8 km) and the traffic level (3 on a scale of 1–10). With just those two numbers, how could you make a prediction for travel time?",
            p1: "Start simple: take 8 (distance) and 3 (traffic). Try a few ways to combine them and watch how the result changes.",
            fig1Label: "Interactive · Operation Explorer",
            fig1Hint: "Try each operation. Notice how the weighted sum gives you the most control — you can tune how much each input contributes.",
            p2: "Now those two numbers represent real measurements of your daily commute. The weighted sum gives you a predicted travel time in minutes. By adjusting the weights, you control how much distance and traffic each contribute to the prediction.",
            fig2Label: "Interactive · Weight Sliders",
            fig2Hint: "Drag the weights. A weight of 2 means that input matters twice as much. A weight of 0 ignores it entirely. Negative weights flip the contribution.",
            p3: "Weights can be any number — bigger than 2, smaller than −1 — we're only limiting the slider to keep things readable. A high weight on distance means each kilometer adds a lot of travel time. A negative weight would mean more distance somehow reduces time (physically nonsensical — but the model doesn't know physics, it just follows the math).",
            p4: "One more tiny upgrade. What if both measurements are zero — you live at school and there's no traffic — but you still need 5 minutes to walk out the door? Or the opposite: even with a decent commute, you always arrive early. That 'default offset' is a bias — a constant push that shifts the predicted time up or down.",
            fig3Label: "Interactive · Adding Bias",
            fig3Hint: "The bias shifts the entire output up or down. A positive bias makes the neuron 'eager' — it fires even with weak inputs. A negative bias makes it reluctant.",
            calloutTitle: "You just built a neuron",
            calloutText: "Weighted sum + bias. That's it. Every neural network in the world — from the simplest to GPT-4 — is built from units doing exactly this. The rest is scale and clever architecture.",
            bridge: "You now have a single computing unit: inputs × weights + bias = output. But right now it's just a formula. Let's put all the pieces together and see it work.",
            letterDemo: {
                title: "From Letters to Numbers",
                step1: "Start with two letters",
                step2: "Each letter has a number (its position in the alphabet)",
                step3: "Now compute: multiply each by a weight, add a bias",
                step4: "This score helps predict the next letter",
                showComputation: "See the computation →",
                scoreFor: "prediction score for",
                insight: "Different weights would give different scores. The network's job is to find weights that produce the right predictions. That's learning.",
            },
            operations: {
                title: "Operation Explorer",
                inputLabel: "Inputs",
                addBtn: "Add",
                multiplyBtn: "Multiply",
                weightedSumBtn: "Weighted Sum",
                resultPrefix: "Result",
                weightNote: "Weights are fixed at 1.0 for now — you'll control them in the next step.",
            },
            weights: {
                title: "Weighted Inputs",
                inputLabel1: "x₁ (distance in km)",
                inputLabel2: "x₂ (traffic level)",
                weightLabel: "Weight",
                biasLabel: "default offset",
                sumLabel: "Weighted Sum",
                formula: "Predicted commute time",
                contributionLabel: "Contribution breakdown",
                outputLabel: "Predicted time",
                hint: "Drag the weights to see how each input's importance changes the output.",
            },
            bias: {
                title: "Adding Bias",
                biasLabel: "Bias (b)",
                resultLabel: "Output (z)",
                active: "Active",
                barelyActive: "Barely active",
                inactive: "Inactive",
                hint: "Positive bias = eager to fire. Negative bias = reluctant.",
            },
            challenge1: {
                question: "Switch to Weighted Sum. With distance = 8 and traffic = 3, what predicted time do you get when both weights are 1?",
                hint: "With weights at 1, the weighted sum is just plain addition.",
                success: "Exactly! 1×8 + 1×3 = 11 minutes. Weights at 1 mean each factor contributes equally — just like plain addition.",
            },
            challenge2: {
                question: "Make traffic dominate the prediction. Can you set the weights so traffic accounts for about 70% of the predicted time?",
                hint: "Try pushing w₂ up and w₁ down. Compare the two weighted bars (w₁·x₁ vs w₂·x₂).",
                success: "Nice. By turning weights into volume knobs, you can make one factor drown out the other — exactly what a neuron does when it learns what matters.",
            },
            inputsFixedTitle: "Inputs are data, weights are learnable",
            inputsFixed: "Here's something crucial: the distance and traffic level are measurements — you can't change them. The only things you CAN change are the weights. This is the fundamental rule of neural networks: inputs are fixed data, weights are the knobs the model learns to turn.",
            countingVsLearning: {
                title: "Counting vs Learning",
                countingCol: "N-gram (Counting)",
                learningCol: "Neural Network (Learning)",
                row1Label: "How it works",
                row1Counting: "Counts how often patterns appear in text",
                row1Learning: "Computes a prediction using weights and inputs",
                row2Label: "What it stores",
                row2Counting: "A giant table of counts — one entry per pattern",
                row2Learning: "A small set of numbers (weights + bias) that encode knowledge",
                row3Label: "New patterns",
                row3Counting: "If it hasn't seen the pattern, it has no answer",
                row3Learning: "Can make a reasonable guess by combining what it knows",
                row4Label: "Scaling",
                row4Counting: "Table explodes exponentially with context size",
                row4Learning: "Same small set of weights, regardless of how many examples",
            },
            predict1: "Before you try — which operation do you think gives you the most control over the result?",
            predict2: "What happens if you set one weight to zero? Try to predict before you drag.",
            predict3: "If both inputs are zero, what should the output be? What if you want a non-zero default?",
        },
        hero: {
            eyebrow: "Chapter 3 · From Counting to Learning",
            titlePrefix: "Neural",
            titleSuffix: "Networks",
            description: "Counting hit a wall — N-grams can't generalize to unseen patterns. What if, instead of memorizing every combination, we could build a machine that learns from examples?",
            recap: "← Previously: you discovered that statistical models reach their limit. Now let's build something fundamentally different.",
        },
        history: {
            title: "The History of Neural Networks",
            summary: "From 1943 to modern deep learning — a story of breakthroughs, winters, and persistence.",
            subtitle: "Eight decades of innovation, setbacks, and triumph",
            p1: "In 1943, McCulloch and Pitts proposed that a brain cell could be modeled as a logical gate: receive signals, and if strong enough, fire. They showed networks of these units could compute anything. This was pure theory — no computers existed yet to test it. But the seed was planted: intelligence could be mechanical.",
            p2: "In 1958, Frank Rosenblatt built the Mark I Perceptron at Cornell — the first machine that learned from experience. Weighing 5 tons and using photocells as inputs, it adjusted its own parameters to recognize simple shapes. The New York Times called it 'the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.' The hype was real.",
            p3: "In 1969, Minsky and Papert proved single-layer perceptrons had fundamental limits. They could not learn XOR — a simple pattern any child could grasp. Funding dried up overnight. The AI Winter lasted nearly two decades. Researchers abandoned the field. Neural networks became a cautionary tale.",
            p3_5: "But the idea refused to die. Throughout the 1970s, a handful of researchers kept working in obscurity. The key insight — that you could train multi-layer networks by propagating errors backward through the layers — was discovered independently by several groups. It would take until 1986 for the idea to break through.",
            p4: "In 1986, Rumelhart, Hinton, and Williams published their paper on backpropagation — the algorithm that showed multi-layer networks could learn. The mathematics had existed since the 1970s, but now it was practical. Hidden layers changed everything. The thaw began, slowly.",
            p5: "It took another 25 years, massive datasets, and the GPU revolution before deep learning conquered the world. ImageNet 2012. AlphaGo 2016. GPT-3 2020. But the seed planted in 1943 never stopped growing — it just needed time, scale, and compute to bloom.",
        },
        artificialNeuron: {
            title: "Putting It Together",
            lead: "You built the pieces — weights, sum, bias. Now let's wire them into a single unit and watch it compute. This is the artificial neuron: the atom of every neural network.",
            p1: "Below is everything you just learned in one interactive diagram. Drag the sliders to change inputs, weights, and bias — and watch how each one affects the final output in real time.",
            p1Highlight: "weight",
            p1End: "",
            p2: "Here's what happens inside: the neuron multiplies each input by its weight (to control importance), adds them together, adds the bias (a starting push), and finally passes the result through a simple filter called an activation function. ReLU is the simplest: if the number is positive, it passes through unchanged. If it's negative, it becomes zero. Think of it as a gate that only lets positive signals through.",
            formulaCaption: "Multiply each input by its weight, sum everything, add the bias, then apply an activation function f. This is the atomic operation of every neural network.",
            p3: "With the right weights and bias, a single neuron can",
            p3Highlight: "draw a straight line through data and classify everything on one side as A and the other as B",
            p3End: ". Training is how the neuron finds those right numbers — starting from random guesses and improving step by step.",
            calloutTitle: "What are parameters?",
            calloutText: "Weights and biases together are called parameters. At the start, they're random noise. By the end, they encode everything the network has learned — stored as nothing more than a list of decimal numbers.",
            formalizeParagraph: "In math, everything you just explored can be written in one line:",
            formulaCaptionMoved: "Multiply each input by its weight, sum everything, add the bias, then pass it through an activation function f. This is the atomic operation of every neural network.",
            bridgeToScaling: "One neuron can compute a weighted sum and fire (or not). That's powerful — but also limited. What happens when we put many of these units together?",
            walkthrough: {
                title: "Breaking it down with a real example",
                scenarioTitle: "Scenario:",
                scenarioText: "A neuron is trying to predict your commute time.",
                intro: "It considers two inputs: distance to school (x₁) and traffic level (x₂). Let's watch it make a prediction.",
                step1: "Start with the inputs",
                step2: "Multiply each input by its weight",
                step2Desc: "Weights set importance. Here, distance matters a lot (w₁ is high) while traffic matters less (w₂ is small).",
                step2Hint: "Distance contributed 1.2 points, traffic contributed only 0.18 points.",
                step3: "Add the weighted inputs together (Σ)",
                step3Hint: "Σ (sigma) just means \"add them all up.\"",
                step4: "Add the bias",
                step4Desc: "Bias is the baseline push. Positive bias makes the neuron easier to activate; negative bias makes it pickier.",
                step4Hint: "A negative bias reduces the score before activation.",
                step5: "Apply the activation function",
                step5Desc: "The activation decides how strongly the neuron \"fires.\" ReLU keeps positive values and clips negatives to 0.",
                step5Hint: "Since 0.88 is positive, it passes through unchanged.",
                resultTitle: "Final output",
                resultTextPart: "The neuron outputs",
                resultDesc: "This could be interpreted as a confident prediction of a short commute.",
                finalNote: "That's all a neuron does: multiply by weights, add bias, then apply an activation.",
            },
            biological: {
                title: "Biology vs Machine",
                subtitle: "Inspired by nature, but vastly simpler",
                bioLabel: "Biological Neuron",
                artLabel: "Artificial Neuron",
                dendrites: "Dendrites receive signals from other neurons",
                cellBody: "Cell body processes incoming signals",
                axon: "Axon transmits the output signal",
                synapse: "Synapses connect neurons with varying strengths",
                inputsArt: "Inputs (numbers fed in)",
                weightsArt: "Weights (importance of each input)",
                sumArt: "Weighted sum + bias",
                activationArt: "Activation function (fire or not)",
                caveat: "The resemblance is intentional but shallow. Real neurons are complex electrochemical systems with timing, inhibition, and plasticity. Artificial neurons are a mathematical simplification that captures just one idea: signals in, computation, signal out.",
            },
            predict4: "Can you trace the path from inputs to output in your head before seeing it animated?",
            perceptronLabel: "Interactive · The Perceptron",
            perceptronHint: "Click each layer to see inputs flow through weights, bias, and activation. This is the complete artificial neuron.",
        },
        parallelNeurons: {
            title: "Interactive · Parallel Neurons",
            neuronCount: "{n} neuron(s)",
            sameInputs: "Same inputs",
            multipleOutputs: "Multiple outputs",
            reluToggle: "ReLU",
            reluOn: "On",
            reluOff: "Off",
            inactiveTitle: "Neuron is inactive",
            inactiveDesc: "ReLU clipped it to 0",
            inactiveBadge: "Inactive (ReLU → 0)",
            inactiveSummary: "{neurons} produced a negative value, so ReLU clipped the output to 0.",
            insightOne: "1 neuron → 1 output. It can only compute one thing.",
            insightMultiple: "{n} neurons → {n} outputs. Each neuron learns something different from the same data.",
            hint: "Add neurons and toggle ReLU to see how multiple neurons process the same inputs independently.",
        },
        decisionBoundary: {
            title: "Interactive · Decision Boundary",
            hint: "Drag the weight sliders to move the decision line. Try both scenarios — simple and XOR.",
            simpleBtn: "Simple (separable)",
            complexBtn: "Complex (XOR)",
            biasLabel: "bias",
            accuracyLabel: "Accuracy",
            classA: "Class A",
            classB: "Class B",
            insightLinearPerfect: "One neuron, one line — and it perfectly separates the two groups. This is what a single neuron can do.",
            insightLinearTry: "Drag the sliders to find a line that separates blue from red. A single neuron can solve this.",
            insightXor: "No matter how you move the line, you can't separate the groups. One straight line is not enough. This is the XOR problem — and it's why we need more neurons.",
        },
        nonLinearity: {
            title: "What If We Add More Neurons?",
            lead: "One neuron can only draw one straight line. What happens when we combine several? Let's find out — step by step.",
            linearProblem: "Here's the first surprise: if you just stack layers of neurons without anything special between them, you get nothing new. ",
            linearProblemHighlight: "Two layers collapse into one. Ten layers collapse into one.",
            linearProblemEnd: " It's like stacking magnifying glasses — they just combine into a single magnification. Try it:",
            stackingIntro: "Add layers below, then toggle the activation function to see the difference.",
            stackingLabel: "Interactive · Stacking Layers",
            stackingHint: "Add layers and watch: without an activation function, the combined effect is always a straight line. With ReLU, every layer adds a bend.",
            stackingOutro: "Without activation functions, depth is an illusion. The network is still just one big linear operation — no matter how many layers you add.",
            stacking: {
                title: "Layer Stacking Demo",
                layerCount: "{n} layers",
                addRelu: "Add ReLU",
                stillLinear: "Still a straight line!",
                bendCount: "{n} bends!",
                hint: "Linear layers collapse. Non-linearity is what makes depth useful.",
            },
            activationIntro: "The fix is a single small idea: after each neuron's sum, apply a non-linear function. This breaks the collapse. Every layer now adds a genuine bend. Explore the most common activation functions below:",
            activationLabel: "Interactive · Activation Functions",
            activationHint: "Click each function to compare shapes. ReLU is the default for modern networks — simple but effective.",
            p3: "Different functions have different personalities.",
            p3Highlight: "ReLU passes positives unchanged and kills negatives. Sigmoid squashes everything into 0–1. Tanh squashes into −1 to 1.",
            p3End: "The choice matters: ReLU made deep networks practical by avoiding a training problem called vanishing gradients, where error signals shrink to near-zero in early layers.",
            parallelIntro: "Now let's add more neurons. Instead of a single neuron producing a single output, put several side by side — all looking at the same inputs, each with its own weights and bias.",
            parallelOutro: "Each neuron learns to detect a different pattern in the same data. This is called a layer — a group of neurons working in parallel. One output becomes many.",
            whyALine: "But what does each neuron actually do? Think of it as asking one yes-or-no question: is this data point above or below my line? The neuron computes its weighted sum, and the result is either positive (yes) or negative (no). That dividing line is the neuron's decision boundary.",
            whyALineDetail: "For example: 'Is the commute time more than 30 minutes?' One neuron, one question, one straight line splitting the space in two. Multiple neurons ask multiple questions — and by combining their answers, we can describe much more complex regions.",
            boundaryIntro: "Here's what that looks like in practice. A single neuron draws one straight line through the data and says \"everything on this side is A, everything on that side is B.\" Try it — drag the weights to move the line:",
            boundaryOutro: "Start with the simple scenario — one line is enough. Then switch to XOR mode and try to reach 100% accuracy. You'll find it's impossible with a single neuron.",
            xorChallenge: {
                question: "Switch to XOR mode. Try every combination of weights and bias. Can you get 100% accuracy?",
                hint: "No matter how you tilt or shift the line, some red and blue points end up on the wrong side. XOR cannot be solved by a single straight line.",
                success: "Exactly — you can't! One neuron = one line. XOR needs at least two lines to separate the four points. This is why we need multiple neurons working together.",
            },
            layerIntro: "XOR showed one layer isn't enough. What if we stack layers — one after another — so the output of one becomes the input of the next?",
            peak2: "This stumped the field for twenty years.",
            reflection1: "One neuron, one line. Two neurons, two lines. What happens with a hundred?",
            whatIf1Title: "What if you stacked 100 layers without activation?",
            whatIf1Desc: "A proof using matrix algebra that any number of linear layers collapses into one.",
            whatIf1Text: "Each linear layer is just a matrix multiplication: output = W · input. Two layers? That's W₂ · (W₁ · input) = (W₂ · W₁) · input. The product of two matrices is just another matrix. So two layers = one layer. A hundred layers? Still one matrix.",
            whatIf1MatrixLabel: "Matrix multiplication — layers collapse",
            whatIf1Conclusion: "No matter how many layers you stack, without non-linearity between them, the entire network collapses into a single matrix multiplication. Depth is an illusion. This is the linear collapse problem — and it's why activation functions are non-negotiable.",
            xorSolverIntro: "Now for the payoff. Remember the XOR problem that one neuron couldn't solve? With two neurons and ReLU activation, the network can draw two boundary lines — creating a band that separates the classes. Toggle between the modes to see the difference:",
            xorSolverOutro: "Two neurons with ReLU solved what one neuron never could. Each hidden neuron draws one line; the output neuron combines their answers. This is the fundamental power of neural networks: simple pieces, combined with non-linearity, can solve complex problems.",
            deadNeuronIntro: "Now that you understand gradients, here's a catch with ReLU: if a neuron's pre-activation is always negative, ReLU clips it to zero — and zero gradient means zero learning. The neuron is permanently dead. This is the 'dying ReLU' problem.",
            summaryCalloutTitle: "The recipe so far",
            summaryCalloutText: "Take neurons (weighted sum + bias). Add activation functions to unlock non-linearity. Put neurons in parallel to get a layer. Stack layers in series to get depth. That's the architecture of every neural network.",
        },
        xorSolver: {
            title: "Interactive · XOR Solved with ReLU",
            hint: "Toggle between linear (no activation) and ReLU to see how two neurons with activation can solve the XOR problem.",
            linearBtn: "Without Activation",
            reluBtn: "With ReLU",
            classA: "Class A",
            classB: "Class B",
            accuracy: "Accuracy",
            networkLabel: "Network",
            insightLinear: "One straight line can never separate the XOR pattern. No matter where you draw it, some points end up on the wrong side. This is the limit of a single linear neuron.",
            insightRelu: "Two hidden neurons with ReLU create two boundary lines. Together they form a band — the diamond-shaped region that correctly separates all four groups. This is the power of non-linearity.",
        },
        divergence: {
            title: "Interactive · What Happens Without a Learning Rate?",
            hint: "Toggle between full gradient (lr=1) and a small learning rate (lr=0.01) to see the dramatic difference.",
            fullBtn: "Full gradient (η = 1)",
            smallBtn: "Small steps (η = 0.01)",
            exploded: "Loss exploded! 💥",
            converged: "Converged smoothly ✓",
            insightFull: "With η = 1, we subtract the entire gradient. The first step overshoots the minimum so badly that the next step overshoots even more. The loss spirals to infinity. This is divergence.",
            insightSmall: "With η = 0.01, each step is a small fraction of the gradient. The loss drops smoothly toward zero. The weights converge to good values. This is why the learning rate exists.",
        },
        bioVsArtificial: {
            intro: "The artificial neuron was directly inspired by how real brain cells work. McCulloch and Pitts studied biological neurons in 1943 and noticed a pattern: signals arrive through dendrites, get processed in the cell body, and if the total signal is strong enough, the neuron fires an output down the axon. They asked: can we build a mathematical model that does the same thing?",
            bioTitle: "Biological Neuron",
            artTitle: "Artificial Perceptron",
            dendrites: "Dendrites",
            soma: "Cell body",
            axon: "Axon",
            terminals: "Terminals",
            synapses: "Synapses",
            inputs: "Inputs",
            weights: "Weights",
            sumActivation: "Sum + σ",
            output: "Output",
            map: {
                dendritesBio: "Dendrites receive signals from other neurons",
                dendritesArt: "Inputs (x₁, x₂, ...) receive data",
                synapsesBio: "Synapses strengthen or weaken signals",
                synapsesArt: "Weights (w₁, w₂, ...) amplify or reduce inputs",
                somaBio: "Cell body sums all incoming signals",
                somaArt: "Summation: Σ(wᵢ · xᵢ) + bias, then activation σ",
                axonBio: "Axon fires if signal exceeds threshold",
                axonArt: "Output ŷ — the neuron's prediction",
            },
        },
        weightImpact: {
            title: "Interactive · How a Weight Changes the Loss",
            hint: "Drag the weight slider and watch the entire chain update: input × weight + bias → σ → output → loss. Then see what happens if you nudge the weight by 0.1.",
            introText: "Now that we know how to measure error, we can ask the key question: if we change one weight slightly, does the loss go up or down? Let's trace through a real neuron to find out. The chain is simple: input × weight + bias → activation function → output → compare to target → loss.",
            bridge: "You just saw it with your own eyes: nudging the weight by a tiny amount changes the loss. If the loss went up, we should move the weight the other direction. If the loss went down, we should keep going. This idea — 'which direction makes the loss smaller?' — is exactly what the derivative captures. Let's now see this as a graph:",
            nudgeTitle: "What if we nudge the weight by +0.1?",
            shouldDecrease: "Increasing w made loss worse → we should DECREASE the weight",
            shouldIncrease: "Increasing w made loss better → we should INCREASE the weight",
            atMinimum: "We're at (or very near) the minimum! No adjustment needed.",
            insight: "This is the entire trick of training: try a tiny change, see if the loss gets better or worse, then adjust. The derivative automates this for every weight simultaneously.",
        },
        lossDerivative: {
            title: "Interactive · The Derivative as a Graph",
            hint: "Drag the weight and see the loss curve. The slope of the tangent line IS the derivative — it tells you which direction to move.",
            introText: "The same idea we just explored — nudging a weight and checking if the loss goes up or down — can be visualized as a curve. Every possible weight value has a corresponding loss. The derivative at any point is the slope of this curve: steep slope means big effect, flat slope means little effect.",
            showNudge: "🔍 Show me the math: nudge w by 0.01",
            hideNudge: "Hide nudge details",
            changeRatio: "Δloss / Δw =",
            positiveSlope: "Slope is positive → increasing w increases loss → move w LEFT",
            negativeSlope: "Slope is negative → increasing w decreases loss → move w RIGHT",
            zeroSlope: "Slope ≈ 0 → we're at the minimum!",
            insight: "The derivative tells you exactly which direction to adjust each weight. Positive derivative? Decrease the weight. Negative? Increase it. This is the entire foundation of training.",
        },
        flatGradient: {
            title: "Advanced: What If the Derivative Is Zero?",
            desc: "The vanishing gradient problem — why some neurons stop learning entirely.",
            intro: "If the derivative is zero, the weight gets zero update. No learning happens. This isn't just a theoretical concern — it's one of the most important problems in deep learning. Let's see exactly when and why this happens with the sigmoid function.",
            vizTitle: "Interactive · Sigmoid Saturation Zones",
            vizHint: "Drag the input value. In the red zones, the derivative is nearly zero — the neuron is 'saturated' and can't learn.",
            mathTitle: "The math behind saturation",
            mathExplain: "When z is very large or very negative, σ(z) is close to 0 or 1. Then σ(z)(1 − σ(z)) ≈ 0 × 1 = 0 or 1 × 0 = 0. The derivative vanishes. In a deep network, these tiny derivatives multiply together layer by layer — 0.01 × 0.01 × 0.01 = 0.000001. The gradient signal dies before reaching early layers.",
            solution: "This is why ReLU replaced sigmoid in deep networks. ReLU's derivative is exactly 1 for positive inputs — the gradient flows unchanged. Modern architectures also use batch normalization, residual connections, and careful initialization to keep gradients alive.",
            stuck: "STUCK",
            learning: "LEARNING",
            slow: "SLOW",
            flatExplain: "The sigmoid is saturated here. The derivative is nearly zero, so weight updates will be vanishingly small. The neuron can't learn from this input. If most inputs land in this zone, the neuron is effectively dead.",
            healthyExplain: "The sigmoid is in its active zone. The derivative is large enough for meaningful weight updates. The neuron can learn effectively from inputs in this range.",
            slowExplain: "The derivative is small but not zero. Learning will happen, but very slowly. In deep networks, even this small gradient will shrink further as it propagates back through layers.",
        },
        backpropZero: {
            title: "Interactive · Backpropagation with Zero Derivative",
            hint: "Move z into the saturation zones (far from 0) and watch σ'(z) collapse to zero — killing the entire gradient chain. The weight update becomes zero: the neuron stops learning.",
            intro: "You saw that the sigmoid derivative goes to zero in the saturation zones. But what does that actually mean for learning? Let's trace the full backpropagation chain. During training, the gradient for each weight is computed by multiplying several terms together. If ANY one of those terms is zero, the entire gradient is zero — and the weight doesn't change at all.",
            stuck: "σ'(z) ≈ 0 → the gradient chain is broken → the weight cannot update. This neuron is stuck!",
            working: "σ'(z) is large enough → gradient flows → the weight can update normally.",
        },
        batchChallenge: {
            question: "Challenge: In the training demo above, try setting the learning rate very high (e.g. 0.3) with just a single training example. Can you make the loss INCREASE instead of decrease? Why does that happen?",
            hint: "Think about what happens when the model sees only one unusual example. If that example is very different from the average, the gradient pushes the weights too far in one direction — overshooting the minimum. It's like asking one random person for directions: if they send you the wrong way, you end up further from your destination than when you started.",
            success: "Exactly! With a small batch and a high learning rate, a single outlier can push the weights so far that the overall loss goes up. This is why we use mini-batches (groups of examples) and careful learning rates. Imagine asking 10 people for directions instead of 1 — the average answer is much more reliable, even if some individuals are wrong.",
        },
        batchComparison: {
            title: "Interactive · Why Batch Size Matters",
            hint: "Watch all 3 batch sizes train simultaneously. Red (batch=1) is noisy and chaotic; yellow (batch=4) is smoother; green (batch=ALL) is a clean path to the answer.",
            introText: "Why not just train on one example at a time? Imagine you're trying to learn the average height of 12 people. If you only look at one person per step, a very tall person will wildly skew your estimate. Let's watch all 3 batch sizes train side by side — same data, same learning rate, different batch sizes:",
            outroText: "This instability with small batches is a preview of a deeper problem: if your training data isn't representative, your model will learn the wrong patterns entirely. This is called overfitting — and it gets its own section next.",
            insightStart: "Press 'Step' or 'Run all 20' to start training. All three models start from the same wrong prediction (155 cm) and try to find the true mean. Watch how differently they get there.",
            insightMid: "Notice the red line (batch=1) bouncing wildly — each random person yanks it in a different direction. The yellow line (batch=4) is smoother. The green line (batch=ALL) takes the most direct path.",
            insightEnd: "All three eventually get close, but batch=1 took a chaotic zigzag path while batch=ALL went straight. In practice, batch=ALL is too expensive for large datasets (millions of examples), so mini-batches (like 4, 32, or 64) are the sweet spot: noisy enough to explore, stable enough to converge.",
        },
        matrixMultiply: {
            title: "Advanced: A Layer IS a Matrix Multiply",
            desc: "See how a neural network layer is really just matrix multiplication — the operation GPUs are designed to do in parallel.",
            insight: "A layer IS a matrix multiply. Every input gets multiplied by every weight simultaneously. GPUs are designed to do millions of these in parallel — that's why neural networks run on graphics cards.",
            coreOp: "The core operation",
            formalTitle: "Formal notation",
            formalDesc: "In vector notation, a single layer computes y = x·W + b. The weight matrix W transforms the input x into the output y. The bias b shifts the result. Each output neuron j computes a dot product of the input with its column of W:",
            sumExplain: "Each output yⱼ is the sum of all inputs multiplied by the corresponding weights in column j, plus a bias. This is exactly the 'multiply, sum, add bias' pattern from the single neuron — but done in parallel for all outputs at once.",
            dimTitle: "Dimensions matter",
            dimExplain: "The inner dimensions must match: x has n features, W has n rows. The output size m is set by the number of columns in W — that's how many neurons are in the layer.",
            stackTitle: "Stacking layers: deep networks",
            stackDesc: "A deep network is just several matrix multiplies in sequence, with a non-linearity (like ReLU) after each one. The non-linearity is crucial — without it, stacking layers would collapse into a single matrix multiply (since AB = C for any matrices A, B).",
            stackNote: "Each σ is an activation function applied element-wise. Without it, W₁·W₂·W₃ = W_combined — the network would collapse to a single layer no matter how many you stack. The non-linearity is what makes depth useful.",
            scaleTitle: "Real-world scale",
        },
        trainValSplit: {
            title: "Interactive · Train / Validation Split",
            hint: "Click 'Split' to see how data is divided into training and validation sets. The model only trains on the filled dots.",
            trainLabel: "Train",
            valLabel: "Val",
            trainArrow: "Model trains on these",
            valArrow: "Model tested on these",
            splitBtn: "Split the data →",
            resetBtn: "↺ Recombine",
        },
        activationDeriv: {
            title: "Activation Functions & Their Derivatives",
            expandableTitle: "Advanced: activation derivatives & vanishing gradients",
            hint: "Compare how each activation function transforms its input (left) and how its derivative behaves (right). Red zones = vanishing gradient.",
            derivLabel: "derivative",
            vanishingWarning: "Vanishing gradient zone detected",
            sigmoidNote: "Sigmoid's derivative peaks at 0.25 and drops to near-zero for large |x|. In deep networks, these tiny gradients multiply together, making early layers nearly impossible to train.",
            tanhNote: "Tanh has a stronger derivative than sigmoid (peaks at 1.0), but still vanishes for large |x|. Better than sigmoid, but ReLU avoids the problem entirely.",
            reluNote: "ReLU's derivative is exactly 1 for positive inputs — gradients flow unchanged. This is why ReLU made deep networks practical.",
        },
        deadNeuron: {
            title: "The Dead Neuron Problem",
            hint: "A very negative bias means ReLU always outputs zero. Drag the bias up to bring the neuron back to life.",
            outputsLabel: "ReLU output for different inputs",
            deadLabel: "This neuron is dead — it will never learn.",
            aliveLabel: "The neuron is alive and responding to inputs!",
            partialLabel: "Partially active — some inputs get through.",
            deadExplain: "With such a negative bias, the pre-activation is always negative. ReLU clips it to zero. Zero gradient means zero learning.",
            aliveExplain: "Positive pre-activation values pass through ReLU, producing non-zero outputs and gradients.",
        },
        vowelTeaser: {
            title: "Preview: One Neuron Per Vowel",
            hint: "A glimpse of what we'll build in §07 — a network that recognizes vowels.",
            desc: "5 vowels (a, e, i, o, u). One output neuron per vowel. Given an input letter, each neuron outputs how likely it thinks its vowel comes next.",
            forward: "We'll build this for real when we get to From Numbers to Language.",
            inputNodeLabel: "neuron",
            inputValueLabel: "letter",
        },
        stepEpochBatch: {
            title: "Steps, Epochs & Batches",
            hint: "Watch data flow through the network in batches. One full pass through all data = one epoch.",
            stepLabel: "Step",
            epochLabel: "Epoch",
            batchLabel: "Batch",
            dataLabel: "{n} training examples",
            batchSizeLabel: "batch size = {n}",
            epochProgress: "Epoch progress",
            shortcutsHint: "Shortcuts: ←/→ step · Space play/pause",
            play: "▶ Play",
            pause: "⏸ Pause",
            stepBtn: "Step →",
            reset: "↺ Reset",
            complete: "3 epochs complete! The network has seen every example 3 times.",
        },
        gradientNoise: {
            title: "Gradient Noise vs Batch Size",
            hint: "Larger batches give smoother gradients but each step processes more data. Smaller batches are noisier but update weights more frequently per epoch. Compare the curves to see the trade-off.",
            batchSizeLabel: "Batch size",
            trueGradient: "true gradient",
            batchGradient: "batch gradient",
            noise1: "Batch size 1 (SGD): each arrow is from a single example. Very noisy — each points in a different direction.",
            noise8: "Batch size 8: some averaging smooths the noise, but arrows still scatter around the true direction.",
            noise32: "Batch size 32: a good balance. Arrows cluster near the true gradient with moderate noise.",
            noise256: "Batch size 256: very smooth gradients, almost aligned. But each step is expensive to compute.",
            noiseFull: "Full batch: one perfect arrow. No noise, but extremely slow — computes gradient over ALL examples.",
        },
        overfittingPlay: {
            title: "The Overfitting Playground",
            hint: "Three models, same data. Only one generalizes well to new, unseen points. Click 'Test' to find out which.",
            underfit: "Underfitting",
            overfit: "Overfitting",
            optimal: "Optimal",
            showTest: "🧪 Test with unseen data",
            hideTest: "Hide test data",
            testInsight: "Look at the test accuracies! The overfit model scores 100% on training data but only 54% on new data — worse than random for two classes. The optimal model sacrifices a bit of training accuracy (88%) but generalizes far better (85% on test). The underfit model is bad at both — too simple to learn the pattern at all.",
        },
        findingDirection: {
            title: "Can We Fix a Bad Prediction?",
            lead: "We built a neuron. But with random weights, its predictions are completely wrong. Can we figure out how to fix them — without trying every possible combination?",
            peak3: "There HAS to be a better way.",
            reflection2: "You just measured something invisible — how much one number affects another through a chain of operations.",
            whatIf2Title: "What if the derivative is zero everywhere?",
            whatIf2Text: "Then the network can't learn at all. Zero derivative means 'no change detected' — the weights have no idea which direction to move. This is called a vanishing gradient, and it plagued early deep networks until ReLU and careful initialization fixed it.",
        },
        howItLearns: {
            title: "How a Network Learns",
            lead: "We have a neuron that computes: take inputs, multiply by weights, add them up. But ",
            leadHighlight: "those weights start as random numbers",
            leadEnd: " — so the answers are completely wrong. Here's the challenge: can you figure out how to fix them?",
            phaseA: {
                p1: "Let's make this concrete. Imagine we're predicting a commute time. The inputs are fixed facts: distance = 5 km and traffic = 7. We want the right answer to be 30 minutes. But our neuron starts with random weights (w₁ = 4, w₂ = 3) and a random bias — the fixed overhead time like getting into the car and starting the engine. So the first prediction is way off.",
                hint: "The model predicts minutes as w₁ × distance + w₂ × traffic + bias. With random parameters, the prediction is wrong.",
                p2: "The inputs are fixed data. The only things we can change are the weights and the bias. So how would YOU adjust w₁, w₂, and b to make the prediction match 30 minutes? Try it:",
            },
            phaseB: {
                legacyConcrete: "Let's make this concrete. Suppose we know the right answer: when the inputs are 1 and 2, the output should be 3. But right now, the model has random weights (w₁ = 4, w₂ = 3), so it computes 4×1 + 3×2 = 10. That's way off — it should be 3, not 10.",
                intro: "What if we just try changing a weight and see what happens? Use the sliders to adjust w₁ and w₂. Try to make the output equal 3.",
                nudgeHint: "Drag the weight sliders and watch the output change. Notice which weight has more effect.",
                discovery: "Did you notice? Changing w₁ by 1 moves the output by 1 (because x₁ = 1). Changing w₂ by 1 moves the output by 2 (because x₂ = 2). Each weight has a different leverage. But how do we measure that leverage precisely — for any operation, not just this one?",
            },
            phaseC: {
                intro: "That worked for one operation. But our neuron chains MANY operations — multiply, add, activate. We need a way to measure the effect of a change through an entire chain. Start with the simplest case:",
                derivativeHint: "Toggle between addition and multiplication. Change x and y to see how the result changes.",
                nameIt: "For addition, the derivative is always 1. For multiplication (x × y), the derivative of x is y — the effect of x depends on how big y is. This simple idea — measuring how much one thing affects another — is the foundation of how neural networks learn. Now: what happens when operations are chained?",
            },
            phaseD: {
                intro: "In a real neuron, operations are chained: multiply by weight, then add bias. A change at the input ripples through every step. Watch it:",
                chainHint: "Move the x slider and watch how the change propagates through each operation.",
                nameIt: "To find the total effect, multiply the individual effects along the chain. This is the chain rule — and it works no matter how many operations you stack. A network with millions of operations? Same idea, applied backwards from the output all the way to the first weight. Now we know the size of the effect. But which direction should we move the weight?",
            },
            phaseE: {
                intro: "We know the loss — now which direction should we move each weight to reduce it? The slope of the loss curve tells us:",
                dirHint: "Try the three scenarios to see the logic: too high, too low, and just right.",
                rule: "Always move opposite to the direction that makes things worse. If the derivative is positive and the output is too high — decrease the weight. If the output is too low — increase it. This one rule is behind all of neural network training. But we still need a single number that captures how wrong the model is — so we can track progress.",
            },
            phaseF: {
                intro: "The output is 10, we wanted 3, so the error is 7. But raw errors have a problem: positives and negatives can cancel each other out across many examples. We need something better.",
                lossHint: "See why raw errors are problematic and how squaring fixes it.",
                named: "Squaring the error gives us the loss — a single positive number that measures how wrong the model is. The bigger the loss, the worse the prediction. Now we have everything: the direction to move (gradient), the size of each step (learning rate), and a score to minimize (loss). Let's put it all together.",
            },
            phaseG: {
                intro: "One complete training step: compute the output, measure the loss, calculate the gradients, update the weights. Five phases. Watch them happen:",
                calcHint: "Step through the five phases of a single training iteration.",
            },
            naming: {
                title: "Naming what you just learned",
                text: "Computing the output is the forward pass. Squaring the error gives us the loss. Computing how each weight affects the loss gives us the gradients (the backward pass). Updating weights by subtracting the gradient is gradient descent. The full loop — forward, loss, backward, update — is called backpropagation. You just did it by hand!",
            },
            phaseH: {
                intro: "One step moved us closer, but we're not there yet. What if we repeat this process over and over? Each step nudges the weights a little closer to the right values.",
                repeatHint: "Click 'Train one step' or use auto-train to watch the loss decrease over time.",
                outro: "Did you notice? The loss dropped quickly at first, then slowed down. The early steps make big improvements because the weights are far from their target. Later steps make tiny refinements. But there's a hidden control that determines how fast or slow this happens...",
            },
            phaseI: {
                intro: "The learning rate (written as η, the Greek letter 'eta') controls how big each step is. After computing the gradient, we don't jump the full amount — we take a fraction of it. That fraction is the learning rate.",
                lrLabel: "Interactive · Learning Rate Explorer",
                lrHint: "Try all three presets, then use the custom slider to find the sweet spot. Watch the loss curve carefully.",
                outro: "Too small and training crawls. Too large and it explodes. The learning rate is one of the most important choices in training a neural network — and there's no perfect formula. It takes experimentation.",
            },
            phaseJ: {
                intro: "Let's see the full picture of learning. Below is a map of all possible weight combinations. Dark blue means low loss (good). Red means high loss (bad). Watch the weights travel from their random starting point toward a good solution.",
                trajectoryLabel: "Interactive · Weight Landscape",
                trajectoryHint: "The dot traces the path of the weights during training. The dashed green line shows all weight combinations that give the correct output. Watch the dot 'roll downhill' toward low loss.",
                outro: "This is gradient descent in action — the weights follow the slope of the loss landscape, always moving toward lower loss. The learning rate controls how big each step is on this landscape.",
            },
            gradientMeaning: "What do those gradient numbers mean? If the gradient for w₁ is +14, it means: increasing w₁ by 1 would increase the loss by 14. That's bad — so we decrease w₁ instead. The sign tells you the direction; the size tells you how sensitive the loss is to that weight.",
            namingTransition: "Let's pause and give names to everything you just learned.",
            fullVisualizer: "See the full interactive backpropagation visualizer",
            learningRate: {
                title: "The Learning Rate",
                tooSmall: "Too slow",
                justRight: "Good",
                tooLarge: "Too fast",
                custom: "Custom η",
                lossOverTime: "Loss over training steps",
                lossValue: "Loss",
                play: "▶ Train",
                running: "Training...",
                reset: "Reset",
                watchPrompt: "Press Train to watch what happens with this learning rate.",
                verdictDiverge: "The loss exploded!",
                explainDiverge: "The learning rate is too large. Each step overshoots the minimum and makes things worse. The weights bounce wildly instead of converging.",
                verdictConverge: "Smooth convergence!",
                explainConverge: "The learning rate is well-chosen. Each step makes steady progress toward the minimum. The weights settle into good values.",
                verdictSlow: "Still far from the answer...",
                explainSlow: "The learning rate is too small. Each step barely moves the weights. It would take thousands of steps to converge. Training is painfully slow.",
            },
            trajectory: {
                title: "Weight Landscape",
                lowLoss: "Low loss",
                highLoss: "High loss",
                play: "▶ Watch training",
                running: "Training...",
                reset: "Reset",
                hint: "The heatmap shows loss for every (w₁, w₂) combination. The white path traces how gradient descent moves the weights from their starting point (red dot) toward a good solution.",
            },
            predictionError: {
                title: "Prediction vs Reality",
                expected: "Actual (target)",
                got: "Predicted",
                error: "Error",
                offBy: "Adjust the weights and bias to bring the prediction closer to the target.",
                challenge: "Challenge: match the actual commute time",
                challengeDesc: "The actual commute is 30 min. Adjust w₁, w₂, and bias until your prediction is within 1 minute.",
                sensitivity: "Changing {w} by 1 → output changes by {n}",
                comparison: "Predicted vs actual",
                success: "Nailed it! Error < 1 min — the model has learned the right weights.",
            },
            nudge: {
                title: "What If We Nudge a Weight?",
                w1Sensitivity: "When w₁ changes by 1, the output changes by 1 (because x₁ = 1).",
                w2Sensitivity: "When w₂ changes by 1, the output changes by 2 (because x₂ = 2).",
                effectOfW1: "Sensitivity of w₁",
                effectOfW2: "Sensitivity of w₂",
                perUnit: "per unit change",
                perfect: "You found it! The output equals the target.",
                keepTrying: "Keep adjusting to reach the target of 3.",
                challengeW1: "Predict: if w₁ goes up by 1, how much does the output change?",
                challengeW2: "Predict: if w₂ goes up by 1, how much does the output change?",
                guessCorrect: "Correct! Output changes by exactly {n}. This number is the sensitivity — or derivative — of the output with respect to this weight.",
                guessWrong: "Not quite. The answer is {n}. The output changes by exactly x₁ (or x₂) because that's what it's multiplied by.",
                sensitivityNaming: "These numbers — how much output changes per unit of weight — are called sensitivities. In calculus, they have a formal name: derivatives.",
                formalTitle: "Formal notation (∂/∂w)",
                formalText: "In calculus, the derivative of the output with respect to a weight is written ∂output/∂w. It measures exactly what you just computed: how much the output changes when the weight changes by a tiny amount.",
                formalNote: "The ∂ symbol (partial derivative) is used because the output depends on multiple weights simultaneously.",
            },
            derivative: {
                title: "Measuring Sensitivity",
                question: "What happens if x goes up by 1?",
                before: "Before",
                after: "After (x + 1)",
                zChanged: "z changed by",
                thisIs: "This is the derivative",
                addExplain: "For addition, the derivative is always 1 — x always changes z by exactly 1, no matter what y is.",
                mulExplain: "For multiplication, the derivative equals y (currently {y}). The effect of x depends on how big y is!",
                meterLabel: "sensitivity",
                meterLabelRevealed: "derivative",
                sensitivityLabel: "The sensitivity of z to x",
                revealedNote: "Mathematicians call this number the derivative. You discovered it just by asking: what changes?",
            },
            chainRule: {
                title: "Chaining Operations",
                ifXChanges: "If x goes up by 1, what happens at each step?",
                totalEffect: "Total effect of x on the result (chain rule: multiply the derivatives):",
                explanation: "If x changes by 1, the result changes by {w}. We found this by multiplying the local derivatives: {w} × 1 = {w}.",
                startBtn: "▶ Run propagation",
                nextBtn: "Next step →",
                resetBtn: "↺ Reset",
            },
            parabola: {
                title: "Loss Landscape",
                hint: "Drag the point along the curve to see how the slope changes. The slope tells the model which way to move.",
                weightLabel: "Weight",
                lossLabel: "Loss",
                slopeLabel: "Slope",
                slopePositive: "Slope is positive → move weight LEFT to reduce loss",
                slopeNegative: "Slope is negative → move weight RIGHT to reduce loss",
                slopeZero: "Slope is ≈ zero — you’re at the minimum!",
                watchGD: "▶ Watch gradient descent",
                running: "Descending...",
                dragHint: "Drag the point on the curve, or press the button to watch gradient descent in action.",
            },
            gradientDir: {
                title: "Which Direction to Move?",
                tooHigh: "Output too high",
                tooLow: "Output too low",
                justRight: "Just right",
                outputTooHigh: "Output is {output} — higher than target {target}. We need to bring it down.",
                outputTooLow: "Output is {output} — lower than target {target}. We need to push it up.",
                derivativeIs: "The derivative is",
                positive: "positive",
                negative: "negative",
                posExplain: "Increasing this weight would increase the output even more — making things worse.",
                negExplain: "Increasing this weight would increase the output — which is what we need.",
                soDecrease: "So we decrease the weight — move opposite to the derivative.",
                soIncrease: "So we increase the weight — move opposite to the derivative.",
                perfect: "Output matches the target!",
                perfectExplain: "The derivative is zero — no adjustment needed. The weights are already correct.",
                rule: "The rule: always move the weight in the opposite direction of its derivative.",
            },
            lossMotive: {
                title: "Why Square the Error?",
                ourExample: "Our model outputs 10, we wanted 3. The error is:",
                problem: "But what if some errors are positive and some are negative?",
                example1: "Prediction too high",
                example2: "Prediction too low",
                sumRaw: "Sum of raw errors",
                cancelOut: "The errors cancel out! The sum is 0 even though both predictions are wrong by 7.",
                solution: "The fix: square the errors. Squaring makes everything positive.",
                squaringFix: "Both give 49 — no cancellation. Squaring also penalizes big mistakes more than small ones.",
                lossLabel: "The Loss",
                lossExplain: "This single number tells us how wrong the model is. Our goal: make it as small as possible.",
                alternativesTitle: "Loss Function Alternatives",
                mseDesc: "Squared error. Penalizes large mistakes heavily. Smooth gradient everywhere.",
                maeDesc: "Absolute error. Treats all mistakes equally. Gradient is flat (±1).",
                rawDesc: "Raw error. Positive and negative cancel out. Useless for training.",
                alternativesNote: "MSE is the default for regression because its gradient scales with the error size — big mistakes get big corrections. MAE is more robust to outliers but harder to optimize.",
            },
            neuronCalc: {
                title: "One Complete Training Step",
                step: "Step {n} of {total}",
                next: "Next",
                prev: "Previous",
                before: "Loss before",
                after: "Loss after",
                s1Title: "1. Forward Pass",
                s1Desc: "Compute the output using the current weights.",
                s3Title: "2. Compute the Loss",
                s3Desc: "Square the error to get a single number measuring how wrong we are.",
                s6Title: "3. Compute Gradients",
                s6Desc: "How much does each weight contribute to the loss? The gradient tells us.",
                gradExplain: "Both gradients are positive — increasing either weight would increase the loss. So we should decrease both.",
                s7Title: "4. Update Weights",
                s7Desc: "Subtract a small fraction of the gradient from each weight. This moves them toward better values.",
                s8Title: "5. Did It Improve?",
                s8Better: "The loss decreased! One step closer to the right answer.",
                iteration: "Iteration {n}",
                resetAll: "Reset all",
                trainAgain: "Train again ↻",
                lossOverTime: "Loss over iterations",
            },
            repeated: {
                title: "Learning Over Time",
                oneStep: "Train one step",
                auto: "Auto-train (30 steps)",
                training: "Training...",
                reset: "Reset",
                converged: "The model has converged! The output is now very close to the target.",
            },
            batchingTransition: "You've seen how one example flows through the network. But real datasets have thousands or millions of examples. Training on them one-by-one would take forever. The solution: batches.",
            batching: {
                title: "The Mini-Batch Revolution",
                lead: "Processing examples one-at-a-time is painfully slow. Training on the entire dataset at once is impractical. Mini-batches solve both problems — and the noise they introduce turns out to be a feature, not a bug.",
                p1: "In practice, computing gradients one example at a time is inefficient. Modern GPUs can process hundreds of examples in parallel. Instead of updating weights after every single example, we average the gradients from a small batch of examples — typically 32 to 256 — and update once per batch. This is called mini-batch gradient descent.",
                p2: "Batch size controls a fundamental trade-off. A batch size of 1 (stochastic gradient descent, or SGD) gives noisy gradients — each update points in a slightly different direction because it's based on one random example. A batch size equal to the full dataset gives perfectly smooth gradients, but it's slow and can overfit. Mini-batches strike a balance: reasonably stable gradients with efficient computation.",
                p3: "The noise from small batches isn't just a necessary evil — it's helpful. Noisy gradients help the network escape shallow local minima and act as implicit regularization, often leading to better generalization on unseen data. This is why SGD and small mini-batches remain popular despite being noisier than full-batch training.",
                calloutTitle: "Why noise helps",
                calloutText: "Gradient noise isn't just a necessary evil — it's a feature. Small batches introduce randomness that helps the optimizer explore the loss landscape more thoroughly, escaping shallow local minima and finding solutions that generalize better to new data.",
                conclusion: "Modern deep learning standardizes on mini-batches of 32–256 examples. Larger batches train faster per epoch but may generalize worse. Smaller batches are noisier but often find better solutions.",
                visual1Label: "Interactive · Gradient Noise vs Batch Size",
                visual1Hint: "Drag the batch size slider to see how gradient estimates vary. Small batches produce scattered gradient vectors; large batches converge tightly around the true gradient direction.",
                visual2Label: "Loss Curves · Batch Size Comparison",
                visual2Hint: "Three simulated training runs with different batch sizes. Red (batch=1) is noisy but explores well. Green (batch=32) balances stability and exploration. Blue (full batch) is smooth but slow.",
            },
            workedExample: {
                title: "A Concrete Example",
                intro: "Let's watch a single neuron learn from one example, step by step.",
                step1Title: "Starting Values",
                step1Text: "We start with random weights (w=0.5), a bias (b=-0.2), and one training example (x=1.0, target=0.8).",
                step2Title: "1. Forward Pass",
                step2Text: "The neuron computes w*x + b = 0.3. After sigmoid, the prediction is 0.57.",
                step3Title: "2. Loss Calculation",
                step3Text: "The prediction (0.57) is lower than the target (0.8). The loss (how wrong we are) is 0.05.",
                step4Title: "3. Backpropagation",
                step4Text: "We calculate how to change w to reduce that loss. The gradient tells us to increase w.",
                step5Title: "4. Parameter Update",
                step5Text: "We nudge the weight slightly. New w = 0.61. The prediction is now closer to the target!"
            },
            workedForward: "Forward pass",
            workedUpdateNote: "This full loop — forward, loss, backward, update — repeats many times during training.",
        },
        watchingItLearn: {
            title: "Training In Action",
            lead: "Theory is one thing. Watching it happen is another. The demo below runs real training steps on a single neuron.",
            p1: "Press the training button and observe. The loss should drop. The prediction should creep toward the target. Each click runs one cycle of forward pass, backpropagation, and weight update — the same process described in the previous section, but live.",
            p2: "Pay attention to how the weights change. Early steps produce large swings because the gradients are steep. Later steps produce tiny refinements as the neuron settles into a good solution. This is gradient descent in action.",
            alertTitle: "Loss is increasing!",
            alertText: "When the learning rate is too high, gradient descent can overshoot the minimum and cause the loss to diverge. Try reducing η to below 2.0 for stable convergence.",
            landscapeTitle: "Loss Landscape",
            landscapeDesc: "The heatmap below shows how loss varies across weight and bias combinations. Train above and watch the trajectory descend toward the low-loss valley.",
            terminologyIntro: "Before we go further, let's clarify some terminology you'll see everywhere in machine learning.",
            termStep: "Step",
            termStepDesc: "One weight update. Feed a batch of examples, compute the loss, compute gradients, update weights. Done.",
            termEpoch: "Epoch",
            termEpochDesc: "One complete pass through all training data. If you have 1,000 examples and a batch size of 100, one epoch = 10 steps.",
            termBatch: "Batch / Mini-batch",
            termBatchDesc: "A batch is the number of training examples the network processes before updating its weights once. Processing everything at once is too expensive, so we split the data into small chunks.",
            textDemoHint: "Watch the context window slide through the text. Each position creates one training example: the window is the input, the next character is the target.",
            supervisedLearning: "This approach — where we know the correct answer for every training example — is called supervised learning. The model 'supervises' its own correction by comparing predictions to known targets.",
        },
        makingItLearn: {
            title: "Making It Learn",
            lead: "You know the direction. You know the size. Now let's put it all together — compute the loss, follow the gradient, and watch the weights improve step by step.",
            peak4: "You just did what took forty years to figure out.",
            peak5: "Watch the loss fall. The machine is learning.",
            reflection3: "Each step is tiny. But thousands of tiny steps in the right direction can solve problems no human could program by hand.",
            whatIf3Title: "What if the learning rate is exactly 0?",
            whatIf3Text: "The weights never change. The gradient is computed, but multiplied by zero before being applied. The network stays frozen at its random initialization forever — it literally cannot learn. This is why η > 0 is essential.",
        },
        training: {
            sectionTitle: "Training: From One Step to Thousands",
            sectionLead: "You've seen one training step. Now let's repeat it, tune it, and watch the weights find their way to a solution.",
            repeatedIntro: "One step moved us closer, but we're not there yet. What if we repeat this process over and over? Each step nudges the weights a little closer to the right values.",
            repeatedChallenge: {
                question: "How many steps does it take for the loss to become nearly zero? Watch the curve — when does improvement slow down?",
                hint: "The loss drops fast at first because the gradients are steep. Later steps make tiny refinements. This is typical of gradient descent.",
                success: "The loss converges! Early steps made big improvements; later steps refined. This 'fast then slow' pattern is universal in neural network training.",
            },
            divergenceIntro: "But wait — in the training step above, we only subtracted a tiny fraction of the gradient. What happens if we subtract the full gradient instead? Let's find out:",
            lrIntro: "The learning rate (η) controls how big each step is. After computing the gradient, we don't jump the full amount — we take a fraction. That fraction is the learning rate.",
            lrChallenge: {
                question: "Find the learning rate that converges fastest without diverging. Which η gets to low loss in the fewest steps?",
                hint: "Try all three presets first. Too small = slow crawl. Too large = the loss explodes. The sweet spot is somewhere in between.",
                success: "Found it! The optimal η balances step size against stability. This is one of the most important hyperparameter choices in all of deep learning.",
            },
            trajectoryIntro: "Let's see the full picture of learning. Below is a map of all possible weight combinations. Dark blue means low loss. Red means high loss. Watch the weights travel from their random starting point toward a good solution.",
            terminologyIntro: "Before we go further, let's clarify some terminology you'll see everywhere in machine learning.",
            liveIntro: "Theory is one thing. Watching it happen is another. The demo below runs real training steps on a single neuron.",
            liveP1: "Press the training button and observe. The loss should drop. The prediction should creep toward the target. Each click runs one cycle of forward pass, backpropagation, and weight update — the same process you just built by hand, but live.",
            liveDemoLabel: "Interactive · Live Training",
            liveDemoHint: "Click 'Train' to run a training step. Watch the weights update and loss decrease in real time.",
            supervisedTitle: "What is supervised learning?",
            supervisedDef: "Supervised learning = training with known correct answers. Every example is a (input, correct answer) pair. The network's job is to find weights that map inputs to correct answers across all pairs.",
            supervisedExamplesTitle: "Beyond language: supervised learning is everywhere",
            supervisedCard1Input: "Image",
            supervisedCard1Output: "Cat / Dog",
            supervisedCard2Input: "Email",
            supervisedCard2Output: "Spam / Not",
            supervisedCard3Input: "X-ray",
            supervisedCard3Output: "Clear / Tumor",
            supervisedExample1: "Image classification — input: a photo; correct answer: \"cat\" or \"dog\". The network learns which pixel patterns correspond to which label.",
            supervisedExample2: "Spam detection — input: an email; correct answer: \"spam\" or \"not spam\". The network learns which word patterns signal unwanted mail.",
            supervisedExample3: "Medical diagnosis — input: an X-ray; correct answer: \"tumor present\" or \"clear\". The network learns visual features that radiologists use.",
            supervisedNote: "Not all learning is supervised. Unsupervised learning finds structure without labels. Self-supervised learning generates its own labels from the data — which is exactly how language models like GPT are trained. We'll see those approaches later.",
            textDemo: {
                title: "Training Data From Text",
                windowSize: "Context window",
                step: "Step {n} of {total}",
                epoch: "Epoch {n}",
                play: "▶ Play",
                pause: "⏸ Pause",
                stepBtn: "⏭ Step",
                reset: "🔄 Reset",
                speed: "Speed",
                epochNote: "One pass through the data = 1 epoch. In real training, we repeat hundreds of times.",
                shortcutsHint: "Shortcuts: ←/→ step · Space play/pause",
            },
        },
        trainingAtScale: {
            title: "Training at Scale",
            lead: "One neuron, one example, one step. That's the toy version. Real training processes thousands of examples in batches, repeats for many epochs, and must avoid a dangerous trap: overfitting.",
            multiNeuronTeaser: "Everything we've done with a single neuron extends to networks with thousands. Each neuron gets its own gradient, all updated simultaneously. The math is the same — just applied in parallel across every weight in the network.",
            multiNeuronTeaser2: "In the next section, you'll see a real network with 27 output neurons predicting characters. But first, let's make sure our training process is robust enough to handle that scale.",
            reflection4: "The gap between training loss and validation loss is the gap between memorizing and understanding.",
        },
        toyPredictor: {
            title: "Toy Vowel Predictor",
            hint: "A tiny 5-neuron network that learns to predict the next vowel. Watch the weights change as it trains.",
            inputLabel: "Input vowel (click to select)",
            predictionLabel: "Predictions after \"{v}\"",
            targetTag: "target",
            weightsLabel: "Weight matrix (5×5)",
            lossLabel: "Loss",
            stepCount: "Step {n}",
            trainOne: "Train 1 step",
            autoTrain: "▶ Auto-train",
            stop: "⏸ Stop",
            reset: "↺ Reset",
            converged: "The network learned the pattern! Each vowel correctly predicts the next one in the sequence.",
        },
        beatMachine: {
            title: "Beat the Machine",
            hint: "Can you predict the next character better than a trained bigram network? 10 rounds, you vs the machine.",
            you: "You",
            network: "Network",
            round: "Round {n}/{total}",
            contextLabel: "Context",
            prompt: "What character comes next?",
            yourGuess: "Your guess",
            networkPredictions: "Network's top predictions",
            next: "Next round →",
            seeResults: "See results",
            youWin: "You beat the machine!",
            networkWins: "The network wins this round.",
            summary: "The network uses bigram probabilities learned from thousands of text examples. It always picks the statistically most likely character.",
            playAgain: "↺ Play again",
        },
        contextLimit: {
            title: "Context Window Limitations",
            hint: "See how prediction accuracy changes with context size. More context = better predictions.",
            windowLabel: "Context window:",
            contextWindow: "context",
            predictNext: "predict",
            modelSees: "Model sees:",
            predicts: "predicts →",
            trueAnswer: "True answer",
            insight1: "With only 1 character of context, the model can't distinguish 'q' after 'the ' vs 'q' after anything else. It treats all q's the same.",
            insight2: "Two characters help: 'qu' is much more informative than just 'u'. But longer-range patterns are still invisible.",
            insight3: "Three characters capture common patterns like 'tho' → 'u' (thought). But to truly understand context, we need more — and that's what hidden layers and embeddings provide.",
        },
        fromNumbers: {
            title: "From Numbers to Letters",
            lead: "We've been working with abstract numbers. But remember the bigram model? It predicted the next character. Can our neurons do that?",
            vowelPatternIntro: "We want to teach a tiny network a simple pattern: after 'a' comes 'e', after 'e' comes 'i', after 'i' comes 'o', after 'o' comes 'u', and after 'u' comes 'a' again. Five vowels cycling in order — simple enough for us, but the network has to discover this from scratch.",
            bigramCallback: "Remember from the Bigram chapter how we turned text into numbers? Each character got an index — 'a' = 0, 'b' = 1, and so on. The network will use these same indices as inputs. The difference is that instead of counting transitions in a table, the network will learn the pattern through weights and gradient descent.",
            toyIntro: "Before tackling the full 27-character alphabet, let's start tiny. Five vowels. One neuron per vowel. A network so small you can see every single weight.",
            toyOutro: "That tiny network just learned to predict the next vowel from nothing but random numbers and gradient descent. The same process scales to any number of characters.",
            encodingIntro: "But wait — neurons only understand numbers. How do we feed letters into a network? We need a way to convert characters into numbers.",
            encodingCaveat: {
                title: "A note on encodings",
                text: "Real neural networks don't use alphabet position as input — that implies 'z' is somehow 26× more than 'a'. We'll see better encodings (embeddings) in the MLP chapter, where each character gets its own learned vector.",
            },
            challengeIntro: "Think you understand the patterns? Try to beat the network. It has learned bigram statistics from thousands of examples — can your intuition match its probability tables?",
            contextLimitIntro: "Our single-layer network has a fundamental limitation: it can only see one character at a time. What happens when the answer depends on more context? Try it below — you'll see the network can't distinguish 'th' from 'sh' because it only sees the last character.",
            mlpBridge: "To fix this, we need two things: more context and a smarter way to represent characters. More context means looking at 2, 3, or more previous characters. Smarter representations mean learning what characters have in common — that's what embeddings provide. To fix this, we need a smarter representation of characters — and that's what embeddings provide. The MLP chapter builds exactly this.",
            trainingDataIntro: "First, let's see where the training data comes from. Every position in the text becomes one example: ",
            trainingDataIntroHighlight: "the context window is the input",
            trainingDataIntroEnd: ", the next character is the target.",
            p1: "If we have 27 possible characters (a–z plus space), we need 27 output neurons — each one produces a score for its character. Click different input characters to see how the network distributes its predictions.",
            networkViz: {
                label: "Interactive · Network Diagram",
                arch: "1 input → 27 outputs",
                inputPrompt: "Click an input character",
                topRawScores: "Top raw scores (logits)",
                logitsNote: "These raw numbers are NOT probabilities yet. They can be negative, positive, or anything. We need softmax to turn them into a valid probability distribution.",
                hint: "The glowing neurons are the top-3 predictions. Click different inputs to see how the raw scores shift.",
            },
            softmaxMath: {
                title: "Math: Why softmax(Wx) Recovers the Bigram Table",
                desc: "A beautiful result: the optimal 27×27 weight matrix W exactly encodes log-bigram-probabilities.",
                intro: "Here's the remarkable mathematical connection. Our network computes softmax(W·x) where W is a 27×27 matrix and x is a one-hot vector selecting a row. The probability of character j following character i is:",
                explain: "When x is a one-hot vector for character i, the matrix product W·x simply selects row i of W. So Wᵢⱼ is the logit (raw score) for character j given input i. After training, the optimal Wᵢⱼ converges to log P(j|i) — the log of the bigram probability. Softmax then exponentiates and normalizes, recovering the exact bigram distribution.",
                note: "This is why the neural network and the counting table give the same answer: with 27 inputs and 27 outputs, the weight matrix has exactly 27×27 = 729 parameters — the same number of entries as the bigram table. The architecture constrains the network to learn nothing more and nothing less than the bigram distribution.",
            },
            p2: "These raw scores — called logits — don't sum to 1. They're not probabilities yet. We need a function that turns any list of numbers into a valid probability distribution.",
            softmaxHint: "Toggle between raw logits and softmax probabilities. Try the temperature slider to see how it sharpens or flattens the distribution.",
            p3: "Now we have a complete system: input a context window of characters, compute 27 scores, apply softmax, and get a probability distribution over the next character.",
            p4: "Here's the astonishing result. Train this single-layer neural network on the same text the bigram model used for counting...",
            comparisonLabel: "Neural Network vs Bigram Table",
            comparisonHint: "Watch the neural network's learned probabilities converge toward the exact same values as the bigram frequency table.",
            p5: "It converges to the exact same probabilities as the bigram frequency table! The neural network learns what counting would have given us.",
            whyCalloutTitle: "Why the same result?",
            whyCalloutText: "With one input character and no hidden layers, the mathematically optimal solution IS the bigram table. The architecture constrains what can be learned. To beat the bigram, we need structural changes: larger context windows, hidden layers, and a smarter way to represent characters.",
            peak6: "Random numbers learned what counting gave us.",
            p6: "This is both beautiful and limiting. To surpass the bigram, we need larger context windows, hidden layers, and a smarter way to represent characters. That's exactly what the next chapter delivers.",
            multiNeuron: {
                title: "27 Output Neurons",
                inputLabel: "Input: \"th\" → predicting next character",
                logitsLabel: "Raw scores (logits) — do NOT sum to 1",
            },
            softmax: {
                title: "Softmax Transform",
                rawBtn: "Raw Logits",
                softmaxBtn: "After Softmax",
                sumLabel: "Sum",
                rawHint: "Raw logits can be any number — positive, negative, large, small. They are NOT probabilities.",
                softmaxHint: "Softmax turns any list of numbers into a probability distribution that sums to 1. Lower temperature = sharper, higher = flatter.",
                neuronsLabel: "neurons",
                probsLabel: "probabilities",
                logitsLabel: "logits",
            },
        },
        bridge: {
            title: "The Bridge: Tables to Parameters",
            lead: "Here is the payoff. A single-layer neural network trained to predict the next character converges to the exact same probabilities as a bigram counting table. Counting and learning arrive at the same answer.",
            p1: "A bigram model stores one count per character pair. A neural network stores shared parameters that encode knowledge about all pairs at once. When you add a hidden layer, the network goes further: it learns that vowels behave similarly, that certain consonant clusters share patterns. Knowledge about \"a\" transfers to \"e\" because they activate similar neurons.",
            p2: "The visualization below shows both systems side by side.",
            p2Highlight: "Watch the neural network's predictions converge toward the bigram table",
            p2End: " — then surpass it, because learned parameters generalize where raw counts cannot.",
            insightTitle: "From Tables to Representations",
            insightText: "A bigram table stores 9,216 independent counts. A neural network with a small hidden layer stores fewer parameters — but organized so that similar characters share structure. This is the seed of the idea that becomes word embeddings, attention, and modern language models.",
            p3: "This bridge — from counting to learning — is the most important conceptual leap in language modeling. Everything that follows builds on it.",
            explanation: {
                title: "From Counting to Learning",
                text: "An N-gram table needs a new row for every possible context. A neural network learns shared weights that can handle context it has NEVER seen before by noticing similarities."
            }
        },
        overfitting: {
            label: "The Overfitting Problem",
            heading: "The Risk of Overfitting",
            lead: "A network that trains perfectly on every example can still fail completely on new data. It memorized instead of learned. This is overfitting — one of the most important concepts in all of machine learning.",
            p1: "When a network trains, loss decreases and predictions improve. The natural instinct is to keep going — train longer, push loss as low as possible. But this leads to a trap: the network can achieve near-zero error on the training examples while learning nothing general. It memorizes the specific patterns in the data it saw, including the noise and quirks, and becomes useless on new examples.",
            p2: "Think of a student preparing for an exam. One student memorizes every answer to every practice problem without understanding the underlying concepts. Another student learns the principles and can apply them to new problems. On the practice test, both score perfectly. On the real exam with different questions, only the second student succeeds. The first student overfit to the practice set.",
            p3: "The solution is a train/validation split. Hold out a portion of the data — say 20% — that the model never sees during training. Measure performance on both the training set (data the model learns from) and the validation set (held-out data). Training loss always decreases as the model gets better at fitting what it sees. Validation loss follows a U-curve: it improves as the model learns real patterns, then worsens as the model starts memorizing training-specific noise.",
            p4: "Model complexity amplifies this trade-off. A small model struggles to fit even the training data (underfitting). A right-sized model fits the training data and generalizes to new data. A huge model with far more parameters than needed memorizes the training set perfectly but fails on validation data (overfitting). More capacity is not always better.",
            p5: "How do we fight overfitting? More training data dilutes the memorization effect. Regularization techniques penalize model complexity. Early stopping halts training when validation loss stops improving. Dropout, data augmentation, and BatchNorm all help. The key insight: validation loss is the only honest metric. A model can lie to you with perfect training performance while being completely useless in practice.",
            conclusion: "Overfitting is why we split data, why we watch validation metrics closely, and why bigger models aren't always better. Every ML practitioner learns this lesson — usually by watching a model train to 'perfect' performance and then fail spectacularly on real-world data. The MLP explorer you'll see next flags overfitting automatically, but now you know what it means and why it matters.",
            callout1Title: "Why we can't just minimize training loss",
            callout1Text: "Driving training loss to zero is easy — just memorize every training example. But the goal isn't to ace a test you've already seen. It's to predict patterns you've never encountered. Validation loss is the only honest measure of whether a model has learned something real.",
            callout2Title: "The generalization test",
            callout2Text: "A model that performs well on training data but poorly on validation data has failed the generalization test. It learned the noise, not the signal. In ML, the validation set is the ultimate judge — it's the only data the model hasn't seen and can't cheat on.",
            visual1Label: "Comparison · Good Fit vs Overfit",
            visual1Hint: "Same training data, two different models. The overfit model achieves zero training error by fitting a complex curve through every point. But on new test data (hollow circles), it fails. The good-fit model accepts some training error but generalizes correctly.",
            visual2Label: "Interactive · Train vs Validation Loss Over Time",
            visual2Hint: "Drag the epoch slider to see how training and validation loss evolve. Training loss always decreases. Validation loss forms a U-curve — improving, then worsening as overfitting begins. The optimal stopping point is where validation loss is lowest.",
            statusUnderfitting: "Still learning",
            statusOptimal: "Optimal",
            statusOverfitting: "Overfitting",
        },
        powerAndLimits: {
            title: "Power, Limits, and What Comes Next",
            lead: "A single neuron draws straight lines. That is both its power and its limit.",
            p1: "Consider the XOR problem: four points on a 2D plane where opposite corners share a label. No single straight line can separate them.",
            p1Highlight: "A single neuron will fail at XOR no matter how long you train it.",
            p1End: "This was proven in 1969, and it froze neural network research for nearly two decades.",
            p2: "The fix is simple: stack neurons into layers. One hidden layer of just two neurons can solve XOR. The first layer splits the space into curved regions; the second combines them. A network with one large-enough hidden layer can approximate any continuous function — the Universal Approximation Theorem.",
            p3: "But a single neuron or a shallow network still processes a fixed-size input. It has no memory across time steps and no way to focus on the most relevant parts of its context. For language, this matters: pronouns refer to nouns many tokens back. A fixed window cannot reliably bridge that gap.",
            p4: "The building blocks you have learned — weighted sums, activations, backpropagation, gradient descent — are the same blocks inside every modern AI system. The next step is to stack them into a multi-layer perceptron and apply them directly to language.",
            calloutTitle: "Next: MLP Language Model",
            calloutText: "In the next chapter, we replace the N-gram lookup table with a neural network that uses dense embeddings and learned weights. You will see an MLP generalize beyond exact matches and produce better text with fewer parameters.",
        },
        cta: {
            title: "Continue the Journey",
            subtitle: "Our network matched the bigram perfectly. But to beat it, we need more context — and a smarter way to represent characters. That's exactly what embeddings provide.",
            whatsNextTitle: "What's next in the MLP chapter",
            whatsNext1: "More context — feed the network the last 3–5 characters instead of just one",
            whatsNext2: "Embeddings — represent each character as a dense vector the network can reason about",
            whatsNext3: "Deeper networks — hidden layers that compose simple patterns into complex structure",
            labButton: "Open Free Lab",
            labDesc: "Experiment with perceptrons, activations, and training in the interactive playground.",
            mlpButton: "Next: Building a Language Model",
            mlpDesc: "Stack neurons into layers, add embeddings, and build a real character-level language model.",
        },
        footer: {
            text: "From counting to learning — you now understand the core building blocks of neural networks. Next: stack them into layers and apply them to language.",
            brand: "LM-LAB · Educational Mode"
        }
    }
};
